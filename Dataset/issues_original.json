[
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Issue_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1596320174000,
        "Issue_closed_time":1603980914000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] new ver. of azure cli is not compatible with the old  package; Content: ### description\r\nwe fixed -sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nthe new version of azure-cli is not compatible with the old  package and throws an error when creating  workspace:\r\n\r\n```\r\nunable to create the workspace. \r\n azure error: invalidrequestcontent\r\nmessage: the request content was invalid and could not be deserialized: 'could not find member 'template' on object of type 'deploymentdefinition'. path 'template', line 1, position 12.'.\r\n```\r\n\r\nthere is an open issue at azure cli about the similar error: https:\/\/github.com\/azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### in which platform does it happen?\r\nlinux ubuntu\r\n(haven't tested on other platforms)\r\n\r\n### how do we replicate the issue?\r\ninstall reco_pyspark and run operationalization notebook.\r\n\r\n### expected behavior (i.e. solution)\r\nfix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### other comments\r\ni'm working on #1158 and #900.\r\nif fixing the azure-cli-core version is okay, then i will address this issue together.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the new version of Azure CLI was not compatible with the old package, resulting in an error when creating a workspace.",
        "Issue_preprocessed_content":"Title: new ver. of azure cli is not compatible with the old package; Content: description we fixed sdk ver but not on azure cli core . the new version of azure cli is not compatible with the old package and throws an error when creating workspace there is an open issue at azure cli about the similar error in which platform does it happen? linux ubuntu haven't tested on other platforms how do we replicate the issue? install and run operationalization notebook. expected behavior fix the version of azure cli other comments i'm working on and . if fixing the azure cli core version is okay, then i will address this issue together."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1589",
        "Issue_title":"Azure ML mounting Storage Account",
        "Issue_label":[
            "bug",
            "Workspace Management",
            "ADO"
        ],
        "Issue_creation_time":1631278836000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Hello, \r\n\r\nWe are trying to mount an Azure Storage account in Azure ML. This works perfectly fine, until we start a child run. In the logs of the child run, we can see the following:\r\nSet Dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4\r\n\r\nBut when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': No such file or directory\r\n\r\nThe code to start the child run can be found below.\r\nThank you for your help.\r\n\r\n`child_config = ScriptRunConfig(source_directory='.',\r\n                                       script='src\/main_child.py',\r\n                                       arguments=arguments,\r\n                                       environment=environment,\r\n                                       docker_runtime_config=DockerConfiguration(use_docker=True),\r\n                                       compute_target=compute_target)\r\nrun.submit_child(child_config)`\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: mounting storage account; Content: hello, \r\n\r\nwe are trying to mount an azure storage account in . this works perfectly fine, until we start a child run. in the logs of the child run, we can see the following:\r\nset dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/ml-studio-01\/\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4\r\n\r\nbut when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/ml-studio-01\/\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': no such file or directory\r\n\r\nthe code to start the child run can be found below.\r\nthank you for your help.\r\n\r\n`child_config = scriptrunconfig(source_directory='.',\r\n                                       script='src\/main_child.py',\r\n                                       arguments=arguments,\r\n                                       environment=environment,\r\n                                       docker_runtime_config=dockerconfiguration(use_docker=true),\r\n                                       compute_target=compute_target)\r\nrun.submit_child(child_config)`\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to mount an Azure Storage Account in a child run, resulting in an error when trying to access the mount.",
        "Issue_preprocessed_content":"Title: mounting storage account; Content: hello, we are trying to mount an azure storage account in . this works perfectly fine, until we start a child run. in the logs of the child run, we can see the following set dataset target path to but when we try to access the mount, we get the following error no such file or directory the code to start the child run can be found below. thank you for your help."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Issue_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1630006433000,
        "Issue_closed_time":1632248052000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: identity based access no longer works (with azure sql db datastore) in v1.33 of  sdk; Content: seems like recent upgrade to v1.33 for  sdk has changed how identity based access worked? previously if you had a datastore (ex. sql) with no credentials and then tried to register a dataset, it would prompt you to login to get your aad auth token to see if you had permission to get access to the underlying data source. seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab ad auth token:\r\n**_getting data access token with assigned identity (client_id=clientid)._**\r\n\r\n\r\ni have verified the underlying datastore does not have managed identity on and v1.32 sdk prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. has any changes been made to the identity based access feature from on v1.33 sdk? according to the sdk docs, running the tabulardataset.to_pandas_dataframe() command should prompt an ad login if using no credentialed datastore into dataset creation. fyi currently using azure sql db as datastore, any clarifications would be appreciated!\r\n.core.datastore class -  python | microsoft docs\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with identity based access no longer working in v1.33 of the SDK when trying to register a dataset with an Azure SQL DB datastore.",
        "Issue_preprocessed_content":"Title: identity based access no longer works in of sdk; Content: seems like recent upgrade to for sdk has changed how identity based access worked? previously if you had a datastore with no credentials and then tried to register a dataset, it would prompt you to login to get your aad auth token to see if you had permission to get access to the underlying data source. seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab ad auth token data access token with assigned identity i have verified the underlying datastore does not have managed identity on and sdk prompts me to log in at and gives a code to enter and identity based access works normally after. has any changes been made to the identity based access feature from on sdk? according to the sdk docs, running the command should prompt an ad login if using no credentialed datastore into dataset creation. fyi currently using azure sql db as datastore, any clarifications would be appreciated! class python microsoft docs"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Issue_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1623592472000,
        "Issue_closed_time":1626798489000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: how to copy files into  docker image while deploying ml model using  model deploy command; Content: i am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\neg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\ncopy *.txt \/var\/-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nfailed\r\nerror: {'azure-cli-ml version': '1.29.0', 'error': webserviceexception:\r\n\tmessage: image creation polling reached non-successful terminal state, current state: failed\r\nerror response from server:\r\nstatuscode: 400\r\nmessage: failed to parse steps: copy is not an allowed dockerfile instruction. allowed instructions: arg, env, expose, label, run\r\n\tinnerexception none\r\n\terrorresponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"image creation polling reached non-successful terminal state, current state: failed\\nerror response from server:\\nstatuscode: 400\\nmessage: failed to parse steps: copy is not an allowed dockerfile instruction. allowed instructions: arg, env, expose, label, run\"\r\n    }\r\n}}\r\n\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while trying to deploy an ML model using the az ml model deploy command with additional files, as the copy instruction was not allowed in the dockerfile.",
        "Issue_preprocessed_content":"Title: how to copy files into docker image while deploying ml model using model deploy command; Content: i am trying to deploy ml model using az ml model deploy command with additional files. eg az ml model deploy ds but it gives an error as below"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Issue_title":"Azure ML Run docker command to pull public image failed ",
        "Issue_label":[
            "bug",
            "Pipelines",
            "Compute"
        ],
        "Issue_creation_time":1614273729000,
        "Issue_closed_time":1614604742000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  run docker command to pull public image failed ; Content: 'm going through this notebook: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\ni need to start the training using the docker image from my local registry. i provided all required data in the environment i created:\r\n\r\nconda_env.docker.enabled = true\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"mytoken\"\r\nconda_env.docker.base_image_registry.password = \"mypassword\"\r\n\r\nconda_env.python.user_managed_dependencies = true\r\n\r\nsrc = scriptrunconfig(source_directory='-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=true)\r\n\r\nand when i start the pipeline i got: \"failedpullingimage: unable to pull docker image\\n\\timagename: run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nif i set conda_env.python.user_managed_dependencies = false\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. but on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to pull a public image from their local registry for training a machine learning model, resulting in an \"unauthorized: authentication required\" error.",
        "Issue_preprocessed_content":"Title: run docker command to pull public image failed; Content: 'm going through this notebook i need to start the training using the docker image from my local registry. i provided all required data in the environment i created true mytoken mypassword true src run and when i start the pipeline i got failedpullingimage unable to pull docker run docker command to pull public image failed with error error response from daemon unauthorized authentication required if i set false then the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. but on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error run docker command to pull public image failed with error error response from daemon unauthorized authentication required"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Issue_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Issue_label":[
            "Pipelines",
            "doc-bug",
            "2.0",
            "ADO"
        ],
        "Issue_creation_time":1612300739000,
        "Issue_closed_time":1620257629000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: local execution is not supported for  pipelines. valueerror: please specify a remote compute_target. ; Content: \r\nwhen i try to run a pipeline with target as \"local\" it gives me an error. \r\nvalueerror: please specify a remote compute_target. \r\nthis should be mentioned somewhere in the end of the page under target section. \r\nalso please specify why pipelines cannot be run on local target? people like me waste a lot of time trying this & then realize its a shortcoming in the  python sdk. \r\nplease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### document details\r\n\r\n\u26a0 *do not edit this section. it is required for docs.microsoft.com \u279f github issue linking.*\r\n\r\n* id: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* version independent id: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n*  [.core.runconfig.runconfiguration class -  python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* content source: [-docset\/stable\/docs-ref-autogen\/-core\/.core.runconfig.runconfiguration.yml](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/stable\/docs-ref-autogen\/-core\/.core.runconfig.runconfiguration.yml)\r\n* service: **machine-learning**\r\n* sub-service: **core**\r\n* github login: @debfro\r\n* microsoft alias: **debfro**",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to run a pipeline with a local target, resulting in a ValueError and a lack of documentation on why pipelines cannot be run on local targets.",
        "Issue_preprocessed_content":"Title: local execution is not supported for pipelines. valueerror please specify a remote; Content: when i try to run a pipeline with target as local it gives me an error. valueerror please specify a remote this should be mentioned somewhere in the end of the page under target section. also please specify why pipelines cannot be run on local target? people like me waste a lot of time trying this & then realize its a shortcoming in the python sdk. please update this documentation page as soon as possible. document details do not edit this section. it is required for github issue id f c e c fe b f de c f version independent id a c b c b a f f bbdbce content content source service machine learning sub service core github login microsoft alias debfro"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1817",
        "Issue_title":"Unable to login in Azure ML extension in VSCode",
        "Issue_label":[
            "customer-issue",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1669213419000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.22.0\r\nOS: linux\r\nOS Release: 5.15.0-1022-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.68.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1976096\r\ns extension.js:2:1972783\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to login in  extension in vscode; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.22.0\r\nos: linux\r\nos release: 5.15.0-1022-azure\r\nproduct: visual studio code\r\nproduct version: 1.68.1\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1976096\r\ns extension.js:2:1972783\r\n```\r\n\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the Azure Account extension in Visual Studio Code, where they were unable to login and received an unknown error retrieving subscriptions from the extension.",
        "Issue_preprocessed_content":"Title: unable to login in extension in vscode; Content: does this occur consistently? repro steps . . action error type error message unknown error retrieving susbcriptions from azure account extension version os linux os release product visual studio code product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1754",
        "Issue_title":"Mutliple consecutive sign-in requests from Azure ML plugin VS Code",
        "Issue_label":[
            "customer-issue",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1665696667000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: linux\r\nOS Release: 5.15.0-1017-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: mutliple consecutive sign-in requests from  plugin vs code; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.20.0\r\nos: linux\r\nos release: 5.15.0-1017-azure\r\nproduct: visual studio code\r\nproduct version: 1.72.2\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered multiple consecutive sign-in requests from the plugin VS Code, resulting in an unknown error retrieving subscriptions from the Azure Account Extension.",
        "Issue_preprocessed_content":"Title: mutliple consecutive sign in requests from plugin vs code; Content: does this occur consistently? repro steps . . action error type error message unknown error retrieving susbcriptions from azure account extension version os linux os release product visual studio code product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1745",
        "Issue_title":"Problem signing into Azure ML using VSCode with latest version",
        "Issue_label":[
            "customer-issue",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1664990324000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: problem signing into  using vscode with latest version; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.20.0\r\nos: win32\r\nos release: 10.0.19044\r\nproduct: visual studio code\r\nproduct version: 1.71.2\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a problem signing into Azure using VSCode with the latest version, resulting in an unknown error retrieving subscriptions from the Azure Account Extension.",
        "Issue_preprocessed_content":"Title: problem signing into using vscode with latest version; Content: does this occur consistently? repro steps . . action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1737",
        "Issue_title":"Working with Azure ML Studio on VSCode",
        "Issue_label":[
            "customer-issue",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1664468718000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.18.0\r\nOS: darwin\r\nOS Release: 21.6.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: working with  studio on vscode; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.18.0\r\nos: darwin\r\nos release: 21.6.0\r\nproduct: visual studio code\r\nproduct version: 1.71.2\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an unknown error retrieving subscriptions from the Azure Account Extension while working with Studio on VSCode, with the action \"azureaccount.onsessionschanged\", error type \"123\", and call stack provided.",
        "Issue_preprocessed_content":"Title: working with studio on vscode; Content: does this occur consistently? repro steps . . action error type error message unknown error retrieving susbcriptions from azure account extension version os darwin os release product visual studio code product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Issue_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Issue_label":[
            "bug",
            "area-ml-resource-management",
            "iteration-candidate",
            "area-getting-started"
        ],
        "Issue_creation_time":1661895451000,
        "Issue_closed_time":1663956142000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: update treeview asset labels to match  studio.; Content: in particular:\r\n- experiments needs to be renamed to jobs\r\n- datasets needs to be renamed to data\r\n\r\nfurther changes probably aren't absolutely necessary right now, but should be considered as well. see #616.",
        "Issue_original_content_gpt_summary":"The user encountered challenges in updating the treeview asset labels to match studio, in particular renaming experiments to jobs and datasets to data, with further changes to be considered.",
        "Issue_preprocessed_content":"Title: update treeview asset labels to match studio.; Content: in particular experiments needs to be renamed to jobs datasets needs to be renamed to data further changes probably aren't absolutely necessary right now, but should be considered as well. see ."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Issue_title":"Can't use Azure ML features when remotely connected to a compute",
        "Issue_label":[
            "bug",
            "iteration-candidate"
        ],
        "Issue_creation_time":1661811043000,
        "Issue_closed_time":1665527881000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can't use  features when remotely connected to a compute; Content: since we changed the vscode--remote extension to be of ui type it is not supported anymore in the web or codespaces.\r\n\r\ngiven that main extension depends on vscode--remote, main is also unavailable in the web or codespaces.\r\n\r\nchanging the dependency should enable the main extension in the web context again.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they were unable to use certain features when remotely connected to a compute due to a dependency on the vscode--remote extension.",
        "Issue_preprocessed_content":"Title: can't use features when remotely connected to a compute; Content: since we changed the vscode remote extension to be of ui type it is not supported anymore in the web or codespaces. given that main extension depends on vscode remote, main is also unavailable in the web or codespaces. changing the dependency should enable the main extension in the web context again."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Issue_title":"error when installing AZURE ML training model piece",
        "Issue_label":[
            "bug",
            "arc_ml"
        ],
        "Issue_creation_time":1631563330000,
        "Issue_closed_time":1631711922000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error when installing  training model piece; Content: i was getting an error when logonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. so, i installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"dataset with name 'mnist_opendataset' is not found\".\r\n\r\nany help troubleshooting this error will be appreciated, i am trying to demo this to a customer. next week.\r\n\r\n**text of the output when error is encountered:**\r\n\r\n\r\ninstalling amlarc-compute k8s extension was successful.\r\n\r\nwarning: falling back to use azure cli login credentials.\r\nif you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication.\r\nplease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk.\r\nlibrary configuration succeeded\r\n\r\nwarning: falling back to use azure cli login credentials.\r\nif you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication.\r\n\r\nplease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk.\r\nclass kubernetescompute: this is an experimental class, and may change at any time. please see https:\/\/aka.ms\/experimental for more information.\r\nclass kubernetescompute: this is an experimental class, and may change at any time. please see https:\/\/aka.ms\/experimental for more information.\r\nfound compute target: arc-ml\r\n\"\r\n training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nwarning: falling back to use azure cli login credentials.\r\nif you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication.\r\nplease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk.\r\nwarning: command group 'ml job' is experimental and under development. reference and support levels: https:\/\/aka.ms\/cli_refstatus\r\nuploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?b\/s]\r\n\r\n**error: code: usererror**\r\n**message: dataset with name 'mnist_opendataset' is not found.**\r\n**you cannot call a method on a null-valued expression.**\r\n**at c:\\temp\\logonscript.ps1:267 char:4**\r\n**+    $runid = ($job | grep '\\\"name\\\":').split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ categoryinfo          : invalidoperation: (:) [], runtimeexception**\r\n    **+ fullyqualifiederrorid : invokemethodonnull**\r\n\r\n**runid:**\r\n**training model, hold tight...**\r\n**error: argument --name\/-n: expected one argument**_****\r\n\r\ntry this:\r\naz ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nshow the status of a job using --query argument to execute a jmespath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nread more about the command in reference docs\r\njob status:\r\nerror: argument --name\/-n: expected one argument\r\n\r\ntry this:\r\naz ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nshow the status of a job using --query argument to execute a jmespath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nread more about the command in reference docs\r\njob status:\r\nerror: argument --name\/-n: expected one argument\r\n\r\ntry this:\r\naz ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nshow the status of a job using --query argument to execute a jmespath query on the results of commands.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when installing a training model piece, including an error when running logonscript.ps1, an inability to find grep, and an error when trying to show the status of a job using the --query argument.",
        "Issue_preprocessed_content":"Title: error when installing training model piece; Content: i was getting an error when was running and trying to use grep in one line, but it could not find grep anywhere. so, i installed grep via chocolatey, and now the script goes further to line ,and gives me the error below. grep executes but now the error says dataset with name is not found . any help troubleshooting this error will be appreciated, i am trying to demo this to a customer. next week. text of the output when error is encountered installing amlarc compute k s extension was successful. warning falling back to use azure cli login credentials. if you run your code in unattended mode, where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to for different authentication mechanisms in sdk. library configuration succeeded warning falling back to use azure cli login credentials. if you run your code in unattended mode, where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to for different authentication mechanisms in sdk. class kubernetescompute this is an experimental class, and may change at any time. please see for more . class kubernetescompute this is an experimental class, and may change at any time. please see for more . found compute target arc ml training model + + category invalidoperation , runtimeexception + fullyqualifiederrorid invokemethodonnull runid training model, hold error argument expected one try this az ml job show name my job id query name name,jobstatus status output table resource group my resource group workspace name my workspace show the status of a job using query argument to execute a jmespath query on the results of commands. read more about the command in reference docs job status error argument expected one argument try this az ml job show name my job id query name name,jobstatus status output table resource group my resource group workspace name my workspace show the status of a job using query argument to execute a jmespath query on the results of commands. read more about the command in reference docs job status error argument expected one argument try this az ml job show name my job id query name name,jobstatus status output table resource group my resource group workspace name my workspace show the status of a job using query argument to execute a jmespath query on the results of commands."
    },
    {
        "Issue_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Issue_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1585108798000,
        "Issue_closed_time":1600123381000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: resourcelimitexceeded for ml.m4.xlarge when running  studio demo in a new aws account; Content: when walking through the  studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new aws account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`resourcelimitexceeded: an error occurred (resourcelimitexceeded) when calling the createendpoint operation: the account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 instances, with current utilization of 0 instances and a request delta of 1 instances. please contact aws support to request an increase for this limit.`\r\n\r\nsuggestions:\r\n\r\n- the \"prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nplease lmk which is preferable and i will submit a pr\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a resourcelimitexceeded error when running code cell [17] to create an endpoint to host the model in a new AWS account, due to the default service limit of 'ml.m4.xlarge for endpoint usage' being set to 0 instances.",
        "Issue_preprocessed_content":"Title: resourcelimitexceeded for when running studio demo in a new aws account; Content: when walking through the studio tour for the first time in a new aws account, the usual service limit issue is hit when running code cell to create an endpoint to host the model. suggestions the prerequistes section could address this proactively, with a link to the service limit increase page, the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of please lmk which is preferable and i will submit a pr"
    },
    {
        "Issue_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/69",
        "Issue_title":"sagemaker studio tour missing\/out of order steps",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1585107785000,
        "Issue_closed_time":1593275392000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"regarding this section:\r\nhttps:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/blob\/master\/doc_source\/gs-studio-end-to-end.md#keep-track-of-machine-learning-experiments\r\n\r\n- step 1 \"Run the following cell...\" refers to the 8th code cell in the notebook.  The previous 7 code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc.  The doc  goes from having the user clone the repo:\r\n\r\n`git clone https:\/\/github.com\/awslabs\/amazon-sagemaker-examples.git`\r\n\r\nstraight to having the user run the 8th code cell in the notebook, skipping the first 7 code cells.\r\n\r\n-  step 2 \"Create trials and associate....\" refers to code cell 11 in the notebook, but again jumps straight from cell 8 without ever running cells 9, or 10\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  studio tour missing\/out of order steps; Content: regarding this section:\r\nhttps:\/\/github.com\/awsdocs\/amazon--developer-guide\/blob\/master\/doc_source\/gs-studio-end-to-end.md#keep-track-of-machine-learning-experiments\r\n\r\n- step 1 \"run the following cell...\" refers to the 8th code cell in the notebook.  the previous 7 code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc.  the doc  goes from having the user clone the repo:\r\n\r\n`git clone https:\/\/github.com\/awslabs\/amazon--examples.git`\r\n\r\nstraight to having the user run the 8th code cell in the notebook, skipping the first 7 code cells.\r\n\r\n-  step 2 \"create trials and associate....\" refers to code cell 11 in the notebook, but again jumps straight from cell 8 without ever running cells 9, or 10\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges with the studio tour missing\/out of order steps, as the walkthrough doc skipped from having the user clone the repo to having the user run the 8th code cell in the notebook, without running the first 7 code cells, and then jumped straight from cell 8 to cell 11 without running cells 9 or 10.",
        "Issue_preprocessed_content":"Title: studio tour of order steps; Content: regarding this section step run the following refers to the th code cell in the notebook. the previous code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc. the doc goes from having the user clone the repo straight to having the user run the th code cell in the notebook, skipping the first code cells. step create trials and refers to code cell in the notebook, but again jumps straight from cell without ever running cells , or"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/170",
        "Issue_title":"[bug] : Model not loading while using existing container image to setup MME on sagemaker",
        "Issue_label":[
            "type: bug"
        ],
        "Issue_creation_time":1602135852000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [x] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [x] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nGetting this error, when invoking a MME on sagemaker setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image.\r\n\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=14448): Max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70966dba50>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nTraceback (most recent call last):\r\n  File \"\/sagemaker\/python_service.py\", line 157, in _handle_load_model_post\r\n    self._wait_for_model(model_name)\r\n  File \"\/sagemaker\/python_service.py\", line 247, in _wait_for_model\r\n    response = session.get(url)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\n\r\n*DLC image\/dockerfile:*\r\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04\r\n*Current behavior:*\r\n\r\n*Expected behavior:*\r\nModel should load up and return prediction\r\n*Additional context:*\r\nI have setup a MME using the above mentioned container and invoking the endpoint using a lambda. The model files are in placed in S3 and are in the correct directory structure with a version number. ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] : model not loading while using existing container image to setup mme on ; Content: checklist\r\n- [x] i've prepended issue tag with type of change: [bug]\r\n- [ ] (if applicable) i've attached the script to reproduce the bug\r\n- [x] (if applicable) i've documented below the dlc image\/dockerfile this relates to\r\n- [ ] (if applicable) i've documented below the tests i've run on the dlc image\r\n- [x] i'm using an existing dlc image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] i've built my own container based off dlc (and i've attached the code used to build my own image)\r\n\r\n*concise description:*\r\ngetting this error, when invoking a mme on  setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image.\r\n\r\nurllib3.exceptions.maxretryerror: httpconnectionpool(host='localhost', port=14448): max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (caused by newconnectionerror('<urllib3.connection.httpconnection object at 0x7f70966dba50>: failed to establish a new connection: [errno 111] connection refused'))\r\ntraceback (most recent call last):\r\n  file \"\/\/python_service.py\", line 157, in _handle_load_model_post\r\n    self._wait_for_model(model_name)\r\n  file \"\/\/python_service.py\", line 247, in _wait_for_model\r\n    response = session.get(url)\r\n  file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get\r\n    return self.request('get', url, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\r\n    r = adapter.send(request, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\r\n    raise connectionerror(e, request=request)\r\n\r\n*dlc image\/dockerfile:*\r\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04\r\n*current behavior:*\r\n\r\n*expected behavior:*\r\nmodel should load up and return prediction\r\n*additional context:*\r\ni have setup a mme using the above mentioned container and invoking the endpoint using a lambda. the model files are in placed in s3 and are in the correct directory structure with a version number. ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with model not loading while using an existing container image to setup a MME on, resulting in a MaxRetryError and ConnectionError.",
        "Issue_preprocessed_content":"Title: model not loading while using existing container image to setup mme on; Content: checklist i've prepended issue tag with type of change i've attached the script to reproduce the bug i've documented below the dlc this relates to i've documented below the tests i've run on the dlc image i'm using an existing dlc image listed here i've built my own container based off dlc concise description getting this error, when invoking a mme on setup using container image. httpconnectionpool max retries exceeded with url traceback file line , in file line , in response file line , in get return url, kwargs file line , in request resp file line , in send r kwargs file line , in send raise connectionerror dlc current behavior expected behavior model should load up and return prediction additional context i have setup a mme using the above mentioned container and invoking the endpoint using a lambda. the model files are in placed in s and are in the correct directory structure with a version number."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Issue_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1615292808000,
        "Issue_closed_time":1632465008000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error: no kind \"trainingjob\" is registered for version \".aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"; Content: error: no kind \"trainingjob\" is registered for version \".aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to register a \"trainingjob\" with the version \".aws.amazon.com\/v1\" in the scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\".",
        "Issue_preprocessed_content":"Title: error no kind trainingjob is registered for version in scheme; Content: error no kind trainingjob is registered for version in scheme"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/125",
        "Issue_title":"SageMaker Operator Types fails KubeBuilder Pattern validation check",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1595360580000,
        "Issue_closed_time":1596827786000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nSageMaker Operator Types, when included as part of KubeBuilder V2 custom CRD definition fail due to validation errors of unescaped regex patterns. \r\n\r\n```\r\n\/go\/bin\/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n```\r\n\r\n**What you expected to happen**:\r\nKubeBuilder should generate CRD specification which includes AWS SageMaker Operator Types\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```\r\nimport (\r\n\tcommonv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n)\r\n\r\n\/\/ GuestbookSpec defines the desired state of Guestbook\r\ntype GuestbookSpec struct {\r\n\t\/\/ INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\r\n\t\/\/ Important: Run \"make\" to regenerate code after modifying this file\r\n\r\n\tAlgorithmSpecification *commonv1.AlgorithmSpecification `json:\"algorithmSpecification\"`\r\n\r\n\tEnableInterContainerTrafficEncryption *bool `json:\"enableInterContainerTrafficEncryption,omitempty\"`\r\n\r\n\tEnableNetworkIsolation *bool `json:\"enableNetworkIsolation,omitempty\"`\r\n...\r\n\/\/Run make install with above  types in custom operator\r\nmake install \r\n```\r\n\r\n**Anything else we need to know?**:\r\nTried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:Pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):Version: version.Version{KubeBuilderVersion:\"2.3.1\", KubernetesVendor:\"1.16.4\", GitCommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", BuildDate:\"2020-03-26T16:42:00Z\", GoOs:\"unknown\", GoArch:\"unknown\"}\r\n- Operator version (controller image tag):\tgithub.com\/aws\/amazon-sagemaker-operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  operator types fails kubebuilder pattern validation check; Content: <!-- please use this template while reporting a bug and provide as much info as possible. not doing so may result in your bug not being addressed in a timely manner. thanks!\r\n\r\nif you would like to report a vulnerability or have a security concern regarding aws cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**what happened**:\r\n operator types, when included as part of kubebuilder v2 custom crd definition fail due to validation errors of unescaped regex patterns. \r\n\r\n```\r\n\/go\/bin\/controller-gen \"crd:trivialversions=true\" rbac:rolename=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n```\r\n\r\n**what you expected to happen**:\r\nkubebuilder should generate crd specification which includes  operator types\r\n\r\n**how to reproduce it (as minimally and precisely as possible)**:\r\n```\r\nimport (\r\n\tcommonv1 \"github.com\/aws\/amazon--operator-for-k8s\/api\/v1\/common\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n)\r\n\r\n\/\/ guestbookspec defines the desired state of guestbook\r\ntype guestbookspec struct {\r\n\t\/\/ insert additional spec fields - desired state of cluster\r\n\t\/\/ important: run \"make\" to regenerate code after modifying this file\r\n\r\n\talgorithmspecification *commonv1.algorithmspecification `json:\"algorithmspecification\"`\r\n\r\n\tenableintercontainertrafficencryption *bool `json:\"enableintercontainertrafficencryption,omitempty\"`\r\n\r\n\tenablenetworkisolation *bool `json:\"enablenetworkisolation,omitempty\"`\r\n...\r\n\/\/run make install with above  types in custom operator\r\nmake install \r\n```\r\n\r\n**anything else we need to know?**:\r\ntried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked\r\n\r\n**environment**:\r\n- kubernetes version (use `kubectl version`):version: version.version{kubebuilderversion:\"2.3.1\", kubernetesvendor:\"1.16.4\", gitcommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", builddate:\"2020-03-26t16:42:00z\", goos:\"unknown\", goarch:\"unknown\"}\r\n- operator version (controller image tag):\tgithub.com\/aws\/amazon--operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a\r\n- os (e.g: `cat \/etc\/os-release`):\r\n- kernel (e.g. `uname -a`):\r\n- installation method:\r\n- others:\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where operator types, when included as part of kubebuilder v2 custom crd definition, failed due to validation errors of unescaped regex patterns.",
        "Issue_preprocessed_content":"Title: operator types fails kubebuilder pattern validation check; Content: what happened operator types, when included as part of kubebuilder v custom crd definition fail due to validation errors of unescaped regex patterns. what you expected to happen kubebuilder should generate crd specification which includes operator types how to reproduce it anything else we need to know? tried copying the above types and escaped the regex pattern with quotes and everything worked environment kubernetes version version gitcommit b abeb e b edf f ca aa a , builddate t z , goos unknown , goarch unknown operator version os kernel installation method others"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Issue_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1592335014000,
        "Issue_closed_time":1599678360000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error building  types due to missing types in common\/manual_deepcopy; Content: <!-- please use this template while reporting a bug and provide as much info as possible. not doing so may result in your bug not being addressed in a timely manner. thanks!\r\n\r\nif you would like to report a vulnerability or have a security concern regarding aws cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**what happened**:\r\nerror building  types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerfile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon--operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon--operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.deepcopy undefined (type tag has no field or method deepcopy)\r\nmake: *** [vet] error 2**\r\n\r\n**what you expected to happen**:\r\npackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**how to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nimport of  types in go client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon--operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**anything else we need to know?**:\r\n\r\n**environment**:\r\n- kubernetes version (use `kubectl version`): \r\n- operator version (controller image tag): v1.1.0\r\n- os (e.g: `cat \/etc\/os-release`):\r\n- kernel (e.g. `uname -a`):\r\n- installation method:\r\n- others:\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge building types due to missing types in common\/manual_deepcopy.",
        "Issue_preprocessed_content":"Title: error building types due to missing types in; Content: what happened error building types due to missing types in base afccd example nj$ make all go creating new module tmp go found in go fmt go vet undefined make error what you expected to happen packaged types refer to types in which are missing how to reproduce it import of types in go client fails build import anything else we need to know? environment kubernetes version operator version os kernel installation method others"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Issue_title":"unable to kick off the sagemaker job",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1583773133000,
        "Issue_closed_time":1599677796000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":15.0,
        "Issue_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to kick off the  job; Content: \r\ndeployed the sample mnist training job but seems its not getting invoked on the \r\n\r\n```\r\nkubectl describe trainingjob            \r\nname:         xgboost-mnist\r\nnamespace:    default\r\nlabels:       <none>\r\nannotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiversion\":\".aws.amazon.com\/v1\",\"kind\":\"trainingjob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\napi version:  .aws.amazon.com\/v1\r\nkind:         trainingjob\r\nmetadata:\r\n  creation timestamp:  2020-03-09t06:58:17z\r\n  generation:          1\r\n  resource version:    117181\r\n  self link:           \/apis\/.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  uid:                 5a907178-61d3-11ea-b461-02efd6507006\r\nspec:\r\n  algorithm specification:\r\n    training image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    training input mode:  file\r\n  hyper parameters:\r\n    name:   max_depth\r\n    value:  5\r\n    name:   eta\r\n    value:  0.2\r\n    name:   gamma\r\n    value:  4\r\n    name:   min_child_weight\r\n    value:  6\r\n    name:   silent\r\n    value:  0\r\n    name:   objective\r\n    value:  multi:softmax\r\n    name:   num_class\r\n    value:  10\r\n    name:   num_round\r\n    value:  10\r\n  input data config:\r\n    channel name:      train\r\n    compression type:  none\r\n    content type:      text\/csv\r\n    data source:\r\n      s 3 data source:\r\n        s 3 data distribution type:  fullyreplicated\r\n        s 3 data type:               s3prefix\r\n        s 3 uri:                     s3:\/\/<my-bucket>\/xgboost-mnist\/train\/\r\n    channel name:                    validation\r\n    compression type:                none\r\n    content type:                    text\/csv\r\n    data source:\r\n      s 3 data source:\r\n        s 3 data distribution type:  fullyreplicated\r\n        s 3 data type:               s3prefix\r\n        s 3 uri:                     s3:\/\/<my-bucket>\/xgboost-mnist\/validation\/\r\n  output data config:\r\n    s 3 output path:  s3:\/\/<my-bucket>\/xgboost-mnist\/models\/\r\n  region:             us-east-2\r\n  resource config:\r\n    instance count:     1\r\n    instance type:      ml.m4.xlarge\r\n    volume size in gb:  5\r\n  role arn:             arn:aws:iam::<account>:role\/_execution_role\r\n  stopping condition:\r\n    max runtime in seconds:  86400```\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to kick off a sample mnist training job, as the job was not being invoked on the kubectl describe trainingjob.",
        "Issue_preprocessed_content":"Title: unable to kick off the job; Content: deployed the sample mnist training job but seems its not getting invoked on the"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/188",
        "Issue_title":"pip install stepfunctions fails in SageMaker Studio Notebook",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1651588352000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### What did you do?\r\n\r\n<!--\r\n-->pip install stepfunctions fails in SageMaker Studio Notebook\r\n\r\nNotebook is using the Python3 (Data Science) kernel.\r\n\r\n\r\n\r\n\r\n### Reproduction Steps\r\n\r\n<!--\r\n--> pip install stepfunctions\r\n\r\n### What did you expect to happen?\r\n\r\n<!--\r\n-->I expected to be able to install AWS stepfunctions.\r\n\r\n### What actually happened?\r\n\r\n<!--\r\n-->\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\nCollecting stepfunctions\r\n  Using cached stepfunctions-2.3.0.tar.gz (67 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [22 lines of output]\r\n      \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\r\n        % (opt, underscore_opt)\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 36, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in <module>\r\n          \"IPython\",\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\r\n          _setup_distribution = dist = klass(attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\r\n          for k, v in attrs.items()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\r\n          self.finalize_options()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\r\n          for ep in sorted(loaded, key=by_order):\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\r\n          loaded = map(lambda e: e.load(), filtered)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\r\n          return functools.reduce(getattr, attrs, module)\r\n      AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n\r\n### Environment\r\n\r\n  - **AWS Step Functions Data Science Python SDK version  : 2.3.0\r\n  - **Python Version:** <!-- Version of Python (run the command `python3 --version`) --> 3.7\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stack-traces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, slack, etc -->\r\n\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pip install stepfunctions fails in  studio notebook; Content: ### what did you do?\r\n\r\n<!--\r\n-->pip install stepfunctions fails in  studio notebook\r\n\r\nnotebook is using the python3 (data science) kernel.\r\n\r\n\r\n\r\n\r\n### reproduction steps\r\n\r\n<!--\r\n--> pip install stepfunctions\r\n\r\n### what did you expect to happen?\r\n\r\n<!--\r\n-->i expected to be able to install aws stepfunctions.\r\n\r\n### what actually happened?\r\n\r\n<!--\r\n-->\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: cryptographydeprecationwarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: cryptographydeprecationwarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\ncollecting stepfunctions\r\n  using cached stepfunctions-2.3.0.tar.gz (67 kb)\r\n  preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [22 lines of output]\r\n      \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: userwarning: usage of dash-separated 'description-file' will not be supported in future versions. please use the underscore name 'description_file' instead\r\n        % (opt, underscore_opt)\r\n      traceback (most recent call last):\r\n        file \"<string>\", line 36, in <module>\r\n        file \"<pip-setuptools-caller>\", line 34, in <module>\r\n        file \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in <module>\r\n          \"ipython\",\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\r\n          _setup_distribution = dist = klass(attrs)\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\r\n          for k, v in attrs.items()\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\r\n          self.finalize_options()\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\r\n          for ep in sorted(loaded, key=by_order):\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\r\n          loaded = map(lambda e: e.load(), filtered)\r\n        file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\r\n          return functools.reduce(getattr, attrs, module)\r\n      attributeerror: type object 'distribution' has no attribute '_finalize_feature_opts'\r\n      [end of output]\r\n  \r\n  note: this error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 encountered error while generating package metadata.\r\n\u2570\u2500> see above for output.\r\n\r\nnote: this is an issue with the package mentioned above, not pip.\r\nhint: see above for details.\r\n\r\n\r\n### environment\r\n\r\n  - **aws step functions data science python sdk version  : 2.3.0\r\n  - **python version:** <!-- version of python (run the command `python3 --version`) --> 3.7\r\n\r\n### other\r\n\r\n<!-- e.g. detailed explanation, stack-traces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, slack, etc -->\r\n\r\n\r\n\r\n\r\n--- \r\n\r\nthis is :bug: bug report",
        "Issue_original_content_gpt_summary":"The user encountered an error while attempting to install the AWS Step Functions Data Science Python SDK in a Studio notebook using the Python3 (Data Science) kernel.",
        "Issue_preprocessed_content":"Title: pip install stepfunctions fails in studio notebook; Content: what did you do? pip install stepfunctions fails in studio notebook notebook is using the python kernel. reproduction steps pip install stepfunctions what did you expect to happen? i expected to be able to install aws stepfunctions. what actually happened? cryptographydeprecationwarning is deprecated, use instead from import cryptographydeprecationwarning is deprecated, use instead from import collecting stepfunctions using cached preparing metadata error error subprocess exited with error python did not run successfully. exit code > userwarning usage of dash separated 'description file' will not be supported in future versions. please use the underscore name instead % traceback file , line , in file , line , in file line , in ipython , file line , in setup return file line , in setup dist klass file line , in for k, v in file line , in file line , in for ep in sorted file line , in loaded map , filtered file line , in load return attrs, module attributeerror type object 'distribution' has no attribute note this error originates from a subprocess, and is likely not a problem with pip. error metadata generation failed encountered error while generating package metadata. > see above for output. note this is an issue with the package mentioned above, not pip. hint see above for details. environment aws step functions data science python sdk version python version other this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/17",
        "Issue_title":"Support Tensorflow with the new sagemaker.tensorflow.serving.Model",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1578691549000,
        "Issue_closed_time":1579559963000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"TrainingPipeline needs to be updated to accommodate the `sagemaker.tensorflow.serving.Model` from Tensorflow package.\r\n\r\nRelated Thread: https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1201",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: support tensorflow with the new .tensorflow.serving.model; Content: trainingpipeline needs to be updated to accommodate the `.tensorflow.serving.model` from tensorflow package.\r\n\r\nrelated thread: https:\/\/github.com\/aws\/-python-sdk\/issues\/1201",
        "Issue_original_content_gpt_summary":"The user encountered a challenge in updating the training pipeline to support the new `.tensorflow.serving.model` from the TensorFlow package.",
        "Issue_preprocessed_content":"Title: support tensorflow with the new; Content: trainingpipeline needs to be updated to accommodate the from tensorflow package. related thread"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Issue_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1668737794000,
        "Issue_closed_time":1668738306000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: inability to reimage  studio lab instance to get the space back; Content: hello,\r\n\r\nafter i tried to build a conda environment using mlu-tab.yml i was ran out of space with no environment created. after i deleted all files from my home folder i still had 95% of my space used. there is no way to \"reimage\" my studio lab instance and get back the initial 30gb of space.\r\n\r\ni followed the aws machine learning university course and cloned the examples for tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nafter that i was stupid enough to try creating the conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\ncurrently i have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nhow can i reimage  studio lab instance to get the space back or uninstall all libraries installed by creating the conda environment?\r\n\r\nos: windows 10\r\nbrowser: chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with their AWS Machine Learning University course, where they were unable to reimage their studio lab instance to get back the initial 30GB of space after running out of space while trying to build a conda environment.",
        "Issue_preprocessed_content":"Title: inability to reimage studio lab instance to get the space back; Content: hello, after i tried to build a conda environment using i was ran out of space with no environment created. after i deleted all files from my home folder i still had % of my space used. there is no way to reimage my studio lab instance and get back the initial gb of space. i followed the aws machine learning university course and cloned the examples for tabular data course after that i was stupid enough to try creating the conda environment using the file. the environment creation ate all my space available and creation was failed. currently i have % space usage of my folder with no files in it. how can i reimage studio lab instance to get the space back or uninstall all libraries installed by creating the conda environment? os windows browser chrome"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Issue_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1667438077000,
        "Issue_closed_time":1667627477000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: we are experiencing elevated fault rate in start runtime api. the  studio lab team is working to restore the service.; Content: its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in \r\nhow long is this going to even take man",
        "Issue_original_content_gpt_summary":"The user is experiencing an elevated fault rate in the Start Runtime API, and has been unable to run CPU or GPU runtimes for more than three days, raising questions about how long the issue will take to resolve.",
        "Issue_preprocessed_content":"Title: we are experiencing elevated fault rate in start runtime api. the studio lab team is working to restore the service.; Content: its been more than days and im still getting this issue, i cant run cpu or even gpu runtimes in how long is this going to even take man"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Issue_title":"How to get root access in SageMaker Studio Lab",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1654778335000,
        "Issue_closed_time":1655933766000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: how to get root access in  studio lab; Content: hi, i am trying to install some libraries in studio lab which requires root privileges. \r\n\r\nbelow i have run `whoami` to check if i am root user. (i am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nbelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\ni followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\non running `su -` , it asks for the password, but we don't have any password for studio lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\ncan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nplease let me know if my query is not clear. ",
        "Issue_original_content_gpt_summary":"The user is encountering challenges in trying to gain root access in studio lab in order to install libraries that require root privileges.",
        "Issue_preprocessed_content":"Title: how to get root access in studio lab; Content: hi, i am trying to install some libraries in studio lab which requires root privileges. below i have run to check if i am root user. below you can see the error on running sudo > i followed link to install sudo. on running , it asks for the password, but we don't have any password for studio lab. can anyone tell how to get root access or a way to install libraries which require root packages which installs using sudo . please let me know if my query is not clear."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Issue_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1647978773000,
        "Issue_closed_time":1655626980000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to open database file, unexpected error while saving file: d2l-pytorch--studio-lab\/dash\/untitled.ipynb unable to open database file; Content: **describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**to reproduce**\r\ni've deleted some of the unwanted notebooks from studio lab's files and now i am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected error while saving a file in d2l-pytorch--studio-lab\/dash\/untitled.ipynb, unable to open database file, and was unable to install libraries with pip, create new files, or start the kernel.",
        "Issue_preprocessed_content":"Title: unable to open database file, unexpected error while saving file unable to open database file; Content: describe the bug to reproduce i've deleted some of the unwanted notebooks from studio lab's files and now i am getting this error. cannot install libraries with pip, cannot create new files, cannot even start kernel"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Issue_title":"Can't open project on amazon sagemaker",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1645584371000,
        "Issue_closed_time":1668499115000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can't open project on ; Content: hello, i can't open my project on . when i am clicking the 'open project' button, it is loading indefinitely, and i can't do anything with the files. i have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from gpu to cpu but nothing did work. can you please take a look into my account and resolve the issue? a screenshot is attached here to understand better. thanks!\r\n<img width=\"1363\" alt=\"screen shot 2022-02-22 at 9 45 35 pm\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with opening a project on , where the 'open project' button is loading indefinitely and none of the attempted solutions have worked.",
        "Issue_preprocessed_content":"Title: can't open project on; Content: hello, i can't open my project on . when i am clicking the 'open project' button, it is loading indefinitely, and i can't do anything with the files. i have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from gpu to cpu but nothing did work. can you please take a look into my account and resolve the issue? a screenshot is attached here to understand better. thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Issue_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1643305784000,
        "Issue_closed_time":1643319424000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] \"open in in  studio lab\" button process fails when attempting to \"copy notebooks only\"; Content: **describe the bug**\r\ncloning a single notebook using the \"open in in  studio lab\" fails.  cloning the whole repo works.  \r\n\r\nusing 's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, i get this error:\r\n```\r\nunable to copy notebook to project.\r\nthe link to this notebook is broken or blocked. if this is a private github notebook, sign in to github before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/nlp_disaster_recovery_translation.ipynb\r\n```\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. create md cell with `[![open in  studio lab](https:\/\/studiolab..aws\/studiolab.svg)](https:\/\/studiolab..aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/nlp_disaster_recovery_translation.ipynb)`  and run it\r\n2. click on the button that appears once you run the cell.  will open new tab in browser\r\n3. in the new pop up tab, click \"copy to project\".  will open new tab in browser\r\n4. in the new pop up tab's modal, select \"copy notebook only\"\r\n5. error will now appear\r\n\r\n**expected behavior**\r\nmy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**desktop (please complete the following information):**\r\n - os: [windows 11]\r\n - browser [chrome]\r\n - version [97.0.4692.71]\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to clone a single notebook using the \"open in in  studio lab\" button, where the process fails and an error message appears instead of the expected behavior.",
        "Issue_preprocessed_content":"Title: open in in studio lab button process fails when attempting to copy notebooks only; Content: describe the bug cloning a single notebook using the open in in studio lab fails. cloning the whole repo works. using 's sample, i get this error to reproduce steps to reproduce the behavior . create md cell with and run it . click on the button that appears once you run the cell. will open new tab in browser . in the new pop up tab, click copy to project . will open new tab in browser . in the new pop up tab's modal, select copy notebook only . error will now appear expected behavior my notebook will open and appear, just as it would with cloning a directory screenshots desktop os browser version"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/52",
        "Issue_title":"couldn't open Project on amazon sagemaker studio lab",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642548428000,
        "Issue_closed_time":1643050102000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"**Describe the bug**\r\nAmazon Sagemaker studio lab is not opening jupyter notebook. It is loading indefinitely at Preparing project run time after that i am getting `There was a problem when starting the project runtime. This should be resolved shortly.` Please try again later. It's been almost a week and it still hasn't been resolved. Even though I tried shifting the runtime from CPU to GPU but issue still persists. Any help would be appreciated.\r\n\r\n**The error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: couldn't open project on  studio lab; Content: **describe the bug**\r\n studio lab is not opening jupyter notebook. it is loading indefinitely at preparing project run time after that i am getting `there was a problem when starting the project runtime. this should be resolved shortly.` please try again later. it's been almost a week and it still hasn't been resolved. even though i tried shifting the runtime from cpu to gpu but issue still persists. any help would be appreciated.\r\n\r\n**the error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with studio lab not opening jupyter notebook, and is receiving an error message after attempting to shift the runtime from CPU to GPU.",
        "Issue_preprocessed_content":"Title: couldn't open project on studio lab; Content: describe the bug studio lab is not opening jupyter notebook. it is loading indefinitely at preparing project run time after that i am getting please try again later. it's been almost a week and it still hasn't been resolved. even though i tried shifting the runtime from cpu to gpu but issue still persists. any help would be appreciated. the error i am getting is"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Issue_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1641220236000,
        "Issue_closed_time":1641229646000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"as the title suggests\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: i just wonder if i can initialize my  studio lab? ; Content: as the title suggests\r\n",
        "Issue_original_content_gpt_summary":"The user is wondering if they can initialize their studio lab.",
        "Issue_preprocessed_content":"Title: i just wonder if i can initialize my studio lab?; Content: as the title suggests"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Issue_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1639662702000,
        "Issue_closed_time":1639666969000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can't configure profile with aws cli for using aws built-in  algorithms ; Content: hi everybody,\r\n\r\ni am trying to use aws built-in algorithms in  studio lab. for that i need an execution role and region etc. \r\nwhen i try to run my code it outputs\r\n\r\nvalueerror: must setup local aws configuration with a region supported by .\r\n\r\nis it even possible to link access aws resources in studiolab?\r\n\r\nmany thanks in advance!\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge in configuring their profile with AWS CLI for using AWS built-in algorithms in Studio Lab.",
        "Issue_preprocessed_content":"Title: can't configure profile with aws cli for using aws built in algorithms; Content: hi everybody, i am trying to use aws built in algorithms in studio lab. for that i need an execution role and region etc. when i try to run my code it outputs valueerror must setup local aws configuration with a region supported by . is it even possible to link access aws resources in studiolab? many thanks in advance!"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/issues\/37",
        "Issue_title":"Pytorch Sagemaker Container STDERR output",
        "Issue_label":[
            "type: bug",
            "priority: high"
        ],
        "Issue_creation_time":1571735353000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"In Pytorch images all the prints in stderr are not catched and are ignored:\r\n\r\n\r\n### Describe the problem\r\n\r\n### Minimal repro \/ logs\r\nEntrypoint.py:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    import sys\r\n    sys.stderr.write('Coucou stderr')\r\n    sys.stdout.write('Coucou stdout')\r\n```\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nestimator = PyTorch(entry_point='entrypoint.py',\r\n                    role=role,\r\n                    framework_version='1.1.0',\r\n                    train_instance_count=1,\r\n                    train_instance_type='local',\r\n                )\r\nestimator.fit({'config': 's3:\/\/sagemaker-eu-*************\/config\/test_sagemaker_1.json'})\r\n```\r\n\r\n<details><summary>LOGS<\/summary>\r\n<p>\r\n\r\nCreating tmpqp7i_4w3_algo-1-8gd7b_1 ... \r\nAttaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,345 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,349 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,363 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,365 sagemaker_pytorch_container.training INFO     Invoking user training script.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Module entrypoint does not provide a setup.py. \r\nalgo-1-8gd7b_1  | Generating setup.py\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating setup.cfg\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating MANIFEST.in\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,490 sagemaker-containers INFO     Installing module with the following command:\r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m pip install . \r\nalgo-1-8gd7b_1  | Processing \/opt\/ml\/code\r\nalgo-1-8gd7b_1  | Building wheels for collected packages: entrypoint\r\nalgo-1-8gd7b_1  |   Running setup.py bdist_wheel for entrypoint ... done\r\nalgo-1-8gd7b_1  |   Stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3\r\nalgo-1-8gd7b_1  | Successfully built entrypoint\r\nalgo-1-8gd7b_1  | Installing collected packages: entrypoint\r\nalgo-1-8gd7b_1  | Successfully installed entrypoint-1.0.0\r\nalgo-1-8gd7b_1  | You are using pip version 18.1, however version 19.3.1 is available.\r\nalgo-1-8gd7b_1  | You should consider upgrading via the 'pip install --upgrade pip' command.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,054 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,069 sagemaker-containers INFO     Invoking user script\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Training Env:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | {\r\nalgo-1-8gd7b_1  |     \"additional_framework_parameters\": {},\r\nalgo-1-8gd7b_1  |     \"channel_input_dirs\": {\r\nalgo-1-8gd7b_1  |         \"config\": \"\/opt\/ml\/input\/data\/config\"\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\r\nalgo-1-8gd7b_1  |     \"hosts\": [\r\nalgo-1-8gd7b_1  |         \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |     ],\r\nalgo-1-8gd7b_1  |     \"hyperparameters\": {},\r\nalgo-1-8gd7b_1  |     \"input_config_dir\": \"\/opt\/ml\/input\/config\",\r\nalgo-1-8gd7b_1  |     \"input_data_config\": {\r\nalgo-1-8gd7b_1  |         \"config\": {\r\nalgo-1-8gd7b_1  |             \"TrainingInputMode\": \"File\"\r\nalgo-1-8gd7b_1  |         }\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"input_dir\": \"\/opt\/ml\/input\",\r\nalgo-1-8gd7b_1  |     \"is_master\": true,\r\nalgo-1-8gd7b_1  |     \"job_name\": \"sagemaker-pytorch-2019-10-22-09-06-18-353\",\r\nalgo-1-8gd7b_1  |     \"log_level\": 20,\r\nalgo-1-8gd7b_1  |     \"master_hostname\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"model_dir\": \"\/opt\/ml\/model\",\r\nalgo-1-8gd7b_1  |     \"module_dir\": \"s3:\/\/sagemaker-eu-west-1-*********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\r\nalgo-1-8gd7b_1  |     \"module_name\": \"entrypoint\",\r\nalgo-1-8gd7b_1  |     \"network_interface_name\": \"eth0\",\r\nalgo-1-8gd7b_1  |     \"num_cpus\": 2,\r\nalgo-1-8gd7b_1  |     \"num_gpus\": 0,\r\nalgo-1-8gd7b_1  |     \"output_data_dir\": \"\/opt\/ml\/output\/data\",\r\nalgo-1-8gd7b_1  |     \"output_dir\": \"\/opt\/ml\/output\",\r\nalgo-1-8gd7b_1  |     \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\r\nalgo-1-8gd7b_1  |     \"resource_config\": {\r\nalgo-1-8gd7b_1  |         \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |         \"hosts\": [\r\nalgo-1-8gd7b_1  |             \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |         ]\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"user_entry_point\": \"entrypoint.py\"\r\nalgo-1-8gd7b_1  | }\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Environment variables:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | SM_HOSTS=[\"algo-1-8gd7b\"]\r\nalgo-1-8gd7b_1  | SM_NETWORK_INTERFACE_NAME=eth0\r\nalgo-1-8gd7b_1  | SM_HPS={}\r\nalgo-1-8gd7b_1  | SM_USER_ENTRY_POINT=entrypoint.py\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_PARAMS={}\r\nalgo-1-8gd7b_1  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]}\r\nalgo-1-8gd7b_1  | SM_INPUT_DATA_CONFIG={\"config\":{\"TrainingInputMode\":\"File\"}}\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\r\nalgo-1-8gd7b_1  | SM_CHANNELS=[\"config\"]\r\nalgo-1-8gd7b_1  | SM_CURRENT_HOST=algo-1-8gd7b\r\nalgo-1-8gd7b_1  | SM_MODULE_NAME=entrypoint\r\nalgo-1-8gd7b_1  | SM_LOG_LEVEL=20\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\r\nalgo-1-8gd7b_1  | SM_INPUT_DIR=\/opt\/ml\/input\r\nalgo-1-8gd7b_1  | SM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DIR=\/opt\/ml\/output\r\nalgo-1-8gd7b_1  | SM_NUM_CPUS=2\r\nalgo-1-8gd7b_1  | SM_NUM_GPUS=0\r\nalgo-1-8gd7b_1  | SM_MODEL_DIR=\/opt\/ml\/model\r\nalgo-1-8gd7b_1  | SM_MODULE_DIR=s3:\/\/sagemaker-eu-west-1-***********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\r\nalgo-1-8gd7b_1  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/sagemaker-eu-west-1-**********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"}\r\nalgo-1-8gd7b_1  | SM_USER_ARGS=[]\r\nalgo-1-8gd7b_1  | SM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\r\nalgo-1-8gd7b_1  | SM_CHANNEL_CONFIG=\/opt\/ml\/input\/data\/config\r\nalgo-1-8gd7b_1  | PYTHONPATH=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Invoking script with the following command:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m entrypoint\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Coucou stdout2019-10-22 09:06:23,102 sagemaker-containers INFO     Reporting training SUCCESS\r\ntmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0\r\nAborting on container exit...\r\n===== Job Complete =====\r\n<\/p>\r\n<\/details>\r\n\r\nAs you see the coucou stdout has been printed, stderr has been ignored. In distant mode same result.\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pytorch  container stderr output; Content: in pytorch images all the prints in stderr are not catched and are ignored:\r\n\r\n\r\n### describe the problem\r\n\r\n### minimal repro \/ logs\r\nentrypoint.py:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    import sys\r\n    sys.stderr.write('coucou stderr')\r\n    sys.stdout.write('coucou stdout')\r\n```\r\n\r\n```\r\nfrom .pytorch import pytorch\r\nestimator = pytorch(entry_point='entrypoint.py',\r\n                    role=role,\r\n                    framework_version='1.1.0',\r\n                    train_instance_count=1,\r\n                    train_instance_type='local',\r\n                )\r\nestimator.fit({'config': 's3:\/\/-eu-*************\/config\/test__1.json'})\r\n```\r\n\r\n<details><summary>logs<\/summary>\r\n<p>\r\n\r\ncreating tmpqp7i_4w3_algo-1-8gd7b_1 ... \r\nattaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,345 -containers info     imported framework _pytorch_container.training\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,349 -containers info     no gpus detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,363 _pytorch_container.training info     block until all host dns lookups succeed.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,365 _pytorch_container.training info     invoking user training script.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 -containers info     module entrypoint does not provide a setup.py. \r\nalgo-1-8gd7b_1  | generating setup.py\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 -containers info     generating setup.cfg\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 -containers info     generating manifest.in\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,490 -containers info     installing module with the following command:\r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m pip install . \r\nalgo-1-8gd7b_1  | processing \/opt\/ml\/code\r\nalgo-1-8gd7b_1  | building wheels for collected packages: entrypoint\r\nalgo-1-8gd7b_1  |   running setup.py bdist_wheel for entrypoint ... done\r\nalgo-1-8gd7b_1  |   stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3\r\nalgo-1-8gd7b_1  | successfully built entrypoint\r\nalgo-1-8gd7b_1  | installing collected packages: entrypoint\r\nalgo-1-8gd7b_1  | successfully installed entrypoint-1.0.0\r\nalgo-1-8gd7b_1  | you are using pip version 18.1, however version 19.3.1 is available.\r\nalgo-1-8gd7b_1  | you should consider upgrading via the 'pip install --upgrade pip' command.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,054 -containers info     no gpus detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,069 -containers info     invoking user script\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | training env:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | {\r\nalgo-1-8gd7b_1  |     \"additional_framework_parameters\": {},\r\nalgo-1-8gd7b_1  |     \"channel_input_dirs\": {\r\nalgo-1-8gd7b_1  |         \"config\": \"\/opt\/ml\/input\/data\/config\"\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"framework_module\": \"_pytorch_container.training:main\",\r\nalgo-1-8gd7b_1  |     \"hosts\": [\r\nalgo-1-8gd7b_1  |         \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |     ],\r\nalgo-1-8gd7b_1  |     \"hyperparameters\": {},\r\nalgo-1-8gd7b_1  |     \"input_config_dir\": \"\/opt\/ml\/input\/config\",\r\nalgo-1-8gd7b_1  |     \"input_data_config\": {\r\nalgo-1-8gd7b_1  |         \"config\": {\r\nalgo-1-8gd7b_1  |             \"traininginputmode\": \"file\"\r\nalgo-1-8gd7b_1  |         }\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"input_dir\": \"\/opt\/ml\/input\",\r\nalgo-1-8gd7b_1  |     \"is_master\": true,\r\nalgo-1-8gd7b_1  |     \"job_name\": \"-pytorch-2019-10-22-09-06-18-353\",\r\nalgo-1-8gd7b_1  |     \"log_level\": 20,\r\nalgo-1-8gd7b_1  |     \"master_hostname\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"model_dir\": \"\/opt\/ml\/model\",\r\nalgo-1-8gd7b_1  |     \"module_dir\": \"s3:\/\/-eu-west-1-*********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\r\nalgo-1-8gd7b_1  |     \"module_name\": \"entrypoint\",\r\nalgo-1-8gd7b_1  |     \"network_interface_name\": \"eth0\",\r\nalgo-1-8gd7b_1  |     \"num_cpus\": 2,\r\nalgo-1-8gd7b_1  |     \"num_gpus\": 0,\r\nalgo-1-8gd7b_1  |     \"output_data_dir\": \"\/opt\/ml\/output\/data\",\r\nalgo-1-8gd7b_1  |     \"output_dir\": \"\/opt\/ml\/output\",\r\nalgo-1-8gd7b_1  |     \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\r\nalgo-1-8gd7b_1  |     \"resource_config\": {\r\nalgo-1-8gd7b_1  |         \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |         \"hosts\": [\r\nalgo-1-8gd7b_1  |             \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |         ]\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"user_entry_point\": \"entrypoint.py\"\r\nalgo-1-8gd7b_1  | }\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | environment variables:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | sm_hosts=[\"algo-1-8gd7b\"]\r\nalgo-1-8gd7b_1  | sm_network_interface_name=eth0\r\nalgo-1-8gd7b_1  | sm_hps={}\r\nalgo-1-8gd7b_1  | sm_user_entry_point=entrypoint.py\r\nalgo-1-8gd7b_1  | sm_framework_params={}\r\nalgo-1-8gd7b_1  | sm_resource_config={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]}\r\nalgo-1-8gd7b_1  | sm_input_data_config={\"config\":{\"traininginputmode\":\"file\"}}\r\nalgo-1-8gd7b_1  | sm_output_data_dir=\/opt\/ml\/output\/data\r\nalgo-1-8gd7b_1  | sm_channels=[\"config\"]\r\nalgo-1-8gd7b_1  | sm_current_host=algo-1-8gd7b\r\nalgo-1-8gd7b_1  | sm_module_name=entrypoint\r\nalgo-1-8gd7b_1  | sm_log_level=20\r\nalgo-1-8gd7b_1  | sm_framework_module=_pytorch_container.training:main\r\nalgo-1-8gd7b_1  | sm_input_dir=\/opt\/ml\/input\r\nalgo-1-8gd7b_1  | sm_input_config_dir=\/opt\/ml\/input\/config\r\nalgo-1-8gd7b_1  | sm_output_dir=\/opt\/ml\/output\r\nalgo-1-8gd7b_1  | sm_num_cpus=2\r\nalgo-1-8gd7b_1  | sm_num_gpus=0\r\nalgo-1-8gd7b_1  | sm_model_dir=\/opt\/ml\/model\r\nalgo-1-8gd7b_1  | sm_module_dir=s3:\/\/-eu-west-1-***********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\r\nalgo-1-8gd7b_1  | sm_training_env={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"traininginputmode\":\"file\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/-eu-west-1-**********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"}\r\nalgo-1-8gd7b_1  | sm_user_args=[]\r\nalgo-1-8gd7b_1  | sm_output_intermediate_dir=\/opt\/ml\/output\/intermediate\r\nalgo-1-8gd7b_1  | sm_channel_config=\/opt\/ml\/input\/data\/config\r\nalgo-1-8gd7b_1  | pythonpath=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | invoking script with the following command:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m entrypoint\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | coucou stdout2019-10-22 09:06:23,102 -containers info     reporting training success\r\ntmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0\r\naborting on container exit...\r\n===== job complete =====\r\n<\/p>\r\n<\/details>\r\n\r\nas you see the coucou stdout has been printed, stderr has been ignored. in distant mode same result.\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where prints in stderr were not caught and ignored when using pytorch images.",
        "Issue_preprocessed_content":"Title: pytorch container stderr output; Content: in pytorch images all the prints in stderr are not catched and are ignored describe the problem minimal repro \/ logs creating attaching to , containers imported framework , containers no gpus detected , block until all host dns lookups succeed. , invoking user training script. , containers module entrypoint does not provide a generating , containers generating , containers generating , containers installing module with the following command m pip install . processing building wheels for collected packages entrypoint running for entrypoint done stored in directory successfully built entrypoint installing collected packages entrypoint successfully installed you are using pip version however version is available. you should consider upgrading via the 'pip install upgrade pip' command. , containers no gpus detected , containers invoking user script training env , , algo gd b , hosts , hyperparameters , , true, pytorch , , algo gd b , entrypoint , eth , , , , environment variables invoking script with the following command m entrypoint coucou stdout , containers reporting training success exited with code aborting on container job complete as you see the coucou stdout has been printed, stderr has been ignored. in distant mode same result."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/88",
        "Issue_title":"renaming of mxnet-model-server in sagemaker-inference package 1.5.3 causing entrypoint with command `serve` to fail",
        "Issue_label":[
            "type: bug"
        ],
        "Issue_creation_time":1607044885000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n`sagemaker-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`.\r\n\r\nall containers defined in this repository install `sagemaker-inference` as a dependency of this repo itself, on lines\r\n\r\n```dockerfile\r\nRUN pip install --no-cache-dir \"sagemaker-pytorch-inference<2\"\r\n```\r\n\r\nand this repo's `setup.py` has an `install_requires` which includes `sagemaker-inference>=1.3.1`. as a result, `sagemaker-inference=1.5.3` installed.\r\n\r\nso while the `Dockerfile`'s `CMD` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `ENTRYPOINT` with `serve` as a build arg will fail with message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in <module>\r\n    serving.main()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 39, in main\r\n    _start_model_server()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f\r\n    return Retrying(*dargs, **dkw).call(f, *args, **kw)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call\r\n    return attempt.get(self._wrap_exception)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get\r\n    six.reraise(self.value[0], self.value[1], self.value[2])\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call\r\n    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 35, in _start_model_server\r\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/model_server.py\", line 94, in start_model_server\r\n    subprocess.Popen(multi_model_server_cmd)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'multi-model-server': 'multi-model-server'\r\n\r\n```\r\n\r\n**To reproduce**\r\n1. build any container\r\n1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model`\r\n1. `docker run [tag name] serve`\r\n\r\n**Expected behavior**\r\ntensorflow serving serves the mounted model \/ `inference.py`\r\n\r\n**System information**\r\nA description of your system. Please provide:\r\n- **Toolkit version**: 2.0.5, but should apply to all versions\r\n- **Framework version**: 1.4, but should apply to all versions\r\n- **Python version**: 3.7\r\n- **CPU or GPU**: cpu, but should apply to both\r\n- **Custom Docker image (Y\/N)**: N",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: renaming of mxnet-model-server in -inference package 1.5.3 causing entrypoint with command `serve` to fail; Content: **describe the bug**\r\n`-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`.\r\n\r\nall containers defined in this repository install `-inference` as a dependency of this repo itself, on lines\r\n\r\n```dockerfile\r\nrun pip install --no-cache-dir \"-pytorch-inference<2\"\r\n```\r\n\r\nand this repo's `setup.py` has an `install_requires` which includes `-inference>=1.3.1`. as a result, `-inference=1.5.3` installed.\r\n\r\nso while the `dockerfile`'s `cmd` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `entrypoint` with `serve` as a build arg will fail with message:\r\n\r\n```\r\ntraceback (most recent call last):\r\n  file \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in <module>\r\n    serving.main()\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_pytorch_serving_container\/serving.py\", line 39, in main\r\n    _start_model_server()\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f\r\n    return retrying(*dargs, **dkw).call(f, *args, **kw)\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call\r\n    return attempt.get(self._wrap_exception)\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get\r\n    six.reraise(self.value[0], self.value[1], self.value[2])\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call\r\n    attempt = attempt(fn(*args, **kwargs), attempt_number, false)\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_pytorch_serving_container\/serving.py\", line 35, in _start_model_server\r\n    model_server.start_model_server(handler_service=handler_service)\r\n  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_inference\/model_server.py\", line 94, in start_model_server\r\n    subprocess.popen(multi_model_server_cmd)\r\n  file \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__\r\n    restore_signals, start_new_session)\r\n  file \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nfilenotfounderror: [errno 2] no such file or directory: 'multi-model-server': 'multi-model-server'\r\n\r\n```\r\n\r\n**to reproduce**\r\n1. build any container\r\n1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model`\r\n1. `docker run [tag name] serve`\r\n\r\n**expected behavior**\r\ntensorflow serving serves the mounted model \/ `inference.py`\r\n\r\n**system information**\r\na description of your system. please provide:\r\n- **toolkit version**: 2.0.5, but should apply to all versions\r\n- **framework version**: 1.4, but should apply to all versions\r\n- **python version**: 3.7\r\n- **cpu or gpu**: cpu, but should apply to both\r\n- **custom docker image (y\/n)**: n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with renaming of mxnet-model-server in -inference package 1.5.3 causing entrypoint with command `serve` to fail.",
        "Issue_preprocessed_content":"Title: renaming of mxnet model server in inference package causing entrypoint with command to fail; Content: describe the bug recently released which included updating the name of the model server artifact and command from to . all containers defined in this repository install as a dependency of this repo itself, on lines and this repo's has an which includes . as a result, installed. so while the 's value will succeed, attempts to use the with as a build arg will fail with message to reproduce . build any container . mount a model and into . expected behavior tensorflow serving serves the mounted model \/ system a description of your system. please provide toolkit version but should apply to all versions framework version but should apply to all versions python version cpu or gpu cpu, but should apply to both custom docker image n"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Issue_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1659604135000,
        "Issue_closed_time":1662449543000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] highlight incorrect field in screenshot of importing  workspace; Content: **describe the bug**\r\na clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `autostopidletimeinminutes` also is required.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. go to '...'\r\n2. click on '....'\r\n3. scroll down to '....'\r\n4. see error\r\n\r\n**expected behavior**\r\na clear and concise description of what you expected to happen.\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**versions (please complete the following information):**\r\n - release version installed [e.g. v1.0.3]\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the incorrect field was highlighted in a screenshot of importing a workspace, and the field 'autostopidletimeinminutes' was also required.",
        "Issue_preprocessed_content":"Title: highlight incorrect field in screenshot of importing workspace; Content: describe the bug a clear and concise description of what the bug is. should highlight field the field also is required. to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/45",
        "Issue_title":"[Bug] In HongKong region, After user stop sagemaker workspace manually, web console show \"UNKNOWN\" status",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657604472000,
        "Issue_closed_time":1659958133000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Deploy SWB in hongkong reigon\r\n2. Create a Sagemaker workspace\r\n3. Click \"Stop\" button.\r\n5. workspace status show \"UNKNOWN\"\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] in hongkong region, after user stop  workspace manually, web console show \"unknown\" status; Content: **describe the bug**\r\na clear and concise description of what the bug is.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. deploy swb in hongkong reigon\r\n2. create a  workspace\r\n3. click \"stop\" button.\r\n5. workspace status show \"unknown\"\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the Hong Kong region where, after manually stopping a workspace, the web console showed an \"unknown\" status.",
        "Issue_preprocessed_content":"Title: in hongkong region, after user stop workspace manually, web console show unknown status; Content: describe the bug a clear and concise description of what the bug is. to reproduce steps to reproduce the behavior . deploy swb in hongkong reigon . create a workspace . click stop button. . workspace status show unknown"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Issue_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657451336000,
        "Issue_closed_time":1657702977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  template, after auto stoped, workspace env status is not updated; Content: **describe the bug**\r\nafter  workspace stopped automatically, workspace env status is not updated.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. set  workspace config's autostopidletimeinminutes as 10 minutes\r\n2. create  workspace and wait for more than 10 minutes,\r\n3. check  notebook instances to confirm the instance status is stopped\r\n4. check service workbench workspace status, it is still \"available\"\r\n\r\n**expected behavior**\r\n1. above step 4, workspace status should be \"stopped\"\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**versions (please complete the following information):**\r\n - release version installed [e.g. v1.0.3]\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the workspace environment status was not updated after the workspace was automatically stopped.",
        "Issue_preprocessed_content":"Title: template, after auto stoped, workspace env status is not updated; Content: describe the bug after workspace stopped automatically, workspace env status is not updated. to reproduce steps to reproduce the behavior . set workspace config's autostopidletimeinminutes as minutes . create workspace and wait for more than minutes, . check notebook instances to confirm the instance status is stopped . check service workbench workspace status, it is still available expected behavior . above step , workspace status should be stopped screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/36",
        "Issue_title":"Sagemaker dependencies",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1563873801000,
        "Issue_closed_time":1564216116000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Hi,\r\n\r\nGood day.\r\n\r\nCould you add to the Sagemaker section?:\r\n```\r\npip install urllib3==1.24.3\r\npip install PyYAML==3.13\r\npip install ipython\r\n```\r\nFrom https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. \r\n\r\nKeep up the good work!",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  dependencies; Content: hi,\r\n\r\ngood day.\r\n\r\ncould you add to the  section?:\r\n```\r\npip install urllib3==1.24.3\r\npip install pyyaml==3.13\r\npip install ipython\r\n```\r\nfrom https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. \r\n\r\nkeep up the good work!",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to install additional dependencies in order to train a DeepRacer model locally with GPU support.",
        "Issue_preprocessed_content":"Title: dependencies; Content: hi, good day. could you add to the section? from keep up the good work!"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Issue_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Issue_label":[
            "bug",
            "Usage"
        ],
        "Issue_creation_time":1561357678000,
        "Issue_closed_time":1562359564000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: when pretrained model is not found,  falls into an infinite silent loop; Content: i mistakenly put my model in pretrained folder but outside the model subfolder. in such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nwhile i did not fix the issue, i added logging to make it verbose. i will try to upload a patch.",
        "Issue_original_content_gpt_summary":"The user encountered an infinite silent loop when a pretrained model was not found, and added logging to make the issue more verbose.",
        "Issue_preprocessed_content":"Title: when pretrained model is not found, falls into an infinite silent loop; Content: i mistakenly put my model in pretrained folder but outside the model subfolder. in such case an exception is caught silently and then a sleep is called only to retry the exact behaviour. while i did not fix the issue, i added logging to make it verbose. i will try to upload a patch."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Issue_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Issue_label":[
            "bug",
            "docker images"
        ],
        "Issue_creation_time":1560810377000,
        "Issue_closed_time":1564215404000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: no tensorflow reported when trying to run nvidia image for ; steps to reproduce:\r\ni followed instructions in the readme, but instead of `docker pull nabcrr\/-rl-tensorflow:console` i did `docker pull nabcrr\/-rl-tensorflow:nvidia` and then tagged it as instructed. before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` i went to that file and commented out the line that lonon mentioned in #17 \r\n\r\nexpected result:\r\nwhen running `(cd rl_coach; Content: ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nactual result:\r\n```\r\nalgo-1-vrm2i_1  | error: ld.so: object '\/libchangehostname.so' from ld_preload cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | reporting training failure\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   file \"\/usr\/local\/lib\/python3.6\/dist-packages\/_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   file \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   file \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   file \"\/usr\/local\/lib\/python3.6\/dist-packages\/_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | modulenotfounderror: no module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | no module named 'tensorflow'\r\n```\r\n\r\nsystem info:\r\nubuntu 18.04.2 lts\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nmon jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| nvidia-smi 418.56       driver version: 418.56       cuda version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| gpu  name        persistence-m| bus-id        disp.a | volatile uncorr. ecc |\r\n| fan  temp  perf  pwr:usage\/cap|         memory-usage | gpu-util  compute m. |\r\n|===============================+======================+======================|\r\n|   0  geforce gtx 660m    off  | 00000000:01:00.0 n\/a |                  n\/a |\r\n| n\/a   46c    p8    n\/a \/  n\/a |    266mib \/  1999mib |     n\/a      default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| processes:                                                       gpu memory |\r\n|  gpu       pid   type   process name                             usage      |\r\n|=============================================================================|\r\n|    0                    not supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to run the NVIDIA image for TensorFlow, where the expected result of the GPU being detected and training beginning was not achieved due to an error with the TensorFlow module.",
        "Issue_preprocessed_content":"Title: no tensorflow reported when trying to run nvidia image for; Content: steps to reproduce i followed instructions in the readme, but instead of i did and then tagged it as instructed. before running i went to that file and commented out the line that lonon mentioned in expected result when running my gpu is detected and training begins actual result system ubuntu lts"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Issue_title":"Can not compile SageMaker examples",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1571075742000,
        "Issue_closed_time":1586730089000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can not compile  examples; Content: trying our your kubeflow\/ notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a pipeline compile error when trying out the Kubeflow\/notebook in their workshop.",
        "Issue_preprocessed_content":"Title: can not compile examples; Content: trying our your notebook in your workshop and received a pipeline compile error."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-solutions\/mlops-workload-orchestrator\/issues\/6",
        "Issue_title":"Error with sagemaker_layer in lambda \"create_sagemakermodel\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1620397656000,
        "Issue_closed_time":1620839615000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nWhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createModel\" when I run the pipeline from scratch. The error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**To Reproduce**\r\nThe only steps I took was to unfold it as it naturally comes. This bug prevented me from creating a sagemaker model for both the batch and realtime pipelines.\r\n\r\n**Expected behavior**\r\nThe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**Solution to that moment**\r\nTry to fix the numpy versions issue by re-creating the `sagemaker_layer` layer, via pip installation of the libraries. However, there were conflicts with other modified libraries at the time of `pip install numpy`. For this reason, I had to choose to use the default AWS library that comes with numpy \"AWSLambda-Python38-SciPy1x-v29\". For this, I had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create_sagemaker_model - I add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python38-SciPy1x:29\u201d\r\n\r\nWith this, I stop throwing that error at me. I think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. Please check if it still exists in the new versions.\r\n\r\n**Please complete the following information about the solution:**\r\n- [ ] Version: [e.g. v1.1.0]\r\n\r\n\r\nTo get the version of the solution, you can look at the description of the created CloudFormation stack. For example, \"(SO0136) - AWS MLOps Framework. Version v1.1.0\".\r\n\r\n- [ ] Region: [e.g. us-east-1]\r\n- [ ] Was the solution modified from the version published on this repository? No\r\n- [ ] If the answer to the previous question was yes, are the changes available on GitHub? -\r\n- [ ] Have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] Were there any errors in the CloudWatch Logs? Yes\r\n\r\n**Additional context**\r\nI am a Solution Architect of an advanced AWS partner company, and we are running a proof of concept with a real client.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error with _layer in lambda \"create_model\"; Content: **describe the bug**\r\nwhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createmodel\" when i run the pipeline from scratch. the error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**to reproduce**\r\nthe only steps i took was to unfold it as it naturally comes. this bug prevented me from creating a  model for both the batch and realtime pipelines.\r\n\r\n**expected behavior**\r\nthe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**solution to that moment**\r\ntry to fix the numpy versions issue by re-creating the `_layer` layer, via pip installation of the libraries. however, there were conflicts with other modified libraries at the time of `pip install numpy`. for this reason, i had to choose to use the default aws library that comes with numpy \"awslambda-python38-scipy1x-v29\". for this, i had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create__model - i add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:awslambda-python38-scipy1x:29\u201d\r\n\r\nwith this, i stop throwing that error at me. i think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. please check if it still exists in the new versions.\r\n\r\n**please complete the following information about the solution:**\r\n- [ ] version: [e.g. v1.1.0]\r\n\r\n\r\nto get the version of the solution, you can look at the description of the created cloudformation stack. for example, \"(so0136) - aws mlops framework. version v1.1.0\".\r\n\r\n- [ ] region: [e.g. us-east-1]\r\n- [ ] was the solution modified from the version published on this repository? no\r\n- [ ] if the answer to the previous question was yes, are the changes available on github? -\r\n- [ ] have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] were there any errors in the cloudwatch logs? yes\r\n\r\n**additional context**\r\ni am a solution architect of an advanced aws partner company, and we are running a proof of concept with a real client.",
        "Issue_original_content_gpt_summary":"The user encountered an error related to numpy in the lambda of \"createmodel\" when running the pipeline from scratch, which was solved by re-creating the `_layer` layer and using the default aws library that comes with numpy \"awslambda-python38-scipy1x-v29\".",
        "Issue_preprocessed_content":"Title: error with in lambda; Content: describe the bug when making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of createmodel when i run the pipeline from scratch. the error is to reproduce the only steps i took was to unfold it as it naturally comes. this bug prevented me from creating a model for both the batch and realtime pipelines. expected behavior the ideal and expected behavior is that this error does not occur and you can create the model. solution to that moment try to fix the numpy versions issue by re creating the layer, via pip installation of the libraries. however, there were conflicts with other modified libraries at the time of . for this reason, i had to choose to use the default aws library that comes with numpy awslambda python scipy x v . for this, i had to modify the code as follows in \/ i add the layer arn aws lambda us east layer awslambda python scipy x with this, i stop throwing that error at me. i think it is likely that due to library or version incompatibility issues, this error is by default in the mlops framework solution. please check if it still exists in the new versions. please complete the following about the solution version to get the version of the solution, you can look at the description of the created cloudformation stack. for example, so aws mlops framework. version region was the solution modified from the version published on this repository? no if the answer to the previous question was yes, are the changes available on github? have you checked your for the sevices this solution uses? were there any errors in the cloudwatch logs? yes additional context i am a solution architect of an advanced aws partner company, and we are running a proof of concept with a real client."
    },
    {
        "Issue_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Issue_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1619388470000,
        "Issue_closed_time":1623053107000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: with \" 2.31.1\", \".pytorch.pytorch\" needs to specify both \"framework_version\" and \"py_version\"; Content: in **moon_classification_solution.ipynb**, the original code below would cause an error `valueerror: framework_version or py_version was none, yet image_uri was also none. either specify both framework_version and py_version, or specify image_uri.` so i specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. or i guess just add `!pip install ==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/-deployment\/blob\/master\/mini-projects\/imdb%20sentiment%20analysis%20-%20xgboost%20(batch%20transform)%20-%20solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a pytorch wrapper\r\nfrom .pytorch import pytorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = pytorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    _session=_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they needed to specify both \"framework_version\" and \"py_version\" when using \".pytorch.pytorch\" in order to avoid an error.",
        "Issue_preprocessed_content":"Title: with needs to specify both and; Content: in the original code below would cause an error so i specified , cause the framework version only supports and , which fixed the problem. or i guess just add like notebooks in another would also solve the issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7040",
        "Issue_title":"[bug] Idempotency in kubeflow pipeline sagemaker component. ",
        "Issue_label":[
            "kind\/bug",
            "lifecycle\/stale",
            "area\/components\/aws\/sagemaker"
        ],
        "Issue_creation_time":1639174458000,
        "Issue_closed_time":null,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### What steps did you take\r\n\r\nIf node scales\/up down, the sagemaker component tries to create the same job which fails. Since sagemaker does not let create the same name job. Component controller should be able to detect this and resume the job from existing state. \r\n\r\n### What happened:\r\nthe job hangs\/fail \r\n\r\n### What did you expect to happen:\r\nI expect the job to resume from previous state. \r\n\r\n### Environment:\r\nkfp-1.6\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] idempotency in kubeflow pipeline  component. ; Content: ### what steps did you take\r\n\r\nif node scales\/up down, the  component tries to create the same job which fails. since  does not let create the same name job. component controller should be able to detect this and resume the job from existing state. \r\n\r\n### what happened:\r\nthe job hangs\/fail \r\n\r\n### what did you expect to happen:\r\ni expect the job to resume from previous state. \r\n\r\n### environment:\r\nkfp-1.6\r\n\r\n<!-- don't delete message below to encourage users to support your issue! -->\r\nimpacted by this bug? give it a \ud83d\udc4d. we prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with idempotency in a Kubeflow Pipeline component, where the job hangs\/fails if the node scales\/up down and the component tries to create the same job which fails due to not being able to create the same name job.",
        "Issue_preprocessed_content":"Title: idempotency in kubeflow pipeline component.; Content: what steps did you take if node down, the component tries to create the same job which fails. since does not let create the same name job. component controller should be able to detect this and resume the job from existing state. what happened the job what did you expect to happen i expect the job to resume from previous state. environment impacted by this bug? give it a . we prioritise the issues with the most ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465",
        "Issue_title":"[bug] Unhandled SageMaker training job status 'stopped' causing infinite loop",
        "Issue_label":[
            "help wanted",
            "kind\/bug",
            "good first issue",
            "area\/components\/aws\/sagemaker"
        ],
        "Issue_creation_time":1630204381000,
        "Issue_closed_time":null,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### What steps did you take\r\n\r\nCode gets stuck in infinite loop is SageMaker training job gets stopped (unhandled use case)\r\n\r\n### What happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/sagemaker_training_component.py#L57-L66\r\n\r\nAbove code only caters for training job status `Completed` or `Failed`, so if the training job status is marked as `Stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/sagemaker\/common\/sagemaker_component.py#L197-L201\r\n\r\n### What did you expect to happen:\r\n\r\nTraining job status `stopped` to be catered for\r\n\r\n### Environment:\r\n\r\n### Anything else you would like to add:\r\n\r\n\r\n### Labels\r\n<!-- Please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] unhandled  training job status 'stopped' causing infinite loop; Content: ### what steps did you take\r\n\r\ncode gets stuck in infinite loop is  training job gets stopped (unhandled use case)\r\n\r\n### what happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/\/train\/src\/_training_component.py#l57-l66\r\n\r\nabove code only caters for training job status `completed` or `failed`, so if the training job status is marked as `stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/\/common\/_component.py#l197-l201\r\n\r\n### what did you expect to happen:\r\n\r\ntraining job status `stopped` to be catered for\r\n\r\n### environment:\r\n\r\n### anything else you would like to add:\r\n\r\n\r\n### labels\r\n<!-- please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- don't delete message below to encourage users to support your issue! -->\r\nimpacted by this bug? give it a \ud83d\udc4d. we prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an infinite loop caused by an unhandled training job status of 'stopped' in the Kubeflow Pipelines code.",
        "Issue_preprocessed_content":"Title: unhandled training job status 'stopped' causing infinite loop; Content: what steps did you take code gets stuck in infinite loop is training job gets stopped what happened above code only caters for training job status or , so if the training job status is marked as , it causes an infinite loop in below code what did you expect to happen training job status to be catered for environment anything else you would like to add labels impacted by this bug? give it a . we prioritise the issues with the most ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Issue_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Issue_label":[
            "kind\/bug"
        ],
        "Issue_creation_time":1607683045000,
        "Issue_closed_time":1611093472000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: typeerror:  - deploy model() got an unexpected keyword argument 'update_endpoint'; Content: ### what steps did you take:\r\n[a clear and concise description of what the bug is.]\r\n\r\ni am use the re usable  components for building kubeflow pipelines.\r\n\r\n_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/train\/component.yaml')\r\n_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/model\/component.yaml')\r\n_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/deploy\/component.yaml')\r\n\r\nwhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = _deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='endpoint-price-prediction-model',\r\n        endpoint_config_name='endpointconfig-price-prediction-model',\r\n        update_endpoint=true,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### what happened:\r\ni am getting this error \r\ntypeerror:  - deploy model() got an unexpected keyword argument 'update_endpoint'\r\n\r\ni think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\ntraceback (most recent call last):\r\n--\r\n414 | file \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | file \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = _deploy_op(\r\n424 | typeerror:  - deploy model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### what did you expect to happen:\r\nto update the endpoint without any issue\r\n### environment:\r\n<!-- please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\n 2.1.0\r\n\r\nhow did you deploy kubeflow pipelines (kfp)?\r\n<!-- if you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nkfp version: <!-- if you are not sure, build commit shows on bottom of kfp ui left sidenav. -->\r\n\r\nkfp sdk version: <!-- please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### anything else you would like to add:\r\n[miscellaneous information that will assist in solving the issue.]\r\n\r\nplease help me out \r\n\r\n\/kind bug\r\n<!-- please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when attempting to update an existing endpoint using the Kubeflow Pipelines SDK, resulting in an unexpected keyword argument 'update_endpoint'.",
        "Issue_preprocessed_content":"Title: typeerror deploy model got an unexpected keyword argument; Content: what steps did you take a clear and concise description of what the bug i am use the re usable components for building kubeflow pipelines. when i am trying to update the endpoint that already exists piece of code i used to update the endpoint. deploy the pipeline prediction compiling the pipeline what happened i am getting this error typeerror deploy model file line , in file line , in compile file line , in workflow file line , in file line , in prediction typeerror deploy model ? kfp version kfp sdk version anything else you would like to add miscellaneous that will assist in solving the please help me out bug"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352",
        "Issue_title":"Want to create ANY model and do batch transform in without Training in Amazon SageMaker ",
        "Issue_label":[
            "kind\/bug",
            "status\/triaged"
        ],
        "Issue_creation_time":1597092939000,
        "Issue_closed_time":1632453568000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### What steps did you take: Removed the HPO and Training Jobs only creating the model and batch transforming in SageMaker \r\n[Automatically taking the HPO and Training on SageMaker facing some issue while kfp compile]\r\n\r\n### What happened: Getting output properly in Kubefow. But I want to to see custom  model (ANY) output without HPO and Model training in Sagemaker\r\n\r\n### What did you expect to happen: Without HPO and batch job in SageMaker\r\n\r\n\r\n\r\n\r\n### Anything else you would like to add:\r\nAny open source loan data model using KF would be appriciated \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: want to create any model and do batch transform in without training in  ; Content: ### what steps did you take: removed the hpo and training jobs only creating the model and batch transforming in  \r\n[automatically taking the hpo and training on  facing some issue while kfp compile]\r\n\r\n### what happened: getting output properly in kubefow. but i want to to see custom  model (any) output without hpo and model training in \r\n\r\n### what did you expect to happen: without hpo and batch job in \r\n\r\n\r\n\r\n\r\n### anything else you would like to add:\r\nany open source loan data model using kf would be appriciated \r\n\r\n\/kind bug\r\n<!-- please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges while trying to create a custom model and do batch transform in Kubeflow without training, and was unable to get the expected output.",
        "Issue_preprocessed_content":"Title: want to create any model and do batch transform in without training in; Content: what steps did you take removed the hpo and training jobs only creating the model and batch transforming in automatically taking the hpo and training on facing some issue while kfp compile what happened getting output properly in kubefow. but i want to to see custom model output without hpo and model training in what did you expect to happen without hpo and batch job in anything else you would like to add any open source loan data model using kf would be appriciated bug \/"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Issue_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Issue_label":[
            "help wanted",
            "kind\/bug",
            "status\/triaged"
        ],
        "Issue_creation_time":1588293591000,
        "Issue_closed_time":1589303399000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  custom training job error: unable to locate botocore.credentials; Content: ### what steps did you take:\r\ni run a custom image using the  training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/\/train\/component.yaml) and it ran fine. i am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nthe problem arises however if inside the custom script i use boto3 to manually download an object from s3 - then i get an error: unable to locate credentials ...  \r\n\r\n### what happened:\r\nbelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\ninfo:botocore.credentials:found credentials in environment variables.\r\ninfo:root:submitting training job to ...\r\ninfo:root:created training job with name: trainingjob-20200430232331-lphy\r\ninfo:root:training job in : \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/\/home?region=us-west-2#\/jobs\/trainingjob-20200430232331-lphy\r\ninfo:root:cloudwatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logstream:group=\/aws\/\/trainingjobs;prefix=trainingjob-20200430232331-lphy;streamfilter=typelogstreamprefix\r\ninfo:root:job request submitted. waiting for completion...\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training job is still in status: inprogress\r\ninfo:root:training failed with the following error: algorithmerror: exception during training: unable to locate credentials\r\ntraceback (most recent call last):\r\n  file \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  file \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  file \"\/opt\/conda\/lib\/python3.7\/site-packag\r\ntraceback (most recent call last):\r\n  file \"train.py\", line 81, in <module>\r\n    main()\r\n  file \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  file \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise exception('training job failed')\r\nexception: training job failed\r\n```\r\n\r\n### what did you expect to happen:\r\ni would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### environment:\r\nhow did you deploy kubeflow pipelines (kfp)?\r\ni deployed kubeflow pipelines as part of my kubeflow deployment on aws eks:\r\n\r\nkfp version: \r\nbuild commit: 743746b\r\n\r\nkfp sdk version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to use boto3 to manually download an object from s3 while running a custom image using the training operator in Kubeflow Pipelines on AWS EKS, despite the credentials being found in environment variables.",
        "Issue_preprocessed_content":"Title: custom training job error unable to locate; Content: what steps did you take i run a custom image using the training operator and it ran fine. i am using and the objects from s are being correctly copied over to the specified local channel path. the problem arises however if inside the custom script i use boto to manually download an object from s then i get an error unable to locate credentials what happened below is a copy of the component's logs notice the very first log statement says that the boto credentials are found in environment variables but somehow they never make their way to the boto client that is instantiated inside the custom image what did you expect to happen i would have expected the credentials to be passed to the image that the training operator is running but it is not the case environment how did you deploy kubeflow pipelines ? i deployed kubeflow pipelines as part of my kubeflow deployment on aws eks kfp version build commit b kfp sdk version bug"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Issue_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Issue_label":[
            "priority\/p1",
            "platform\/other",
            "platform\/aws",
            "kind\/bug",
            "area\/samples"
        ],
        "Issue_creation_time":1558527534000,
        "Issue_closed_time":1563269566000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":13.0,
        "Issue_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kubeflow-pipeline running with  throws an error passing k-mean and feature_dim parameters; Content: hi, \r\n\r\ni have copied the git code for  to execute through the kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-\/mnist-classification-pipeline.py\r\n\r\nwhile executing the kubeflow pipeline, i am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\ntraining failed with the following error: clienterror: no value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by validationerror)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='mnist classification pipeline',\r\n    description='mnist classification using kmeans in '\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nplease let me know why this error is appeared and how should it get resolved ?\r\n\r\nregards,\r\nvarun\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error while running a kubeflow pipeline, which was caused by the lack of specified values for the required hyperparameters 'k' and 'feature_dim'.",
        "Issue_preprocessed_content":"Title: kubeflow pipeline running with throws an error passing k mean and parameters; Content: hi, i have copied the git code for to execute through the kubeflow pipeline while executing the kubeflow pipeline, i am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define. error training failed with the following error clienterror no value were specified for 'k', which are required hyperparameter pipeline parameters are name 'mnist classification pipeline', description 'mnist classification using kmeans in ' def please let me know why this error is appeared and how should it get resolved ? regards, varun"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Issue_title":"[bug] Sagemaker Remote Test reporting issues",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600043448000,
        "Issue_closed_time":1626207887000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  remote test reporting issues; Content: checklist\r\n- [x] i've prepended issue tag with type of change: [bug]\r\n- [ ] (if applicable) i've attached the script to reproduce the bug\r\n- [ ] (if applicable) i've documented below the dlc image\/dockerfile this relates to\r\n- [ ] (if applicable) i've documented below the tests i've run on the dlc image\r\n- [ ] i'm using an existing dlc image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] i've built my own container based off dlc (and i've attached the code used to build my own image)\r\n\r\n*concise description:*\r\nsm remote test log doesn't get reported correctly.\r\n\r\nobserved in 2 commits of the pr: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*dlc image\/dockerfile:*\r\nmx 1.6 dlc\r\n\r\n*current behavior:*\r\ngithub shows \"pending\" status.\r\ncodebuild logs show \"failed\" status.\r\nhowever, actual codebuild logs doesn't bear failure log. it terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 i \/ gw1 i \/ gw2 i \/ gw3 i \/ gw4 i \/ gw5 i \/ gw6 i \/ gw7 i\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nsm-cloudwatch log\r\nnavigating to the appropriate sm training log shows that the job ran for 2 hours and ended successfully. it says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 -training-toolkit info     reporting training success\r\n```\r\n\r\n*expected behavior:*\r\n\r\n1. pr commit status should say failed if codebuild log says failed\r\n2. codebuild log should not abruptly hang. it should print out the error. currently it just terminates after printing some logs post session start.\r\n\r\n*additional context:*\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges with remote test reporting issues, where the GitHub PR commit status said \"failed\" while the CodeBuild log said \"failed\" but did not provide any additional information, and the SM-Cloudwatch log showed that the job had run for two hours and ended successfully.",
        "Issue_preprocessed_content":"Title: remote test reporting issues; Content: checklist i've prepended issue tag with type of change i've attached the script to reproduce the bug i've documented below the dlc this relates to i've documented below the tests i've run on the dlc image i'm using an existing dlc image listed here i've built my own container based off dlc concise description sm remote test log doesn't get reported correctly. observed in commits of the pr dlc mx dlc current behavior github shows pending status. codebuild logs show failed status. however, actual codebuild logs doesn't bear failure log. it terminates abruptly. sm cloudwatch log navigating to the appropriate sm training log shows that the job ran for hours and ended successfully. it says expected behavior . pr commit status should say failed if codebuild log says failed . codebuild log should not abruptly hang. it should print out the error. currently it just terminates after printing some logs post session start. additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Issue_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1597888000000,
        "Issue_closed_time":1598551848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] apt-get failure in -local-test builds; Content: \r\n*description:*\r\n\r\nan apt-get error is seen in `-local-test` builds as below. this is because `apt-get` process is already running and in active state.\r\n\r\n```\r\ne: could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: resource temporarily unavailable)\r\n--\r\n294 | e: unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with apt-get failing in -local-test builds due to an active process already using the dpkg frontend lock.",
        "Issue_preprocessed_content":"Title: apt get failure in local test builds; Content: description an apt get error is seen in builds as below. this is because process is already running and in active state."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1650",
        "Issue_title":"[BUG] Unable to train on multiple GPUs in Sagemaker Notebook Terminal",
        "Issue_label":[
            "bug",
            "module: text",
            "Needs Triage"
        ],
        "Issue_creation_time":1649258575000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"- [ X] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\nAutogluon 0.4.0 TextPredictor training on p3.8xl 4-GPU instance in Sagemaker Notebook terminal, with `env.num_gpus: 4` setting.  I get an error in spawning multiprocessing.  When I train with everything the same, but only on a single GPU within the same instance and setup, it trains without a problem.\r\n\r\n**Expected behavior**\r\nTrain across all 4 GPUs in the p3.8xl instance with no errors.\r\n\r\n**To Reproduce**\r\n* SageMaker Notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\nCode:\r\nin `train.py` file\r\n```from argparse import Namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = TextPredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**Screenshots**\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/ec2-user\/SageMaker\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\nIf you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nOS                   : Linux\r\nOS-release           : 4.14.252-131.483.amzn1.x86_64\r\nVersion              : #1 SMP Mon Nov 1 20:48:11 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : None\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] unable to train on multiple gpus in  notebook terminal; Content: - [ x] i have checked that this bug exists on the latest stable version of autogluon\r\n- [ ] and\/or i have checked that this bug exists on the latest mainline of autogluon via source installation\r\n\r\n**describe the bug**\r\nautogluon 0.4.0 textpredictor training on p3.8xl 4-gpu instance in  notebook terminal, with `env.num_gpus: 4` setting.  i get an error in spawning multiprocessing.  when i train with everything the same, but only on a single gpu within the same instance and setup, it trains without a problem.\r\n\r\n**expected behavior**\r\ntrain across all 4 gpus in the p3.8xl instance with no errors.\r\n\r\n**to reproduce**\r\n*  notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\ncode:\r\nin `train.py` file\r\n```from argparse import namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import textpredictor\r\n\r\nargs = namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = textpredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**screenshots**\r\nerror:\r\n```\r\ntraceback (most recent call last):\r\n  file \"<string>\", line 1, in <module>\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/home\/ec2-user\/\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=false)\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._popen(self)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _popen\r\n    return popen(process_obj)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nruntimeerror: \r\n        an attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        this probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        the \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**installed versions**\r\nwhich version of autogluon are you are using?  \r\nif you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\ninstalled versions\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nos                   : linux\r\nos-release           : 4.14.252-131.483.amzn1.x86_64\r\nversion              : #1 smp mon nov 1 20:48:11 utc 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: none\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\npil                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : none\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : none\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to train Autogluon 0.4.0 TextPredictor on a p3.8xl 4-GPU instance in a notebook terminal, resulting in an error in spawning multiprocessing.",
        "Issue_preprocessed_content":"Title: unable to train on multiple gpus in notebook terminal; Content: i have checked that this bug exists on the latest stable version of autogluon i have checked that this bug exists on the latest mainline of autogluon via source installation describe the bug autogluon textpredictor training on gpu instance in notebook terminal, with setting. i get an error in spawning multiprocessing. when i train with everything the same, but only on a single gpu within the same instance and setup, it trains without a problem. expected behavior train across all gpus in the instance with no errors. to reproduce notebook instance python pip install awswrangler pandas python code in file screenshots error installed versions which version of autogluon are you are using? if you are using and newer, please run the following code snippet additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1644",
        "Issue_title":"[BUG] SageMaker endpoint appears unable to load model file \/ use image paths as features",
        "Issue_label":[
            "bug",
            "env: sagemaker",
            "Needs Triage"
        ],
        "Issue_creation_time":1648949150000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"- [x] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\n```python\r\n...\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\nIt appears that the SageMaker endpoint isn't able to find \/ open the model file. I was able to use the example code in the tutorial [Deploying AutoGluon Models with AWS SageMaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html) and managed to deploy an endpoint to SageMaker. But I get this error when I go to make predictions with test data. I wonder if this might be related to transition from MXNet to Pytorch and how their artifacts are typically stored? I'm using `v0.4.0` but the predictor object is `sagemaker.mxnet.model.MXNetPredictor`. This discrepancy in framework seems supported be a related error that I found in a GitHub issue [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1238).\r\n\r\nNote also that I am attempting to adapt the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset), because I'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the SageMaker endpoint. \r\n\r\nHere's the full traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nModelError                                Traceback (most recent call last)\r\n<ipython-input-51-abf97eb84e0a> in <module>\r\n----> 1 predictions = predictor.predict(test_data[:1].values)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\n    159             data, initial_args, target_model, target_variant, inference_id\r\n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\n    162         return self._handle_response(response)\r\n    163 \r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    389                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    390             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 391             return self._make_api_call(operation_name, kwargs)\r\n    392 \r\n    393         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    717             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    718             error_class = self.exceptions.from_code(error_code)\r\n--> 719             raise error_class(parsed_response, operation_name)\r\n    720         else:\r\n    721             return parsed_response\r\n\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 110, in transform\r\n    self.validate_and_initialize(model_dir=model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 158, in validate_and_initialize\r\n    self._model = self._model_fn(model_dir)\r\n  File \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn\r\n    model = TabularPredictor.load(model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load\r\n    predictor = cls._load(path=path)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load\r\n    predictor: TabularPredictor = load_pkl.load(path=path + cls.predictor_file_name)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load\r\n    with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin:\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**To Reproduce**\r\n\r\n1. Train a multi modal model, using code adapted from [Multimodal Data Tables: Tabular, Text, and Image Tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a)\r\n2. Deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189)\r\n\r\n**Screenshots**\r\nNA\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\n`0.4.0`\r\n<details>\r\n\r\n```python\r\n# Replace this code with the output of the following:\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-02\r\ntime                 : 19:45:52.692253\r\npython               : 3.9.7.final.0\r\nOS                   : Linux\r\nOS-release           : 5.4.0-66-generic\r\nVersion              : #74~18.04.2-Ubuntu SMP Fri Feb 5 11:17:31 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 12\r\ncpu_ram_mb           : 64324\r\ncuda version         : None\r\nnum_gpus             : 0\r\ngpu_ram_mb           : []\r\navail_disk_size_mb   : 289302\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.tabular    : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon.vision     : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.21\r\ncatboost             : 1.0.4\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nfastai               : 2.5.3\r\ngluoncv              : 0.11.0\r\nlightgbm             : 3.3.2\r\nmatplotlib           : 3.5.1\r\nnetworkx             : 2.7.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.22.3\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : 1.8.0\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorch                : 1.10.1+cpu\r\ntorchmetrics         : 0.7.2\r\ntqdm                 : 4.63.0\r\ntransformers         : 4.16.2\r\nxgboost              : 1.4.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\n\r\nI am attempting to follow the [tutorial to deploy a model via Sagemaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html), however, adapting to use the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset).\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  endpoint appears unable to load model file \/ use image paths as features; Content: - [x] i have checked that this bug exists on the latest stable version of autogluon\r\n- [ ] and\/or i have checked that this bug exists on the latest mainline of autogluon via source installation\r\n\r\n**describe the bug**\r\n```python\r\n...\r\nmodelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received server error (500) from primary with message \"[errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl'\r\n...\r\nfilenotfounderror: [errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\nit appears that the  endpoint isn't able to find \/ open the model file. i was able to use the example code in the tutorial [deploying autogluon models with ](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws--deployment.html) and managed to deploy an endpoint to . but i get this error when i go to make predictions with test data. i wonder if this might be related to transition from mxnet to pytorch and how their artifacts are typically stored? i'm using `v0.4.0` but the predictor object is `.mxnet.model.mxnetpredictor`. this discrepancy in framework seems supported be a related error that i found in a github issue [here](https:\/\/github.com\/aws\/amazon--examples\/issues\/1238).\r\n\r\nnote also that i am attempting to adapt the example model trained in the [multi-modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the petfinder dataset), because i'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the  endpoint. \r\n\r\nhere's the full traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nmodelerror                                traceback (most recent call last)\r\n<ipython-input-51-abf97eb84e0a> in <module>\r\n----> 1 predictions = predictor.predict(test_data[:1].values)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\n    159             data, initial_args, target_model, target_variant, inference_id\r\n    160         )\r\n--> 161         response = self._session._runtime_client.invoke_endpoint(**request_args)\r\n    162         return self._handle_response(response)\r\n    163 \r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    389                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    390             # the \"self\" in this scope is referring to the baseclient.\r\n--> 391             return self._make_api_call(operation_name, kwargs)\r\n    392 \r\n    393         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    717             error_code = parsed_response.get(\"error\", {}).get(\"code\")\r\n    718             error_class = self.exceptions.from_code(error_code)\r\n--> 719             raise error_class(parsed_response, operation_name)\r\n    720         else:\r\n    721             return parsed_response\r\n\r\nmodelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received server error (500) from primary with message \"[errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl'\r\ntraceback (most recent call last):\r\n  file \"\/usr\/local\/lib\/python3.8\/dist-packages\/_inference\/transformer.py\", line 110, in transform\r\n    self.validate_and_initialize(model_dir=model_dir)\r\n  file \"\/usr\/local\/lib\/python3.8\/dist-packages\/_inference\/transformer.py\", line 158, in validate_and_initialize\r\n    self._model = self._model_fn(model_dir)\r\n  file \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn\r\n    model = tabularpredictor.load(model_dir)\r\n  file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load\r\n    predictor = cls._load(path=path)\r\n  file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load\r\n    predictor: tabularpredictor = load_pkl.load(path=path + cls.predictor_file_name)\r\n  file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load\r\n    with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin:\r\nfilenotfounderror: [errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\n**expected behavior**\r\na clear and concise description of what you expected to happen.\r\n\r\n**to reproduce**\r\n\r\n1. train a multi modal model, using code adapted from [multimodal data tables: tabular, text, and image tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a)\r\n2. deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189)\r\n\r\n**screenshots**\r\nna\r\n\r\n**installed versions**\r\nwhich version of autogluon are you are using?  \r\n`0.4.0`\r\n<details>\r\n\r\n```python\r\n# replace this code with the output of the following:\r\ninstalled versions\r\n------------------\r\ndate                 : 2022-04-02\r\ntime                 : 19:45:52.692253\r\npython               : 3.9.7.final.0\r\nos                   : linux\r\nos-release           : 5.4.0-66-generic\r\nversion              : #74~18.04.2-ubuntu smp fri feb 5 11:17:31 utc 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 12\r\ncpu_ram_mb           : 64324\r\ncuda version         : none\r\nnum_gpus             : 0\r\ngpu_ram_mb           : []\r\navail_disk_size_mb   : 289302\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.tabular    : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon.vision     : 0.4.0\r\nautogluon_contrib_nlp: none\r\nboto3                : 1.21.21\r\ncatboost             : 1.0.4\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nfastai               : 2.5.3\r\ngluoncv              : 0.11.0\r\nlightgbm             : 3.3.2\r\nmatplotlib           : 3.5.1\r\nnetworkx             : 2.7.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.22.3\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\npil                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : 1.8.0\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : none\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorch                : 1.10.1+cpu\r\ntorchmetrics         : 0.7.2\r\ntqdm                 : 4.63.0\r\ntransformers         : 4.16.2\r\nxgboost              : 1.4.2\r\n```\r\n\r\n<\/details>\r\n\r\n**additional context**\r\n\r\ni am attempting to follow the [tutorial to deploy a model via ](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws--deployment.html), however, adapting to use the example model trained in the [multi-modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the petfinder dataset).\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug when attempting to deploy a multi-modal model to , where the endpoint was unable to find\/open the model file, potentially due to a discrepancy in framework between MXNet and PyTorch.",
        "Issue_preprocessed_content":"Title: endpoint appears unable to load model file \/ use image paths as features; Content: i have checked that this bug exists on the latest stable version of autogluon i have checked that this bug exists on the latest mainline of autogluon via source installation describe the bug it appears that the endpoint isn't able to find \/ open the model file. i was able to use the example code in the tutorial and managed to deploy an endpoint to . but i get this error when i go to make predictions with test data. i wonder if this might be related to transition from mxnet to pytorch and how their artifacts are typically stored? i'm using but the predictor object is . this discrepancy in framework seems supported be a related error that i found in a github issue . note also that i am attempting to adapt the example model trained in the , because i'm ultimately try to deploy a multi modal model and figure out how to pass to the endpoint. here's the full traceback expected behavior a clear and concise description of what you expected to happen. to reproduce . train a multi modal model, using code adapted from . deploy the pet finder model screenshots na installed versions which version of autogluon are you are using? additional context i am attempting to follow the , however, adapting to use the example model trained in the ."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1551",
        "Issue_title":"How to make plot_ensemble_model() work in sagemaker (or any jupyter based env)",
        "Issue_label":[
            "bug",
            "help wanted",
            "question"
        ],
        "Issue_creation_time":1645398079000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? I'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure)\r\n\r\nI've tried the following code without success: \r\n%pip install python3-dev\r\n%pip install graphviz\r\n%pip install libgraphviz-dev\r\n%pip install pkg-config\r\n\r\n%pip install pygraphviz\r\n\r\n\r\nThanks for the help!",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: how to make plot_ensemble_model() work in  (or any jupyter based env); Content: has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? i'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure)\r\n\r\ni've tried the following code without success: \r\n%pip install python3-dev\r\n%pip install graphviz\r\n%pip install libgraphviz-dev\r\n%pip install pkg-config\r\n\r\n%pip install pygraphviz\r\n\r\n\r\nthanks for the help!",
        "Issue_original_content_gpt_summary":"The user is encountering difficulty installing pygraphviz in order to make plot_ensemble_model() work in a jupyter based environment.",
        "Issue_preprocessed_content":"Title: how to make work in; Content: has anyone figured out an easy way to make work in jupyter based environments? i'm having a lot of difficulty installing pygraphviz i've tried the following code without success %pip install python dev %pip install graphviz %pip install libgraphviz dev %pip install pkg config %pip install pygraphviz thanks for the help!"
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Issue_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1580947939000,
        "Issue_closed_time":1613263024000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: importerror for tabularprediction in  notebook instance; Content: i got **importerror** when trying to use autogluon in a  instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nthe error i got is:\r\n```\r\nfrom autogluon import tabularprediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nimporterror                               traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import tabularprediction as task\r\n\r\nimporterror: cannot import name 'tabularprediction'\r\n```\r\n\r\nif i try\r\n```\r\nimport autogluon as ag\r\nag.tabularprediction.dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nattributeerror                            traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.tabularprediction.dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nattributeerror: module 'autogluon' has no attribute 'tabularprediction'\r\n```\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an ImportError and AttributeError when trying to use AutoGluon's tabularprediction module in a notebook instance with a Conda Python 3 kernel.",
        "Issue_preprocessed_content":"Title: importerror for tabularprediction in notebook instance; Content: i got importerror when trying to use autogluon in a instance , with kernel being the error i got is if i try for your reference i installed autogluon by using version pip in the notebook as usual. there are errors in the second installation step error has requirement but you'll have boto which is incompatible. error awscli has requirement but you'll have botocore which is"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Issue_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600235220000,
        "Issue_closed_time":1600271697000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: issue with installing glounts on  notebook instance from github; Content: ## description\r\ni am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/gluonts__sdk_tutorial.ipynb but at first step when i ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` i got the following error,\r\n\r\n## error message or code output\r\n```obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  updating .\/src\/gluonts clone\r\n  running command git fetch -q --tags\r\n  running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  installing build dependencies ... error\r\n  error: command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: none\r\n  complete output (14 lines):\r\n  traceback (most recent call last):\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class regexflag(enum.intflag):\r\n  attributeerror: module 'enum' has no attribute 'intflag'\r\n  ----------------------------------------\r\nerror: command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## environment\r\nnote: previously, i installed gluon-ts (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if i do `! pip list` i can see the package is installed but when i ran `!pip uninstall glounts` it says `warning: skipping glounts as it is not installed.`\r\n\r\n- operating system:  notebook instance with conda_mxnet_p36 kernel.\r\n- python version: 3.6\r\n- gluonts version: 0.5.2 is already installed.\r\n- mxnet version:1.6",
        "Issue_original_content_gpt_summary":"The user encountered an issue with installing glounts on a notebook instance from github, resulting in an error message due to an attributeerror with the 'enum' module.",
        "Issue_preprocessed_content":"Title: issue with installing glounts on notebook instance from github; Content: description i am following the instructions on but at first step when i ran i got the following error, error message or code output environment note previously, i installed gluon ts using and if i do i can see the package is installed but when i ran it says operating system notebook instance with kernel. python version gluonts version is already installed. mxnet"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Issue_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1573166280000,
        "Issue_closed_time":1573208758000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: problems using gpu with  on aws-instance \"ml.p2.xlarge\".; Content: ## description\r\nusing  on an aws gpu-instance \"ml.p2.xlarge\", i was not able to run the example `benchmark_m4.py` script (copy\/pasted in ) on gpu. \r\n\r\n## to reproduce\r\nafter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nnext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        deeparestimator,\r\n        trainer=trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on cpu)\r\n\r\n## error message\r\n\r\n```keyerror                                  traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"mase\",\r\n     89             \"smape\",\r\n---> 90             \"msis\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=true)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # when setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": true if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise keyerror(\r\n   1176                     \"none of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nkeyerror: \"none of [index(['dataset', 'estimator', 'rmse', 'mean_wquantileloss', 'mase', 'smape',\\n       'msis'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to run the example script 'benchmark_m4.py' on a GPU-instance \"ml.p2.xlarge\" on AWS, resulting in a KeyError.",
        "Issue_preprocessed_content":"Title: problems using gpu with on aws instance; Content: description using on an aws gpu instance i was not able to run the example script on gpu. to reproduce after starting the instance next cell paste the slightly modified script with a little modification without specifying the context this works fine, but is only running on cpu error message other in addition, before installing gluonts after installing gluonts environment , running on aws instance gluonts version installed using pip. kernel"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Issue_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Issue_label":[
            "Issue: Bug Report \ud83d\udc1e"
        ],
        "Issue_creation_time":1585713762000,
        "Issue_closed_time":1585791347000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  notebooks raise error for `pandas.csvdataset`; ## description\r\nthe conda environment for python3.6 in notebooks cannot find `pandas.csvdataset`\r\n\r\n## context\r\ni'm wanting to use  as my development environment. however, i cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## steps to reproduce\r\n\r\n0. startup a  instance with defaults\r\n\r\nterminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => success!\r\n\r\nnotebook fail:\r\n1. create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> the environments for the terminal and notebooks are separate by design in \r\n2. load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> note that i've started to use the code below; Content: without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # check it exists first. for some reason this is not an idempotent operation?\r\n    current_dir = path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. run `context.catalog.list()`\r\n\r\n## expected result\r\nthe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## actual result\r\n```\r\nclass `pandas.csvdataset` not found.\r\n```\r\n\r\nfull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nstopiteration                             traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not none)\r\n    418         except stopiteration:\r\n\r\nstopiteration: \r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\ndataseterror                              traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except stopiteration:\r\n--> 419             raise dataseterror(\"class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\ndataseterror: class `pandas.csvdataset` not found.\r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\ndataseterror                              traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = abstractdataset.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise dataseterror(\r\n    153                 \"an exception occurred when parsing config \"\r\n--> 154                 \"for dataset `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\ndataseterror: an exception occurred when parsing config for dataset `example_iris_data`:\r\nclass `pandas.csvdataset` not found.\r\n```\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the conda environment for python3.6 in notebooks could not find `pandas.csvdataset`, resulting in an error when running `context.catalog.list()`.",
        "Issue_preprocessed_content":"Title: notebooks raise error for; Content: description the conda environment for in notebooks cannot find context i'm wanting to use as my development environment. however, i cannot get kedro to run as expected in both the notebooks and the terminal . steps to reproduce . startup a instance with defaults terminal success . in the terminal . a. for name b. for example project . > success! notebook fail . create a new notebook in . in a notebook > the environments for the terminal and notebooks are separate by design in . load the kedro context as described > note that i've started to use the code below; without checking if exists, you need to restart the kernel if you want to reload the context as something in the last lines of code causes the next invocation of to point to the root dir not , as intended. . run expected result the notebook should print actual result full trace. investigations so far upon changing the yaml type for from to , we get success on both the terminal and the notebook. however, this is not my desired outcome; the transition to using makes it easier, for me at least, to use both s and local datasets. output from notebook output from terminal your environment include as many relevant details about the environment in which you experienced the bug environment terminal notebook kedro, version kedro, version python anaconda, inc. python anaconda, os"
    },
    {
        "Issue_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Issue_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Issue_label":[
            "bug",
            "priority:medium"
        ],
        "Issue_creation_time":1620111416000,
        "Issue_closed_time":1623682749000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: getting an error from `aws__notebook_instance` table. please see the detail below.; Content: **describe the bug**\r\n\r\ncreate a notebook instance in one of the configured region.\r\nran the below query and got that error\r\n\r\n```\r\nselect * from aws__notebook_instance;\r\nerror: hydrate call listawsnotebookinstancetags failed with panic interface conversion: interface {} is *.notebookinstancesummary, not *.describenotebookinstanceoutput\r\n\r\n```\r\n\r\n\r\n\r\n**steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when running a query on the `aws__notebook_instance` table using Steampipe v0.4.1 and the AWS plugin v0.15.0.",
        "Issue_preprocessed_content":"Title: getting an error from table. please see the detail below.; Content: describe the bug create a notebook instance in one of the configured region. ran the below query and got that error steampipe version plugin version aws"
    },
    {
        "Issue_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Issue_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657726485000,
        "Issue_closed_time":1657782683000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]:  + s3 artifact store fails trying to create a new bucket; Content: ### what happened?\n\nzenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### reproduction steps\n\n1. create a  pipeline.\r\n2. create a s3 artifact store.\r\n3. run the pipeline\r\n\n\n### relevant log output\n\n```shell\ncreating run for pipeline: mnist_pipeline\r\ncache enabled for pipeline mnist_pipeline\r\nusing stack _stack to run pipeline mnist_pipeline...\r\nstep importer has started.\r\nusing cached version of importer.\r\nstep importer has finished in 0.045s.\r\nstep trainer has started.\r\ninfo:botocore.credentials:found credentials in shared credentials file: ~\/.aws\/credentials\r\nparamvalidationerror: parameter validation failed:\r\ninvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nbucket name must match the regex \"^[a-za-z0-9.\\-_]{1,255}$\" or be an arn \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-za-\r\nz0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-za\r\n-z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-za-z0-9\\-]{1,63}$\"\r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\nvalueerror: bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': parameter \r\nvalidation failed:\r\ninvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nbucket name must match the regex \"^[a-za-z0-9.\\-_]{1,255}$\" or be an arn \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-za-\r\nz0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-za\r\n-z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-za-z0-9\\-]{1,63}$\"\n```\n\n\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where zenml was failing to create a s3 bucket due to an incorrect regex in its name.",
        "Issue_preprocessed_content":"Title: + s artifact store fails trying to create a new bucket; Content: contact details system zenml what happened? zenml is trying to create a s bucket and fails due to incorrect regex in its name. reproduction steps . create a pipeline. . create a s artifact store. . run the pipeline relevant log output code of conduct i agree to follow this project's code of conduct"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1089",
        "Issue_title":"[Bug] study fail to mount in SWB 5.2.6 SageMaker Jupyter Notebook",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1671836610000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nSWB 5.2.6 version - SageMaker jupyter notebook workspace can not mount a study.  see error in \/var\/log\/message\r\n\/usr\/local\/bin\/goofys[7248]: main.FATAL Mounting file system: Mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $PATH#012#012stderr:\r\n\r\nLooks like the FUSE package failed to install during on-start.  if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. \r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n - Is the deployment from a forked version of the repository?\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] study fail to mount in swb 5.2.6  jupyter notebook; Content: **describe the bug**\r\nswb 5.2.6 version -  jupyter notebook workspace can not mount a study.  see error in \/var\/log\/message\r\n\/usr\/local\/bin\/goofys[7248]: main.fatal mounting file system: mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $path#012#012stderr:\r\n\r\nlooks like the fuse package failed to install during on-start.  if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. \r\n\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. go to '...'\r\n2. click on '....'\r\n3. scroll down to '....'\r\n4. see error\r\n\r\n**expected behavior**\r\na clear and concise description of what you expected to happen.\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**versions (please complete the following information):**\r\n - release version installed [e.g. v1.0.3]\r\n - is the deployment from a forked version of the repository?\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with mounting a study in swb 5.2.6 jupyter notebook, which was resolved by running \"sudo yum install fuse\" and then running \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json.",
        "Issue_preprocessed_content":"Title: study fail to mount in swb jupyter notebook; Content: describe the bug swb version jupyter notebook workspace can not mount a study. see error in mounting file system mount mount running fusermount exec fusermount executable file not found in $path stderr looks like the fuse package failed to install during on start. if run sudo yum install fuse then you can run to mount the study. to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. versions release version installed is the deployment from a forked version of the repository? additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Issue_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Issue_label":[
            "bug",
            "Review One",
            "Review Two"
        ],
        "Issue_creation_time":1670601495000,
        "Issue_closed_time":1671209314000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] more descriptive error message for \"null is not an object\" while trying to connect to  notebook. ; Content: **describe the bug**\r\nusers get the error \"null is not an object\" when pop-ups are enabled in swb (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nthis error is illegible to the user and causes confusion. can we make the error message more clear such as:\r\n\"service workbench is encountering an error showing content. please enable pop-ups and refresh the page.\"\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. diable pop-ups \r\n2. connect to a workspace\r\n\r\n**expected behavior**\r\nif the workspace is unable to open, a more legible error message should be shown, such as \"service workbench is encountering an error showing content. please enable pop-ups and refresh the page.\"\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**versions (please complete the following information):**\r\n - release version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encounters an illegible error message when trying to connect to a workspace, and is unable to open the workspace due to pop-ups being disabled, requiring a more legible error message to be shown.",
        "Issue_preprocessed_content":"Title: more descriptive error message for null is not an object while trying to connect to notebook.; Content: describe the bug users get the error null is not an object when pop ups are enabled in swb this error is illegible to the user and causes confusion. can we make the error message more clear such as service workbench is encountering an error showing content. please enable pop ups and refresh the to reproduce steps to reproduce the behavior . diable pop ups . connect to a workspace expected behavior if the workspace is unable to open, a more legible error message should be shown, such as service workbench is encountering an error showing content. please enable pop ups and refresh the screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Issue_title":"[Bug] Sagemaker instance does not stop automatically",
        "Issue_label":[
            "bug",
            "Review One",
            "Review Two"
        ],
        "Issue_creation_time":1670370218000,
        "Issue_closed_time":1671059684000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  instance does not stop automatically; Content: **describe the bug**\r\n\r\nidle  notebook instances do not stop after specified time.\r\n\r\nswb runs autostop.py script to automatically stop  notebook instance. the script is used by `on-start` lifecycle rule of the instance cfn template. according to lifecycleconfigonstart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. make sure autostopidletimeinminutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. create a new workspace with  notebook instance\r\n3. leave the instance idle for the time specified (autostopidletimeinminutes )\r\n4. after the specified time see that the instance is not stopped\r\n\r\n**expected behavior**\r\nidle  notebook instance automatically stops after specified time.\r\n\r\n**screenshots**\r\n<img width=\"1308\" alt=\"screen shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**versions (please complete the following information):**\r\n - release version installed v5.0.0\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where idle notebook instances do not stop automatically after the specified time, despite the autostop.py script being used by the instance cfn template.",
        "Issue_preprocessed_content":"Title: instance does not stop automatically; Content: describe the bug idle notebook instances do not stop after specified time. swb runs script to automatically stop notebook instance. the script is used by lifecycle rule of the instance cfn template. according to lifecycleconfigonstart logs, some packages are missing and autostop script doesnt work. to reproduce steps to reproduce the behavior . make sure autostopidletimeinminutes parameter in workspace type config is set to a required time . create a new workspace with notebook instance . leave the instance idle for the time specified . after the specified time see that the instance is not stopped expected behavior idle notebook instance automatically stops after specified time. screenshots versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1073",
        "Issue_title":"[Bug] Sagemaker AWS Credentials not Populating",
        "Issue_label":[
            "bug",
            "Review One",
            "Review Two"
        ],
        "Issue_creation_time":1669825870000,
        "Issue_closed_time":1671215597000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"When a Sagemaker notebook is launched with an associated study, the study folders are not mounted. Digging into this, I see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script.\r\n\r\n**To Reproduce**\r\n- Create a new data source (or use an existing one).\r\n- Select it and create a new Sagemaker instance using those studies. (Ensure that the folder has files so you can see them if they mount.)\r\n- After the instance launches, connect to it and see if the study folders are connected. If not, open a terminal and run `ls ~\/.aws` to see if the credential file is there.\r\n\r\n**Expected behavior**\r\nThe study folders are mounted using the assumed roles in the AWS credentials file, generated by `mount_s3.sh` script.\r\n\r\n**Screenshots**\r\n<img width=\"702\" alt=\"Screen Shot 2022-11-30 at 11 27 45 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853707-789607d5-6fca-4a77-911d-50a5c36a549b.png\">\r\n<img width=\"526\" alt=\"Screen Shot 2022-11-30 at 11 27 36 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853710-824ddc98-dfb5-4c8b-abbe-f19fa2453640.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - Versions 5.0.0 & 4.3.1\r\n\r\n**Additional context**\r\nThis may or may not be associated with the other bug I noted with mounting s3 studies folders, [[Bug] SWB Sagemaker Study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). It seems both of these issues are new, within the last couple weeks. And the environment has had no new deployments or changes within that time. Previous to these last couple weeks we had no issues with Sagemaker and study folders mounting.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  aws credentials not populating; Content: when a  notebook is launched with an associated study, the study folders are not mounted. digging into this, i see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script.\r\n\r\n**to reproduce**\r\n- create a new data source (or use an existing one).\r\n- select it and create a new  instance using those studies. (ensure that the folder has files so you can see them if they mount.)\r\n- after the instance launches, connect to it and see if the study folders are connected. if not, open a terminal and run `ls ~\/.aws` to see if the credential file is there.\r\n\r\n**expected behavior**\r\nthe study folders are mounted using the assumed roles in the aws credentials file, generated by `mount_s3.sh` script.\r\n\r\n**screenshots**\r\n<img width=\"702\" alt=\"screen shot 2022-11-30 at 11 27 45 am\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853707-789607d5-6fca-4a77-911d-50a5c36a549b.png\">\r\n<img width=\"526\" alt=\"screen shot 2022-11-30 at 11 27 36 am\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853710-824ddc98-dfb5-4c8b-abbe-f19fa2453640.png\">\r\n\r\n**versions (please complete the following information):**\r\n - versions 5.0.0 & 4.3.1\r\n\r\n**additional context**\r\nthis may or may not be associated with the other bug i noted with mounting s3 studies folders, [[bug] swb  study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). it seems both of these issues are new, within the last couple weeks. and the environment has had no new deployments or changes within that time. previous to these last couple weeks we had no issues with  and study folders mounting.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges with AWS credentials not populating when launching a notebook with an associated study, resulting in the study folders not being mounted.",
        "Issue_preprocessed_content":"Title: aws credentials not populating; Content: when a notebook is launched with an associated study, the study folders are not mounted. digging into this, i see that in the folder there is no credentials file, which is generated by the script. to reproduce create a new data source . select it and create a new instance using those studies. after the instance launches, connect to it and see if the study folders are connected. if not, open a terminal and run to see if the credential file is there. expected behavior the study folders are mounted using the assumed roles in the aws credentials file, generated by script. screenshots versions versions & additional context this may or may not be associated with the other bug i noted with mounting s studies folders, swb study permission it seems both of these issues are new, within the last couple weeks. and the environment has had no new deployments or changes within that time. previous to these last couple weeks we had no issues with and study folders mounting."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067",
        "Issue_title":"[Bug] SWB Sagemaker Study permission denied",
        "Issue_label":[
            "bug",
            "Review One",
            "Review Two"
        ],
        "Issue_creation_time":1668634688000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change.\r\n\r\nWhat could cause the fs role number to change for a study? What else could cause this permissions denied error? \r\n\r\nThis is a pretty big problem for us, as we have had people actively using SWB and all their work is gone on Sagemaker stop, because the folder they saved to isn't syncing.\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] swb  study permission denied; Content: we encountered an issue where an older  instance (>2 months) was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error: `nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.error unable to access '<bucket a, name redacted>': permission denied`\r\n\r\ncomparing the s3mounts parameter for the  stack of the older instance that fails to sync, and a newer instance (with the same studies), i see that the fs role number for the private workspace study that wouldn't sync is different.\r\n\r\nold stack s3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"private-workspace\",\r\n    \"bucket\": \"<bucket a, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"rolearn\": \"arn:aws:iam::<account redacted>:role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1662735997814\",\r\n    \"prefix\": \"private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"read-only\",\r\n    \"bucket\": \"<bucket a, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"rolearn\": \"arn:aws:iam::<account redacted>:role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\",\r\n    \"prefix\": \"read-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nnew stack s3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"private-workspace\",\r\n    \"bucket\": \"<bucket a, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"rolearn\": \"arn:aws:iam::<account redacted>:role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1668521384862\",\r\n    \"prefix\": \"private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"read-only\",\r\n    \"bucket\": \"<bucket a, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"rolearn\": \"arn:aws:iam::<account redacted>:role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\",\r\n    \"prefix\": \"read-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nsome additional context, this bucket (and the associated swb data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change.\r\n\r\nwhat could cause the fs role number to change for a study? what else could cause this permissions denied error? \r\n\r\nthis is a pretty big problem for us, as we have had people actively using swb and all their work is gone on  stop, because the folder they saved to isn't syncing.\r\n\r\n**versions (please complete the following information):**\r\n - swb 4.3.1\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue where an older instance of SWB (>2 months) was turned on and one of the two study folders associated were not syncing any of the files, resulting in a \"permission denied\" error in the system logs, and causing a major problem for users who had been actively using SWB and had their work disappear.",
        "Issue_preprocessed_content":"Title: swb study permission denied; Content: we encountered an issue where an older instance was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error comparing the s mounts parameter for the stack of the older instance that fails to sync, and a newer instance , i see that the fs role number for the private workspace study that wouldn't sync is different. old stack s mounts parameter new stack s mounts parameter some additional context, this bucket that the two studies are a part of gets updated every couple months to add new study but the existing studies don't typically change. what could cause the fs role number to change for a study? what else could cause this permissions denied error? this is a pretty big problem for us, as we have had people actively using swb and all their work is gone on stop, because the folder they saved to isn't syncing. versions swb"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Issue_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Issue_label":[
            "bug",
            "Review One",
            "Review Two"
        ],
        "Issue_creation_time":1668620515000,
        "Issue_closed_time":1671215663000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":16.0,
        "Issue_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  autostop script not pulling from s3 bucket; Content: **describe the bug**\r\nwe encountered an interesting issue regarding the auto stop script. we had no code changes, but suddenly,  instances started hanging around for days, with no use. looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. when i look at the script, it has this line `print(f'notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. however, the file on the repo, as well as the s3 bucket, does not contain this line. so, after some digging, i found that this line was introduced here, in this commit [aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). what i don't understand is how it got into the  notebook, and why it's not being overridden by the custom config start we have here [-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/-notebook-instance.cfn.yml#l264-l272) this script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**to reproduce**\r\nlaunch a  instance. you can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nthe aws version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/\/autostop.py#l96-l100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = false\r\nelse:\r\n    idle = false\r\n```\r\nand on the `aws-samples\/amazon--notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#l96-l101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = false\r\nelse:\r\n    idle = false\r\n    print('notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**expected behavior**\r\nthe autostop script in the s3 bucket should be the one used for swb  instances.\r\n\r\n**screenshots**\r\n<img width=\"1510\" alt=\"screen shot 2022-11-16 at 12 14 21 pm\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**versions (please complete the following information):**\r\n - swb 4.3.1\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the autostop script not pulling from the s3 bucket, resulting in a syntax error and instances hanging around for days with no use.",
        "Issue_preprocessed_content":"Title: autostop script not pulling from s bucket; Content: describe the bug we encountered an interesting issue regarding the auto stop script. we had no code changes, but suddenly, instances started hanging around for days, with no use. looking into the instance, the cron job was failing, because the script had a syntax error. when i look at the script, it has this line which caused the syntax error. however, the file on the repo, as well as the s bucket, does not contain this line. so, after some digging, i found that this line was introduced here, in this commit . what i don't understand is how it got into the notebook, and why it's not being overridden by the custom config start we have here this script and repo was updated in the last hours to remove this syntax error. to reproduce launch a instance. you can tell which version of the script it's using by looking at the autostop script, and find lines . the aws version of the script on the repo has these lines, and on the repo, expected behavior the autostop script in the s bucket should be the one used for swb instances. screenshots versions swb"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Issue_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1658897296000,
        "Issue_closed_time":1659686697000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  instances can't be launched due to missing tags permission; **describe the bug**\r\nservice workbench appears to be unable to launch  notebook instances at all, due to a missing permission for `:addtags`. this seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. install service workbench from the latest version.\r\n2. create a workspace configuration for a  notebook.\r\n3. launch a workspace using the new configuration.\r\n4. wait a few minutes and observe the error.\r\n\r\n**expected behavior**\r\nexpected the notebook to launch :)\r\n\r\n**screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nerror provisioning environment testnotebook1. reason: errors from cloudformation: [{logicalresourceid : sc-455040667691-pp-auh6sv7j6dwr2, resourcetype : aws::cloudformation::stack, statusreason : the following resource(s) failed to create: [basicnotebookinstance]. rollback requested by user.}, {logicalresourceid : basicnotebookinstance, resourcetype : aws::::notebookinstance, statusreason : user: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-launchconstraint\/servicecatalog is not authorized to perform: :addtags on resource: arn:aws::ap-southeast-2:xxxxxxxxxxxx:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the :addtags action (service: amazon; status code: 400; error code: accessdeniedexception; request id: adee97b7-1c89-47e2-8ca7-5aa374a80004; Content: proxy: null)}, {logicalresourceid : iamrole, resourcetype : aws::iam::role, statusreason : resource creation initiated}, {logicalresourceid : securitygroup, resourcetype : aws::ec2::securitygroup, statusreason : resource creation initiated}, {logicalresourceid : instancerolepermissionboundary, resourcetype : aws::iam::managedpolicy, statusreason : resource creation initiated}, {logicalresourceid : basicnotebookinstancelifecycleconfig, resourcetype : aws::::notebookinstancelifecycleconfig, statusreason : resource creation initiated}, {logicalresourceid : sc-455040667691-pp-auh6sv7j6dwr2, resourcetype : aws::cloudformation::stack, statusreason : user initiated}]\r\n```\r\n\r\n**versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where Service Workbench was unable to launch notebook instances due to a missing permission for `:addtags`.",
        "Issue_preprocessed_content":"Title: instances can't be launched due to missing tags permission; Content: describe the bug service workbench appears to be unable to launch notebook instances at all, due to a missing permission for . this seems to also be the case when custom tags aren't included in the workspace configuration. to reproduce steps to reproduce the behavior . install service workbench from the latest version. . create a workspace configuration for a notebook. . launch a workspace using the new configuration. . wait a few minutes and observe the error. expected behavior expected the notebook to launch screenshots versions also replicated on an older install additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Issue_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Issue_label":[
            "bug",
            "closing-soon-if-no-response"
        ],
        "Issue_creation_time":1631554276000,
        "Issue_closed_time":1633460282000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  notebook-v3 workspace changed to \"unknown\" status and cannot connect anymore; Content: **describe the bug**\r\na  notebook-v3 workspace that was working fine on friday today appears with the status as \"unknown\". \r\nwhen clicking on connect the new window pop up but is empty, and when going back to the swb page, we see the message, \"we have a problem! something went wrong\"\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. go to 'workspaces'\r\n2. look for the workspace that was expected to be \"stoped\"\r\n2. click on 'connect'\r\n4. see error\r\n\r\n**expected behavior**\r\nthat the workspace was \"stopped\" and when clicking on connect we can access to the workspace. \r\n\r\n**screenshots**\r\n![screen shot 2021-09-13 at 1 27 57 pm](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**versions (please complete the following information):**\r\nrelease version installed: 3.3.1\r\n\r\n**additional context**\r\nthe workspace was working fine all previous week, autostop and connect without any issue. unknown status found today.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where a notebook-v3 workspace changed to an \"unknown\" status and could not connect, resulting in an error message when attempting to access the workspace.",
        "Issue_preprocessed_content":"Title: notebook v workspace changed to unknown status and cannot connect anymore; Content: describe the bug a notebook v workspace that was working fine on friday today appears with the status as unknown . when clicking on connect the new window pop up but is empty, and when going back to the swb page, we see the message, we have a problem! something went wrong to reproduce steps to reproduce the behavior . go to 'workspaces' . look for the workspace that was expected to be stoped . click on 'connect' . see error expected behavior that the workspace was stopped and when clicking on connect we can access to the workspace. screenshots versions release version installed additional context the workspace was working fine all previous week, autostop and connect without any issue. unknown status found today."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Issue_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Issue_label":[
            "bug",
            "closing-soon-if-no-response"
        ],
        "Issue_creation_time":1628006476000,
        "Issue_closed_time":1643923114000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: \"null is not an object\" while trying to connect to  notebook.; Content: **describe the bug**\r\noccasionally after starting a  workspace, clicking 'connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> we have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. the notebook window is not opened after clicking on 'connect'.\r\n\r\n**to reproduce**\r\nthe error is intermittent. i *think* it may happen after the sw window has been open a while, because i noticed that the sw window automatically logged me out shortly after seeing this error.\r\n\r\n1. click 'start' for  workspace and wait for the status to change to 'available'. \r\n2. click 'connections', then 'connect'\r\n3. see error\r\n\r\nwhen i logged out and back into service workbench, and was able to connect to the workspace successfully. \r\n\r\n**expected behavior**\r\na new window should open with a jupyter\/ notebook in a new window. \r\n\r\n**versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an intermittent issue where clicking 'connect' to a workspace in Service Workbench 3.2.0 resulted in an error message of \"null is not an object (evaluating 'l.location=s')\" in a red box on the bottom-right of the screen, preventing the notebook window from opening.",
        "Issue_preprocessed_content":"Title: null is not an object while trying to connect to notebook.; Content: describe the bug occasionally after starting a workspace, clicking 'connect' gives an error in the bottom right hand corner of the screen > we have a problem! > null is not an object in a little red box on the bottom right of the screen. the notebook window is not opened after clicking on 'connect'. to reproduce the error is intermittent. i think it may happen after the sw window has been open a while, because i noticed that the sw window automatically logged me out shortly after seeing this error. . click 'start' for workspace and wait for the status to change to 'available'. . click 'connections', then 'connect' . see error when i logged out and back into service workbench, and was able to connect to the workspace successfully. expected behavior a new window should open with a notebook in a new window. versions"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Issue_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Issue_label":[
            "bug",
            "closing-soon-if-no-response"
        ],
        "Issue_creation_time":1622734645000,
        "Issue_closed_time":1624287519000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] blank page on  workspace connect; Content: **describe the bug**\r\nwhen connecting to  workspaces, there is an intermittent issue where a blank browser launches instead of .  the issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n\r\nstep 1: login as an admin \r\nstep 2: create a workspace ()\r\nstep 3: start the workspace \r\nstep 5: click \"connect\"\r\nstep 6: a new blank web browser tab opens \r\nstep 7: click \"connect\" again, another blank web browser tab opens\r\n\r\nuser receives a \"something went wrong\" general error in swb at step 6\r\n\r\nin the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"error from cloudfront\"\r\n   \r\n\r\n**expected behavior**\r\n workspace launch in browser\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**versions (please complete the following information):**\r\n - release version installed 3.0.0\r\n\r\n**additional context**\r\nuser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nthe issue is experienced approximately once a week. sometimes clearing cache solves the issue. other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user is experiencing an intermittent issue where a blank browser launches instead of the workspace when connecting to workspaces, which presents for both newly created and restarted workspaces, and is accompanied by an \"x-cache\" error in the client logs.",
        "Issue_preprocessed_content":"Title: blank page on workspace connect; Content: describe the bug when connecting to workspaces, there is an intermittent issue where a blank browser launches instead of . the issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted. to reproduce steps to reproduce the behavior step login as an admin step create a workspace step start the workspace step click connect step a new blank web browser tab opens step click connect again, another blank web browser tab opens user receives a something went wrong general error in swb at step in the client logs for the browser, there is also this error noted name x cache , value error from cloudfront expected behavior workspace launch in browser screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context user has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. the issue is experienced approximately once a week. sometimes clearing cache solves the issue. other times going to incognito, and it does not solve the issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/928",
        "Issue_title":"[SM-Executor] SageMaker.stop_training_job hangs",
        "Issue_label":[
            "bug",
            "p1",
            "service:sm-executor"
        ],
        "Issue_creation_time":1570606746000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Calling `stop_training_job` from the SageMaker client against an existing by not \"InProgress\" job, causes the client to hang. This only seems to happen within the sm-executor though. \r\n\r\nHere's the output calling the method from the python interpreter within the pod:\r\n```\r\n# .\/python\r\nPython 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import boto3\r\n>>> client = boto3.client(\"sagemaker\")\r\n>>> client.stop_training_job(TrainingJobName=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the StopTrainingJob operation: The request was rejected because the training job is in status Stopped.\r\n>>>\r\n```\r\n\r\nHere is the DEBUG output from the sm-executor\r\n```\r\n2019-10-09 06:39:38,252 INFO: Attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048\r\n2019-10-09 06:39:38,252 DEBUG: Event before-parameter-build.sagemaker.StopTrainingJob: calling handler <function generate_idempotent_uuid at 0x7f54d82671e0>\r\n2019-10-09 06:39:38,253 DEBUG: Event before-call.sagemaker.StopTrainingJob: calling handler <function inject_api_version_header_if_needed at 0x7f54d8268b70>\r\n2019-10-09 06:39:38,253 DEBUG: Making request for OperationModel(name=StopTrainingJob) with params: {'url_path': '\/', 'query_string': '', 'method': 'POST', 'headers': {'X-Amz-Target': 'SageMaker.StopTrainingJob', 'Content-Type': 'application\/x-amz-json-1.1', 'User-Agent': 'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221'}, 'body': b'{\"TrainingJobName\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api.sagemaker.us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x7f54bacde518>, 'has_streaming_input': False, 'auth_type': None}}\r\n2019-10-09 06:39:38,253 DEBUG: Event request-created.sagemaker.StopTrainingJob: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7f54bacde4e0>>\r\n2019-10-09 06:39:38,254 DEBUG: Event choose-signer.sagemaker.StopTrainingJob: calling handler <function set_operation_specific_signer at 0x7f54d82670d0>\r\n2019-10-09 06:39:38,254 DEBUG: Calculating signature using v4 auth.\r\n2019-10-09 06:39:38,254 DEBUG: CanonicalRequest:\r\nPOST\r\n\/\r\n\r\ncontent-type:application\/x-amz-json-1.1\r\nhost:api.sagemaker.us-east-1.amazonaws.com\r\nx-amz-date:20191009T063938Z\r\nx-amz-security-token:FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF\r\nx-amz-target:SageMaker.StopTrainingJob\r\n\r\ncontent-type;host;x-amz-date;x-amz-security-token;x-amz-target\r\n84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40\r\n2019-10-09 06:39:38,254 DEBUG: StringToSign:\r\nAWS4-HMAC-SHA256\r\n20191009T063938Z\r\n20191009\/us-east-1\/sagemaker\/aws4_request\r\n4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db\r\n2019-10-09 06:39:38,255 DEBUG: Signature:\r\nc074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae\r\n2019-10-09 06:39:38,255 DEBUG: Sending http request: <AWSPreparedRequest stream_output=False, method=POST, url=https:\/\/api.sagemaker.us-east-1.amazonaws.com\/, headers={'X-Amz-Target': b'SageMaker.StopTrainingJob', 'Content-Type': b'application\/x-amz-json-1.1', 'User-Agent': b'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221', 'X-Amz-Date': b'20191009T063938Z', 'X-Amz-Security-Token': b'FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF', 'Authorization': b'AWS4-HMAC-SHA256 Credential=ASIAYNI7SS57NFEDAZHQ\/20191009\/us-east-1\/sagemaker\/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae', 'Content-Length': '59'}>\r\n2019-10-09 06:39:38,320 DEBUG: Response headers: {'x-amzn-RequestId': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'Content-Type': 'application\/x-amz-json-1.1', 'Content-Length': '116', 'Date': 'Wed, 09 Oct 2019 06:39:37 GMT', 'Connection': 'close'}\r\n2019-10-09 06:39:38,320 DEBUG: Response body:\r\nb'{\"__type\":\"ValidationException\",\"message\":\"The request was rejected because the training job is in status Stopped.\"}'\r\n2019-10-09 06:39:38,320 DEBUG: Event needs-retry.sagemaker.StopTrainingJob: calling handler <botocore.retryhandler.RetryHandler object at 0x7f54bacde898>\r\n2019-10-09 06:39:38,321 DEBUG: No retry needed.\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [sm-executor] .stop_training_job hangs; Content: calling `stop_training_job` from the  client against an existing by not \"inprogress\" job, causes the client to hang. this only seems to happen within the sm-executor though. \r\n\r\nhere's the output calling the method from the python interpreter within the pod:\r\n```\r\n# .\/python\r\npython 3.7.3 | packaged by conda-forge | (default, jul  1 2019, 21:52:21)\r\n[gcc 7.3.0] :: anaconda, inc. on linux\r\ntype \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import boto3\r\n>>> client = boto3.client(\"\")\r\n>>> client.stop_training_job(trainingjobname=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\")\r\ntraceback (most recent call last):\r\n  file \"<stdin>\", line 1, in <module>\r\n  file \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  file \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.clienterror: an error occurred (validationexception) when calling the stoptrainingjob operation: the request was rejected because the training job is in status stopped.\r\n>>>\r\n```\r\n\r\nhere is the debug output from the sm-executor\r\n```\r\n2019-10-09 06:39:38,252 info: attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048\r\n2019-10-09 06:39:38,252 debug: event before-parameter-build..stoptrainingjob: calling handler <function generate_idempotent_uuid at 0x7f54d82671e0>\r\n2019-10-09 06:39:38,253 debug: event before-call..stoptrainingjob: calling handler <function inject_api_version_header_if_needed at 0x7f54d8268b70>\r\n2019-10-09 06:39:38,253 debug: making request for operationmodel(name=stoptrainingjob) with params: {'url_path': '\/', 'query_string': '', 'method': 'post', 'headers': {'x-amz-target': '.stoptrainingjob', 'content-type': 'application\/x-amz-json-1.1', 'user-agent': 'boto3\/1.9.221 python\/3.7.3 linux\/4.14.128-112.105.amzn2.x86_64 botocore\/1.12.221'}, 'body': b'{\"trainingjobname\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api..us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.config object at 0x7f54bacde518>, 'has_streaming_input': false, 'auth_type': none}}\r\n2019-10-09 06:39:38,253 debug: event request-created..stoptrainingjob: calling handler <bound method requestsigner.handler of <botocore.signers.requestsigner object at 0x7f54bacde4e0>>\r\n2019-10-09 06:39:38,254 debug: event choose-signer..stoptrainingjob: calling handler <function set_operation_specific_signer at 0x7f54d82670d0>\r\n2019-10-09 06:39:38,254 debug: calculating signature using v4 auth.\r\n2019-10-09 06:39:38,254 debug: canonicalrequest:\r\npost\r\n\/\r\n\r\ncontent-type:application\/x-amz-json-1.1\r\nhost:api..us-east-1.amazonaws.com\r\nx-amz-date:20191009t063938z\r\nx-amz-security-token:fqogzxivyxdzemj\/\/\/\/\/\/\/\/\/\/weadfbwyhfmhbwcrxmnqikeah9qxhxpmhbcdkddch4unekdyuxx+8r3yub8kigvzjeuvch64xiaogwnkb2ztrisoyufwgqb2c6+nsptni65yvatyi6+zedrb0rhjlyfe98l5b0decm5ie7o0xq7zflpifttok9h7qenh9n8mae69xevthv0gd34dalxmlufalysvb6+ewo7rvfpjdez+1xqlslkwmbpa8yj+ngjdhxckibgpcwxuxip+zvssx5+genswdzoj\/otdckepxd25outuvf5wn+usakv1u4ddig8mfpumzjg\/m93luuzx3ok88xc6dmwajhayc9xh5n89zyzgxmq5np\/wkcou\/wbolsmdvdaay41kpsa9uwf\r\nx-amz-target:.stoptrainingjob\r\n\r\ncontent-type;host;x-amz-date;x-amz-security-token;x-amz-target\r\n84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40\r\n2019-10-09 06:39:38,254 debug: stringtosign:\r\naws4-hmac-sha256\r\n20191009t063938z\r\n20191009\/us-east-1\/\/aws4_request\r\n4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db\r\n2019-10-09 06:39:38,255 debug: signature:\r\nc074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae\r\n2019-10-09 06:39:38,255 debug: sending http request: <awspreparedrequest stream_output=false, method=post, url=https:\/\/api..us-east-1.amazonaws.com\/, headers={'x-amz-target': b'.stoptrainingjob', 'content-type': b'application\/x-amz-json-1.1', 'user-agent': b'boto3\/1.9.221 python\/3.7.3 linux\/4.14.128-112.105.amzn2.x86_64 botocore\/1.12.221', 'x-amz-date': b'20191009t063938z', 'x-amz-security-token': b'fqogzxivyxdzemj\/\/\/\/\/\/\/\/\/\/weadfbwyhfmhbwcrxmnqikeah9qxhxpmhbcdkddch4unekdyuxx+8r3yub8kigvzjeuvch64xiaogwnkb2ztrisoyufwgqb2c6+nsptni65yvatyi6+zedrb0rhjlyfe98l5b0decm5ie7o0xq7zflpifttok9h7qenh9n8mae69xevthv0gd34dalxmlufalysvb6+ewo7rvfpjdez+1xqlslkwmbpa8yj+ngjdhxckibgpcwxuxip+zvssx5+genswdzoj\/otdckepxd25outuvf5wn+usakv1u4ddig8mfpumzjg\/m93luuzx3ok88xc6dmwajhayc9xh5n89zyzgxmq5np\/wkcou\/wbolsmdvdaay41kpsa9uwf', 'authorization': b'aws4-hmac-sha256 credential=asiayni7ss57nfedazhq\/20191009\/us-east-1\/\/aws4_request, signedheaders=content-type;host;x-amz-date;x-amz-security-token;x-amz-target, signature=c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae', 'content-length': '59'}>\r\n2019-10-09 06:39:38,320 debug: response headers: {'x-amzn-requestid': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '116', 'date': 'wed, 09 oct 2019 06:39:37 gmt', 'connection': 'close'}\r\n2019-10-09 06:39:38,320 debug: response body:\r\nb'{\"__type\":\"validationexception\",\"message\":\"the request was rejected because the training job is in status stopped.\"}'\r\n2019-10-09 06:39:38,320 debug: event needs-retry..stoptrainingjob: calling handler <botocore.retryhandler.retryhandler object at 0x7f54bacde898>\r\n2019-10-09 06:39:38,321 debug: no retry needed.\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where calling the `stop_training_job` method from the client against an existing, but not \"inprogress\" job, caused the client to hang.",
        "Issue_preprocessed_content":"Title: hangs; Content: calling from the client against an existing by not inprogress job, causes the client to hang. this only seems to happen within the sm executor though. here's the output calling the method from the python interpreter within the pod here is the debug output from the sm executor"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Issue_title":"Cannot run benchmark for sagemaker",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1570150277000,
        "Issue_closed_time":1571326528000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: cannot run benchmark for ; Content: when i tried to run benchmark on  with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\ni also tried to run the sample for  https:\/\/github.com\/mxnetedge\/benchmark-ai\/blob\/master\/sample-benchmarks\/\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nbtw, when we wanna run with , besides specify  execution_engine = \"aws.\" and framework , is there anything else we need to specify or change?\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when trying to run a benchmark on Anubis with either their own code or a sample code, resulting in an error message.",
        "Issue_preprocessed_content":"Title: cannot run benchmark for; Content: when i tried to run benchmark on with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. i also tried to run the sample for and it showed with the same error btw, when we wanna run with , besides specify and framework , is there anything else we need to specify or change?"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Issue_title":"The data path inside sagemaker notebook does not work",
        "Issue_label":[
            "bug",
            "needs-triage"
        ],
        "Issue_creation_time":1620285557000,
        "Issue_closed_time":1621931178000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: the data path inside  notebook does not work; Content: the bucket of processed data does not exist (src\/\/fd_sl_training_byo_codes.ipynb)\r\n\r\n\r\n### reproduction steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### error log\r\n\r\nan error occurred (nosuchbucket) when calling the listobjectsv2 operation: the specified bucket does not exist\r\n\r\n\r\n\r\n### environment\r\n\r\n  - **cdk cli version:** 1.75.0 (build 7708242)\r\n  - **framework version:** not installed\r\n  - **node.js version:**  not installed\r\n  - **os               :**\r\n\r\n### other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nthis is :bug: bug report",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the data path inside the notebook did not work, resulting in an error log of \"an error occurred (nosuchbucket) when calling the listobjectsv2 operation: the specified bucket does not exist\".",
        "Issue_preprocessed_content":"Title: the data path inside notebook does not work; Content: the bucket of processed data does not exist reproduction steps aws s ls error log an error occurred when calling the listobjectsv operation the specified bucket does not exist environment cdk cli version framework version not installed version not installed os other this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Issue_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Issue_label":[
            "bug",
            "needs-triage"
        ],
        "Issue_creation_time":1618212282000,
        "Issue_closed_time":1618279737000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  endpoint fail to deploy or time out server error(0) bug; Content: invoke endpoint response time out. \r\n\r\n### reproduction steps\r\n\r\n{\r\n  \"trainingjob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instancetype\": \"ml.c5.9xlarge\",\r\n    \"timeoutinseconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### error log\r\nin inference lambda cloudwatch:\r\n\r\ntask timed out after 120.10 seconds\r\n\r\n\r\nin  training cloudwatch:\r\n\r\n### other\r\n\r\ncause of this bug:\r\n\r\nbackend worker process died.\r\n endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the endpoint failed to deploy or timed out with a server error (0), caused by a conflict between the endpoint deployment code and model training code parameters on n-hidden and hidden_size.",
        "Issue_preprocessed_content":"Title: endpoint fail to deploy or time out server error bug; Content: invoke endpoint response time out. reproduction steps trainingjob , instancetype timeoutinseconds error log in inference lambda cloudwatch task timed out after seconds in training cloudwatch , main loading initial models , main etag ff b de bed fb bc fe ff , main model model loaded. , main initialize inference server with epollserversocketchannel. , main inference api bind to , main initialize metrics server with epollserversocketchannel. , main metrics api bind to model server started. , listening on port , , torch worker started. , python runtime , connecting to , connection accepted , pool thread , pool thread , pool thread , pool thread , pool thread , pool thread , pool thread , setting the default backend to pytorch . you can change it in the file or export the dglbackend environment variable. valid options are pytorch, mxnet, tensorflow , loading model , backend worker process died. , traceback , file line , in , , file line , in , , file line , in , service, result, code , file line , in , service handler, gpu, , file line , in load , , file line , in initialize , , file line , in initialize , , file line , in , , file line , in , , file line , in , , runtimeerror error in loading for heterorgcn , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is environment cdk cli version framework version version os other cause of this bug backend worker process died. endpoint deployment code and model training code parameter conflict on n hidden and this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/accelerate\/issues\/706",
        "Issue_title":"Have accelerate for  Distributed Training: Data Parallelism feature working on AWS Sagemaker yet?",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1663741563000,
        "Issue_closed_time":1664255368000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### System Info\n\n```Shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] One of the scripts in the examples\/ folder of Accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSagemaker Multi-GPU distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### Expected behavior\n\n```Shell\nI'm trying to run a distributed training in a Sagemaker training job, the inference is not working properly, I found it as a future work on huggingface documentation so I'm wondering If that's why it's not working yet on sagemaker Multi-GPU.\r\n\r\nThanks\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: have accelerate for  distributed training: data parallelism feature working on  yet?; Content: ### system info\n\n```shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### information\n\n- [ ] the official example scripts\n- [ ] my own modified scripts\n\n### tasks\n\n- [ ] one of the scripts in the examples\/ folder of accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] my own task or dataset (give details below)\n\n### reproduction\n\n multi-gpu distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### expected behavior\n\n```shell\ni'm trying to run a distributed training in a  training job, the inference is not working properly, i found it as a future work on huggingface documentation so i'm wondering if that's why it's not working yet on  multi-gpu.\r\n\r\nthanks\n```\n",
        "Issue_original_content_gpt_summary":"The user is encountering challenges with multi-GPU distributed data training, where the inference is not working properly, and is wondering if this is due to the feature not being available yet.",
        "Issue_preprocessed_content":"Title: have accelerate for distributed training data parallelism feature working on yet?; Content: system the official example scripts my own modified scripts tasks one of the scripts in the examples\/ folder of accelerate or an officially supported script in the folder of the repo my own task or dataset reproduction multi gpu distributed data training, while it always returns empty tensors. expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Issue_title":"Sagemaker example DAG to use aws session token",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666950742000,
        "Issue_closed_time":1666960895000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  example dag to use aws session token; Content: **describe the bug**\r\ncurrently the example dag for  just uses access key and secret key. we need to use a temporary  access token\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. go to '...'\r\n2. click on '....'\r\n3. scroll down to '....'\r\n4. see error\r\n\r\n**expected behavior**\r\na clear and concise description of what you expected to happen.\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n**desktop (please complete the following information):**\r\n - os: [e.g. ios]\r\n - browser [e.g. chrome, safari]\r\n - version [e.g. 22]\r\n\r\n**smartphone (please complete the following information):**\r\n - device: [e.g. iphone6]\r\n - os: [e.g. ios8.1]\r\n - browser [e.g. stock browser, safari]\r\n - version [e.g. 22]\r\n\r\n**additional context**\r\nadd any other context about the problem here.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the example dag for using an AWS session token, where the access key and secret key were not being used correctly, and needed to be replaced with a temporary access token.",
        "Issue_preprocessed_content":"Title: example dag to use aws session token; Content: describe the bug currently the example dag for just uses access key and secret key. we need to use a temporary access token to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. desktop os browser version smartphone device os browser version additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Issue_title":"XCom Output of Sagemaker Async Operators",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666892144000,
        "Issue_closed_time":1666957435000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: xcom output of  async operators; Content: **describe the bug**\r\nxcom return value of `transformoperatorasync`  and `trainingoperatorasync` does not produce the expected output.\r\n\r\nit seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. run a dag with traditional operators\r\n2. run same dag with async operators\r\n3. compare outputs\r\n\r\n**expected behavior**\r\nthe xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the xcom return value of `transformoperatorasync` and `trainingoperatorasync` does not produce the expected output, with the xcom keys and values not matching the non-async operator output.",
        "Issue_preprocessed_content":"Title: xcom output of async operators; Content: describe the bug xcom return value of and does not produce the expected output. it seems like some key don't match the non async operator output. to reproduce steps to reproduce the behavior . run a dag with traditional operators . run same dag with async operators . compare outputs expected behavior the xcom keys and values should match whatever the traditional non async version of the operators output. screenshots if applicable, add screenshots to help explain your problem."
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Issue_title":"Token error with Sagemaker Async Operators",
        "Issue_label":[
            "bug",
            "area\/async"
        ],
        "Issue_creation_time":1666713848000,
        "Issue_closed_time":1666859588000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: token error with  async operators; Content: **describe the bug**\r\ngetting errors with the new  async operators that i don't get with the traditional ones. i'm using a personal access key, secret, and session token as i did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.clienterror: an error occurred (unrecognizedclientexception) when calling the describetrainingjob operation: the security token included in the request is invalid.\r\n```\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\nuse the  async operators with user access key, secret, and session token\r\n\r\n**expected behavior**\r\nexpect it to not have auth\/token errors.\r\n\r\n\r\n**additional context**\r\nwhen i switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Issue_original_content_gpt_summary":"The user encountered an error with the new async operators when using a personal access key, secret, and session token for authentication, which was not encountered when using the traditional operators.",
        "Issue_preprocessed_content":"Title: token error with async operators; Content: describe the bug getting errors with the new async operators that i don't get with the traditional ones. i'm using a personal access key, secret, and session token as i did with the non async operators for auth. to reproduce steps to reproduce the behavior use the async operators with user access key, secret, and session token expected behavior expect it to not have errors. additional context when i switch back to the traditional operators in the same dag with the same auth creds it works fine. also had similar issues and her auth was setup a little different."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Issue_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1609843354000,
        "Issue_closed_time":1646674184000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] graph tab doesn't render in  studio - jupyter lab; Content: **describe the bug**\r\nwhen trying to execute a .path() query in jupyter lab the graph tab doesn't render, instead it shows\r\n`\"tab(children=(output(layout=layout(max_height='600px', overflow='scroll', width='100%')), force(network=<graph\u2026\"`\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. go to jupyter lab\r\n2. run a query with .path()\r\n\r\n**current behavior**\r\nscreenshot taken from jupyterlab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**expected behavior**\r\nscreenshot taken from jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the graph tab does not render in Jupyter Lab when running a query with .path(), instead of the expected behavior of a graph being displayed.",
        "Issue_preprocessed_content":"Title: graph tab doesn't render in studio jupyter lab; Content: describe the bug when trying to execute a .path , force current behavior screenshot taken from jupyterlab expected behavior screenshot taken from jupyter"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Issue_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657213837000,
        "Issue_closed_time":1660575729000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: led model returns algorithmerror when using  smp training #16890; Content: ### system info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nhello,\r\n\r\nthis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nwe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-valueerror: not enough values to unpack (expected 2, got 1)\r\n\r\nthe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nnew update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nit helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nunexpectedstatusexception: error for training job huggingface-pytorch-training-2022-06-29-04-04-58-606: failed. reason: algorithmerror: executeuserscripterror:\r\nexitcode 1\r\nerrormessage \":runtimeerror: tensors must have same number of dimensions: got 4 and 3\r\n :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set -------------------------------------------------------------------------- primary job  terminated normally, but 1 process returned a non-zero exit code. per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. the first process to do so was:    process name: [[41154,1],0]   exit code:    1\"\r\ncommand \"mpirun --host algo-1:8 \r\n\r\n\r\ni\u2019d greatly appreciate your feedback. please let me know if you need any further information about the project.\n\n### who can help?\n\n[apriltraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/apriltraining.zip)\r\n\n\n### information\n\n- [ ] the official example scripts\n- [x] my own modified scripts\n\n### tasks\n\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\n- [x] my own task or dataset (give details below)\n\n### reproduction\n\nrunning this attached file with the training python file\n\n### expected behavior\n\ni have shared the notebook and the error raised in it for clarification",
        "Issue_original_content_gpt_summary":"The user encountered multiple challenges while attempting to use the Transformers library to train a model, including a ValueError and an UnexpectedStatusException, both related to the shape of the input tensors.",
        "Issue_preprocessed_content":"Title: led model returns algorithmerror when using smp training; Content: system cc , cc , cc hello, this is a follow up on a related post with the below link with the same title we ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations valueerror not enough values to unpack the error is in the within the transformers module expecting a different shape. new update is we tried below to unsqueeze input tensors to the to solve the above error def return it helped moving forward in the process, but we got another error, below, a little further down in the code unexpectedstatusexception error for training job huggingface pytorch training failed. reason algorithmerror executeuserscripterror exitcode errormessage runtimeerror tensors must have same number of dimensions got and environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set primary job terminated normally, but process returned a non zero exit code. per user direction, the job has been aborted. detected that one or more processes exited with non zero status, thus causing the job to be terminated. the first process to do so was process name , exit code command mpirun host algo id greatly appreciate your feedback. please let me know if you need any further about the project. who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction running this attached file with the training python file expected behavior i have shared the notebook and the error raised in it for clarification"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17855",
        "Issue_title":"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1656007789000,
        "Issue_closed_time":1656429784000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### System Info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nPlatform: Sagemaker Deep Learning Container\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@NielsRogge\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nThe error only comes when training on Sagemaker using Huggingface.\r\n\r\nScripts to start training on Sagemaker:\r\n\r\nFolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.huggingface import HuggingFace\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['Role']['Arn']\r\n    sess = sagemaker.Session()\r\n\r\n    sagemaker_session_bucket = 's3-sagemaker-session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': True,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': 'Sagemaker Training'\r\n                       }\r\n\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nEntrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n\r\n\r\ndef run():\r\n    model = LayoutLMv2ForSequenceClassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\"Sagemaker Training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=False)\r\n    # Data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getLogger(__name__)\r\n    logging.basicConfig(\r\n        level=logging.getLevelName(\"INFO\"),\r\n        handlers=[logging.StreamHandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n```shell\r\nHere the log on the error from AWS Cloud Watch:\r\n\r\nInvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val True --epochs 20 --exp_name Sagemaker_Training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import META_ARCH_REGISTRY\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import ShapeSpec\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.ScriptModule\" = torch.jit.script(sigmoid_focal_loss)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\nRuntimeError: \r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 6, in <module>\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: layoutlmv2 training on  error: undefined value has_torch_function_variadic; Content: ### system info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nplatform:  deep learning container\r\n```\r\n\r\n\r\n### who can help?\r\n\r\n@nielsrogge\r\n\r\n### information\r\n\r\n- [ ] the official example scripts\r\n- [x] my own modified scripts\r\n\r\n### tasks\r\n\r\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\r\n- [ ] my own task or dataset (give details below)\r\n\r\n### reproduction\r\n\r\nthe error only comes when training on  using huggingface.\r\n\r\nscripts to start training on :\r\n\r\nfolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport \r\nfrom .huggingface import huggingface\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['role']['arn']\r\n    sess = .session()\r\n\r\n    _session_bucket = 's3--session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': true,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': ' training'\r\n                       }\r\n\r\n    huggingface_estimator = huggingface(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'hf_task': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nentrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import layoutlmv2forsequenceclassification\r\n\r\n\r\ndef run():\r\n    model = layoutlmv2forsequenceclassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.argumentparser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\" training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=false)\r\n    # data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"sm_output_data_dir\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"sm_num_gpus\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getlogger(__name__)\r\n    logging.basicconfig(\r\n        level=logging.getlevelname(\"info\"),\r\n        handlers=[logging.streamhandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### expected behavior\r\n\r\n```shell\r\nhere the log on the error from aws cloud watch:\r\n\r\ninvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val true --epochs 20 --exp_name _training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\ntraceback (most recent call last):\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  file \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  file \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nfile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nfile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nfile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nfile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  file \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nfile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import meta_arch_registry\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import shapespec\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import frozenbatchnorm2d, get_norm, naivesyncbatchnorm, cyclebatchnormlist\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.scriptmodule\" = torch.jit.script(sigmoid_focal_loss)\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._c._jit_script_compile(\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._c._jit_script_compile(\r\nruntimeerror: \r\nundefined value has_torch_function_variadic:\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = f.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nthe above exception was the direct cause of the following exception:\r\ntraceback (most recent call last):\r\n  file \"train.py\", line 6, in <module>\r\nfrom transformers import layoutlmv2forsequenceclassification\r\n  file \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise runtimeerror(\r\nruntimeerror: failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = f.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when training layoutlmv2 on Huggingface, with an undefined value has_torch_function_variadic.",
        "Issue_preprocessed_content":"Title: layoutlmv training on error undefined value; Content: system who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction the error only comes when training on using huggingface. scripts to start training on folder organization entrypoint scripts folder expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Issue_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1651754767000,
        "Issue_closed_time":1654502136000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pip install \"sacremoses>=0.0.50\" breaks on  studio; Content: ### system info\n\n```shell\nthis was verified today on a fresh  studio instance running in us-west-2.\r\n\r\nit's not a transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on  studio at some point.\n```\n\n\n### who can help?\n\n_no response_\n\n### information\n\n- [ ] the official example scripts\n- [ ] my own modified scripts\n\n### tasks\n\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\n- [ ] my own task or dataset (give details below)\n\n### reproduction\n\n1) open an sm studio notebook\r\n\r\n2) run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nthe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to install the sacremoses package version 0.0.50 on an SM Studio notebook, resulting in an error, and a workaround of installing version 0.0.49 was suggested.",
        "Issue_preprocessed_content":"Title: pip install breaks on studio; Content: system who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction open an sm studio notebook run the following cell the obvious workaround for now is expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Issue_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1650634004000,
        "Issue_closed_time":1653922922000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: led model returns algorithmerror when using  smp training; Content: ### system info\n\n```shell\nusing  \r\nmpi_options = {\r\n    \"enabled\" : true,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":true,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": true,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':hhousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = huggingface(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### who can help?\n\n@ydshieh @sgugger\n\n### information\n\n- [ ] the official example scripts\n- [ ] my own modified scripts\n\n### tasks\n\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\n- [ ] my own task or dataset (give details below)\n\n### reproduction\n\n1. create huggingface estimator\r\n2.     training_args = seq2seqtrainingarguments(\r\n        predict_with_generate=true,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=true,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nerror i get:\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  file \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:valueerror: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nprimary job  terminated normally, but 1 process returned\r\na non-zero exit code. per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. the first process to do so was:\r\n  process name: [[41156,1],0]\r\n  exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### expected behavior\n\n```shell\ntraining on a  notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\ni can successfully train the led model with my training data. trying to get it to work with huggingface estimator and  smp i would assume the same outcome.\n```\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to use the Huggingface Estimator and SMP to train a LED model, resulting in an AlgorithmError.",
        "Issue_preprocessed_content":"Title: led model returns algorithmerror when using smp training; Content: system who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . create huggingface estimator . seq seqtrainingarguments error i get , file line , in , raise e , file line , in , output args, kwargs , file line , in forward , return , file line , in , raise e , file line , in , output args, kwargs , file line , in forward , bsz, , valueerror not enough values to unpack primary job terminated normally, but process returned a non zero exit code. per user direction, the job has been aborted. detected that one or more processes exited with non zero status, thus causing the job to be terminated. the first process to do so was process name , exit code expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1862",
        "Issue_title":"[BUG] Update test documentation to connect AzureML with GitHub actions",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669630421000,
        "Issue_closed_time":1669646142000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nSteps:\r\n1. Create a new AzureML workspace.\r\n    - Name: `azureml-test-workspace`\r\n    - Resource group: `recommenders_project_resources`\r\n    - Location: *Make sure you have enough quota in the location you choose*\r\n2. Create two new clusters: `cpu-cluster` and `gpu-cluster`. Go to compute, then compute cluster, then new.\r\n    - Select the CPU VM base. Anything above 32GB of RAM, and 8 cores should be fine.\r\n    - Select the GPU VM base. Anything above 56GB of RAM, and 6 cores, and an NVIDIA K80 should be fine.\r\n3. Add the subscription ID to GitHub action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). Create a new repository secret called `AZUREML_TEST_SUBID` and add the subscription ID as the value.\r\n4. Make sure you have installed [Azure CLI](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. Select your subscription: `az account set -s $AZURE_SUBSCRIPTION_ID`.\r\n5. Create a Service Principal: `az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID --sdk-auth`.\r\n6. Add the output from the Service Principal (should be a JSON blob) as an action secret `AZUREML_TEST_CREDENTIALS`.\r\n\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] update test documentation to connect  with github actions; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\n\r\nsteps:\r\n1. create a new  workspace.\r\n    - name: `-test-workspace`\r\n    - resource group: `recommenders_project_resources`\r\n    - location: *make sure you have enough quota in the location you choose*\r\n2. create two new clusters: `cpu-cluster` and `gpu-cluster`. go to compute, then compute cluster, then new.\r\n    - select the cpu vm base. anything above 32gb of ram, and 8 cores should be fine.\r\n    - select the gpu vm base. anything above 56gb of ram, and 6 cores, and an nvidia k80 should be fine.\r\n3. add the subscription id to github action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). create a new repository secret called `_test_subid` and add the subscription id as the value.\r\n4. make sure you have installed [azure cli](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. select your subscription: `az account set -s $azure_subscription_id`.\r\n5. create a service principal: `az ad sp create-for-rbac --name \"cicd\" --role contributor --scopes \/subscriptions\/$azure_subscription_id --sdk-auth`.\r\n6. add the output from the service principal (should be a json blob) as an action secret `_test_credentials`.\r\n\r\n\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\n\r\n### how do we replicate the issue?\r\n<!--- please be specific as possible (use a list if needed). -->\r\n<!--- for example: -->\r\n<!--- * create a conda environment for pyspark -->\r\n<!--- * run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges in updating test documentation to connect with GitHub Actions, requiring the creation of a new workspace, two new clusters, adding the subscription ID to GitHub Action secrets, installing Azure CLI, selecting the subscription, creating a service principal, and adding the output from the service principal as an action secret.",
        "Issue_preprocessed_content":"Title: update test documentation to connect with github actions; Content: description steps . create a new workspace. name resource group location make sure you have enough quota in the location you choose . create two new clusters and . go to compute, then compute cluster, then new. select the cpu vm base. anything above gb of ram, and cores should be fine. select the gpu vm base. anything above gb of ram, and cores, and an nvidia k should be fine. . add the subscription id to github action secrets . create a new repository secret called and add the subscription id as the value. . make sure you have installed , and that you are logged in . . select your subscription . . create a service principal . . add the output from the service principal as an action secret . in which platform does it happen? how do we replicate the issue? expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1852",
        "Issue_title":"[BUG] AzureML test process is not failing if there is an error in the tests",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1668674781000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nAzureML tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\nWe want to send back a signal to GitHub so if the tests fail, the badge is red and we are notified\r\n\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  test process is not failing if there is an error in the tests; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\n tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors\r\n\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\n\r\n### how do we replicate the issue?\r\n<!--- please be specific as possible (use a list if needed). -->\r\n<!--- for example: -->\r\n<!--- * create a conda environment for pyspark -->\r\n<!--- * run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nsee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\nwe want to send back a signal to github so if the tests fail, the badge is red and we are notified\r\n\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the test process was not failing if there was an error in the tests, making it difficult to identify errors.",
        "Issue_preprocessed_content":"Title: test process is not failing if there is an error in the tests; Content: description tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors in which platform does it happen? how do we replicate the issue? see expected behavior we want to send back a signal to github so if the tests fail, the badge is red and we are notified other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Issue_title":"[BUG] xdeepfm error in AzureML test",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1668591744000,
        "Issue_closed_time":1668600473000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] xdeepfm error in  test; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                epochs_for_synthetic_run=syn_epochs,\r\n                epochs_for_criteo_run=criteo_epochs,\r\n                batch_size_synthetic=1024,\r\n                batch_size_criteo=1024,\r\n                random_seed=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=tol, abs=abs_tol)\r\ne           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\ne             comparison failed\r\ne             obtained: 0.5131\r\ne             expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\n\r\n### how do we replicate the issue?\r\n<!--- please be specific as possible (use a list if needed). -->\r\n<!--- for example: -->\r\n<!--- * create a conda environment for pyspark -->\r\n<!--- * run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nsee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug in xDeepFM where the expected values for AUC and logloss were not met, and encountered challenges in replicating the issue and finding a solution.",
        "Issue_preprocessed_content":"Title: xdeepfm error in test; Content: description in which platform does it happen? how do we replicate the issue? see expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Issue_title":"[BUG] Error in some of the AzureML tests",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1668089448000,
        "Issue_closed_time":1668591607000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] error in some of the  tests; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\nthere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nerror tests\/integration\/examples\/test_notebooks_gpu.py\r\nerror tests\/integration\/examples\/test_notebooks_gpu.py\r\nerror tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nerror: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nerror: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nerror: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\ninfo:submit_groupwise__pytest.py:test execution completed!\r\n\r\n```\r\n\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\n\r\n### how do we replicate the issue?\r\n<!--- please be specific as possible (use a list if needed). -->\r\n<!--- for example: -->\r\n<!--- * create a conda environment for pyspark -->\r\n<!--- * run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered errors in some of the tests for the Microsoft Recommenders project, resulting in 48 warnings and 3 errors in 3.79s.",
        "Issue_preprocessed_content":"Title: error in some of the tests; Content: description there are some errors in which platform does it happen? how do we replicate the issue? expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1716",
        "Issue_title":"[BUG] SASRec integration test unusually long time on AzureML compute cluster",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1652368299000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nRuntime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#L679) varies a lot on the following platforms:\r\n- As part of ADO pipeline, it takes ~562 sec to complete.\r\n- When run as an experiment on AzureML compute cluster triggered using a [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec.\r\n\r\nWe need to investigate why this happens.\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nBoth the machines are of same type (NC6s_V2), and use the same CUDA and CuDNN versions:\r\n`cudatoolkit=11.2`\r\n`cudnn=8.1`\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nTrigger the [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times.\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] sasrec integration test unusually long time on  compute cluster; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\nruntime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#l679) varies a lot on the following platforms:\r\n- as part of ado pipeline, it takes ~562 sec to complete.\r\n- when run as an experiment on  compute cluster triggered using a [github workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec.\r\n\r\nwe need to investigate why this happens.\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\nboth the machines are of same type (nc6s_v2), and use the same cuda and cudnn versions:\r\n`cudatoolkit=11.2`\r\n`cudnn=8.1`\r\n\r\n### how do we replicate the issue?\r\n<!--- please be specific as possible (use a list if needed). -->\r\n<!--- for example: -->\r\n<!--- * create a conda environment for pyspark -->\r\n<!--- * run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\ntrigger the [github workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times.\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runtime of a SASRec integration test varies significantly between two machines of the same type, and needs to investigate why this happens.",
        "Issue_preprocessed_content":"Title: sasrec integration test unusually long time on compute cluster; Content: description runtime of varies a lot on the following platforms as part of ado pipeline, it takes sec to complete. when run as an experiment on compute cluster triggered using a , it takes sec. we need to investigate why this happens. in which platform does it happen? both the machines are of same type , and use the same cuda and cudnn versions how do we replicate the issue? trigger the manually and take a look at pytest logs in the dashboard to see the execution times. expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Issue_title":"[BUG] Remove contrib from azureml",
        "Issue_label":[
            "bug",
            "enhancement",
            "azureml"
        ],
        "Issue_creation_time":1553860230000,
        "Issue_closed_time":1555413510000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":12.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] remove contrib from ; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\nthe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### in which platform does it happen?\r\n<!--- describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- for example: -->\r\n<!--- * azure data science virtual machine. -->\r\n<!--- * azure databricks.  -->\r\n<!--- * other platforms.  -->\r\ndsvm, db\r\n\r\n### expected behavior (i.e. solution)\r\n<!--- for example:  -->\r\n<!--- * the tests for sar pyspark should pass successfully. -->\r\neverything runs\r\n\r\n### other comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they needed to remove the contrib package from a product and check that all the tests pass, while also asking other users if they were using or planning to use the contrib package.",
        "Issue_preprocessed_content":"Title: remove contrib from; Content: description the product team mentioned that contrib package is not recomended for production, we need to remove contrib from here and check that all the tests pass in which platform does it happen? dsvm, db expected behavior everything runs other comments question to are we using contrib anywhere ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Issue_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Issue_label":[
            "bug",
            "operationalization"
        ],
        "Issue_creation_time":1548435846000,
        "Issue_closed_time":1548948415000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: remove  sdk preview private pypi index from operationalize notebook; Content: this [notebook](https:\/\/github.com\/microsoft\/recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to  sdk preview private index. \r\n\r\n    # required packages for  execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/sdktestpypi.azureedge.net\/sdk-release\/preview\/e7501c02541b433786111fe8e140caa1\r\n\r\ngiven that  sdk is now available though regular pypi as a ga product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to remove the reference to the SDK Preview private index from an operationalize notebook.",
        "Issue_preprocessed_content":"Title: remove sdk preview private pypi index from operationalize notebook; Content: this contains a reference to sdk preview private index. required packages for execution, history, and data preparation. extra index url given that sdk is now available though regular pypi as a ga product, and preview versions are unsupported, the extra index url should be removed."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1590",
        "Issue_title":"Unable to open R locfit package in Azure Machine Learning",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1631291354000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I have trained a model locally using the R package locfit. I am now trying to run this in Azure Machine Learning.\r\n\r\nMost guides\/previous questions appear to be in relation to Azure Machine Learning (classic). Although I believe the process outlined in similar posts will be similar (e.g. here, here, I am still unable to get it to work.\r\n\r\nI have outlined the steps I have followed below:\r\n\r\nDownload locfit R package for windows Zip file from here\r\n\r\nPut this downloaded Zip file into a new Zip file entitled \"locfit_package\"\r\n\r\nI upload this \"locfit_package\" zip folder to AML as a dataset (Create Dataset > From Local Files > name: locfit_package dataset type: file > Upload the zip (\"locfit_package\") > Confirm upload is correct\r\n\r\nIn the R terminal I then execute the following code:\r\n\r\n```\r\ninstall.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, verbose = TRUE)\r\n\r\nlibrary(locfit_package, lib.loc=\".\", verbose=TRUE)\r\n\r\nlibrary(locfit)\r\n\r\n```\r\nThe following error message is then returned:\r\n\r\n```\r\nsystem (cmd0): \/usr\/lib\/R\/bin\/R CMD INSTALL\r\n\r\nWarning: invalid package \u2018src\/locfit_package.zip\u2019 Error: ERROR: no packages specified Warning message:\r\n\r\nIn install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status Error in library(locfit_package, lib.loc = \".\", verbose = TRUE) : there is no package called \u2018locfit_package\u2019 Execution halted\r\n\r\n\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to open r locfit package in ; Content: i have trained a model locally using the r package locfit. i am now trying to run this in .\r\n\r\nmost guides\/previous questions appear to be in relation to  (classic). although i believe the process outlined in similar posts will be similar (e.g. here, here, i am still unable to get it to work.\r\n\r\ni have outlined the steps i have followed below:\r\n\r\ndownload locfit r package for windows zip file from here\r\n\r\nput this downloaded zip file into a new zip file entitled \"locfit_package\"\r\n\r\ni upload this \"locfit_package\" zip folder to aml as a dataset (create dataset > from local files > name: locfit_package dataset type: file > upload the zip (\"locfit_package\") > confirm upload is correct\r\n\r\nin the r terminal i then execute the following code:\r\n\r\n```\r\ninstall.packages(\"src\/locfit_package.zip\", lib = \".\", repos = null, verbose = true)\r\n\r\nlibrary(locfit_package, lib.loc=\".\", verbose=true)\r\n\r\nlibrary(locfit)\r\n\r\n```\r\nthe following error message is then returned:\r\n\r\n```\r\nsystem (cmd0): \/usr\/lib\/r\/bin\/r cmd install\r\n\r\nwarning: invalid package \u2018src\/locfit_package.zip\u2019 error: error: no packages specified warning message:\r\n\r\nin install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = null, : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status error in library(locfit_package, lib.loc = \".\", verbose = true) : there is no package called \u2018locfit_package\u2019 execution halted\r\n\r\n\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered challenges while attempting to open the r locfit package in , with the installation of the package resulting in a non-zero exit status.",
        "Issue_preprocessed_content":"Title: unable to open r locfit package in; Content: i have trained a model locally using the r package locfit. i am now trying to run this in . most questions appear to be in relation to . although i believe the process outlined in similar posts will be similar > confirm upload is correct in the r terminal i then execute the following code the following error message is then returned"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1587",
        "Issue_title":"Pandas dataframes with array column values are not correctly persisted as AzureML datasets",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1630657501000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Pandas dataframes with arrays as column values seem to be incorrectly persisted. An example:\r\n\r\n```python\r\ntest_df = pd.DataFrame({'x': [np.random.rand(1000) for _ in range(1000)]})\r\nds = Datastore.get_default(ws)\r\nDataset.Tabular.register_pandas_dataframe(test_df, ds, 'test_dataset')\r\n\r\ntest_df.head()\r\n###\r\n\tx\r\n0\t[0.5044850335733219, 0.6054305053424696, 0.669...\r\n1\t[0.41759815476145723, 0.266477750018155, 0.511...\r\n2\t[0.6777708610872593, 0.16925324567267985, 0.16...\r\n3\t[0.4268294269387616, 0.6540643485117185, 0.033...\r\n4\t[0.6560106490417036, 0.5804652379458484, 0.582...\r\n\r\nDataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head()\r\n###\r\nx\r\n0\tERROR\r\n1\tERROR\r\n2\tERROR\r\n3\tERROR\r\n4\tERROR\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pandas dataframes with array column values are not correctly persisted as  datasets; Content: pandas dataframes with arrays as column values seem to be incorrectly persisted. an example:\r\n\r\n```python\r\ntest_df = pd.dataframe({'x': [np.random.rand(1000) for _ in range(1000)]})\r\nds = datastore.get_default(ws)\r\ndataset.tabular.register_pandas_dataframe(test_df, ds, 'test_dataset')\r\n\r\ntest_df.head()\r\n###\r\n\tx\r\n0\t[0.5044850335733219, 0.6054305053424696, 0.669...\r\n1\t[0.41759815476145723, 0.266477750018155, 0.511...\r\n2\t[0.6777708610872593, 0.16925324567267985, 0.16...\r\n3\t[0.4268294269387616, 0.6540643485117185, 0.033...\r\n4\t[0.6560106490417036, 0.5804652379458484, 0.582...\r\n\r\ndataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head()\r\n###\r\nx\r\n0\terror\r\n1\terror\r\n2\terror\r\n3\terror\r\n4\terror\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where pandas dataframes with array column values were not correctly persisted as datasets.",
        "Issue_preprocessed_content":"Title: pandas dataframes with array column values are not correctly persisted as datasets; Content: pandas dataframes with arrays as column values seem to be incorrectly persisted. an example"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Issue_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1627861519000,
        "Issue_closed_time":1629244160000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pip install `-core` fails on `ruamel.yaml`; Content: ### system specs\r\n**operating system:** windows 10\r\n**python version:** 3.9.5 64-bit\r\n\r\nwhen i run the command:\r\n\r\n```terminal\r\npip install -core\r\n```\r\n\r\ni get an error during the installation, specifically on the `ruamel.yaml` package. i guess the first question i have is there any reason we are restricted to that specific version of `ruamel.yaml`? i was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### partial log\r\n```terminal\r\nattempting uninstall: ruamel.yaml\r\nfound existing installation: ruamel.yaml 0.17.10\r\nuninstalling ruamel.yaml-0.17.10:\r\nsuccessfully uninstalled ruamel.yaml-0.17.10\r\nrunning setup.py install for ruamel.yaml ... error\r\nerror: command errored out with exit status 1:\r\n```\r\n\r\n### full log\r\n[error log from installation run](https:\/\/github.com\/azure\/machinelearningnotebooks\/files\/6913613\/error.log)",
        "Issue_original_content_gpt_summary":"The user encountered an error during the installation of the `ruamel.yaml` package when running the command `pip install -core`, and is wondering if they are restricted to a specific version of the package.",
        "Issue_preprocessed_content":"Title: pip install fails on; Content: system specs operating system windows python version bit when i run the command i get an error during the installation, specifically on the package. i guess the first question i have is there any reason we are restricted to that specific version of ? i was able to install the latest version no problem, so if we could use a later version that would be the easiest fix. partial log full log error log from installation"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1545",
        "Issue_title":"AzureMLException with model.download",
        "Issue_label":[
            "bug",
            "Data4ML",
            "ADO"
        ],
        "Issue_creation_time":1625795312000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Hi!\r\n\r\nWhen trying to download a registered model from the AMLS workspace, I'm getting the following traceback. The file shows up in the `target_dir` (and ADLS path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    432         try:\r\n--> 433             return exec_func()\r\n    434         except exceptions as request_exception:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in exec_func()\r\n    212                                                           max_connections=max_concurrency,\r\n--> 213                                                           validate_content=_validate_check_sum)\r\n    214             file_size = os.stat(path).st_size\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout)\r\n   1855 \r\n-> 1856         with open(file_path, open_mode) as stream:\r\n   1857             blob = self.get_blob_to_stream(\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAzureMLException                          Traceback (most recent call last)\r\n<command-3894832347418984> in <module>\r\n----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\")\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in download(self, target_dir, exist_ok, exists_ok)\r\n    999 \r\n   1000         # download files using sas\r\n-> 1001         file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\r\n   1002         if len(file_paths) == 0:\r\n   1003             raise WebserviceException(\"Illegal state. Unpack={}, Paths in target_dir is \"\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok)\r\n    940                                           \"{}\".format(target_path), logger=module_logger)\r\n    941             sas_to_relative_download_path[sas] = target_path\r\n--> 942             download_file(sas, target_path, stream=True)\r\n    943 \r\n    944         if self.unpack:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency)\r\n    219                                        'present in blob.'.format(file_size, content_length))\r\n    220 \r\n--> 221         return _retry(exec_func, max_retries=max_retries)\r\n    222 \r\n    223     # download using requests.Session\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    443             else:\r\n    444                 module_logger.error('Failed to download file with error: {}'.format(request_exception))\r\n--> 445                 raise AzureMLException('Download of file failed with error: {}'.format(request_exception))\r\n    446         finally:\r\n    447             clean_up_func()\r\n\r\nAzureMLException: AzureMLException:\r\n\tMessage: Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\"\r\n    }\r\n}\r\n\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: exception with model.download; Content: hi!\r\n\r\nwhen trying to download a registered model from the amls workspace, i'm getting the following traceback. the file shows up in the `target_dir` (and adls path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nfilenotfounderror                         traceback (most recent call last)\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    432         try:\r\n--> 433             return exec_func()\r\n    434         except exceptions as request_exception:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in exec_func()\r\n    212                                                           max_connections=max_concurrency,\r\n--> 213                                                           validate_content=_validate_check_sum)\r\n    214             file_size = os.stat(path).st_size\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout)\r\n   1855 \r\n-> 1856         with open(file_path, open_mode) as stream:\r\n   1857             blob = self.get_blob_to_stream(\r\n\r\nfilenotfounderror: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\nexception                          traceback (most recent call last)\r\n<command-3894832347418984> in <module>\r\n----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\")\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/core\/model.py in download(self, target_dir, exist_ok, exists_ok)\r\n    999 \r\n   1000         # download files using sas\r\n-> 1001         file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\r\n   1002         if len(file_paths) == 0:\r\n   1003             raise webserviceexception(\"illegal state. unpack={}, paths in target_dir is \"\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok)\r\n    940                                           \"{}\".format(target_path), logger=module_logger)\r\n    941             sas_to_relative_download_path[sas] = target_path\r\n--> 942             download_file(sas, target_path, stream=true)\r\n    943 \r\n    944         if self.unpack:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency)\r\n    219                                        'present in blob.'.format(file_size, content_length))\r\n    220 \r\n--> 221         return _retry(exec_func, max_retries=max_retries)\r\n    222 \r\n    223     # download using requests.session\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    443             else:\r\n    444                 module_logger.error('failed to download file with error: {}'.format(request_exception))\r\n--> 445                 raise exception('download of file failed with error: {}'.format(request_exception))\r\n    446         finally:\r\n    447             clean_up_func()\r\n\r\nexception: exception:\r\n\tmessage: download of file failed with error: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\tinnerexception none\r\n\terrorresponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"download of file failed with error: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\"\r\n    }\r\n}\r\n\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when trying to download a registered model from the AMLS workspace, resulting in a 0 byte file.",
        "Issue_preprocessed_content":"Title: exception with; Content: hi! when trying to download a registered model from the amls workspace, i'm getting the following traceback. the file shows up in the however the size is bytes, so it is making the file, however no data is being transferred into it."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Issue_title":"Bug: Failure while loading azureml_run_type_providers",
        "Issue_label":[
            "bug",
            "Training",
            "ADO"
        ],
        "Issue_creation_time":1625218579000,
        "Issue_closed_time":1630367426000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: bug: failure while loading _run_type_providers; Content: in a fresh conda environment, i get several warnings that halt the script execution:\r\n```\r\n...\r\nfailure while loading _run_type_providers. failed to load entrypoint .pipelinerun = .pipeline.core.run:pipelinerun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), requirement.parse('docker<5.0.0'), {'-core'}).\r\n...\r\n```\r\n\r\nmy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - -sdk==1.31.0\r\n      - -defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - -mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nthe fix is to specify `docker<5.0.0`. perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### document details\r\n\r\n\u26a0 *do not edit this section. it is required for docs.microsoft.com \u279f github issue linking.*\r\n\r\n* id: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* version independent id: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n*  [ sdk for python -  python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* content source: [-docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/index.md)\r\n* service: **machine-learning**\r\n* sub-service: **core**\r\n* github login: @trevorbye\r\n* microsoft alias: **trbye**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while loading _run_type_providers in a fresh conda environment, which was resolved by specifying 'docker<5.0.0' in the dependencies.",
        "Issue_preprocessed_content":"Title: bug failure while loading; Content: in a fresh conda environment, i get several warnings that halt the script execution my environment is specified by the fix is to specify . perhaps, there are some wrong deps checks somewhere. document details do not edit this section. it is required for github issue id eb c f d a a bec version independent id e c fe a efc c f dc content content source service machine learning sub service core github login microsoft alias trbye"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Issue_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Issue_label":[
            "bug",
            "ADO"
        ],
        "Issue_creation_time":1624997173000,
        "Issue_closed_time":1626388206000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: broken link in aml doc to .core.runconfig.mpiconfiguration; Content: \r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### document details\r\n\r\n\u26a0 *do not edit this section. it is required for docs.microsoft.com \u279f github issue linking.*\r\n\r\n* id: 109d9284-e234-5086-5da6-4155291361c8\r\n* version independent id: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n*  [.core.scriptrunconfig class -  python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.scriptrunconfig?view=azure-ml-py)\r\n* content source: [-docset\/stable\/docs-ref-autogen\/-core\/.core.scriptrunconfig.yml](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/stable\/docs-ref-autogen\/-core\/.core.scriptrunconfig.yml)\r\n* service: **machine-learning**\r\n* sub-service: **core**\r\n* github login: @debfro\r\n* microsoft alias: **debfro**",
        "Issue_original_content_gpt_summary":"The user @debfro encountered a broken link in the Azure Machine Learning Python API documentation for the .core.scriptrunconfig class to the .core.runconfig.mpiconfiguration class.",
        "Issue_preprocessed_content":"Title: broken link in aml doc to; Content: document details do not edit this section. it is required for github issue id d e da c version independent id cc c a faa a ee b cf fb d content content source service machine learning sub service core github login microsoft alias debfro"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Issue_title":"Warning while loading the azureml.core",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1624519136000,
        "Issue_closed_time":1626388114000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: warning while loading the .core; hi,\r\n\r\ni have installed the  using below environment yml, installation happened without any issues but when i import the .core i am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # the python interpreter version.\r\n  # currently  only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # base  sdk\r\n  - -sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**exception**\r\nimport .core\r\n`failure while loading _run_type_providers. failed to load entrypoint automl = .train.automl.run:automlrun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), requirement.parse('cryptography<4.0.0,>=3.3.1; Content: extra == \"crypto\"'), {'pyjwt'}).`\r\n\r\n sdk version:  1.31.0\r\n\r\n\r\nplease help.\r\nthanks",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while loading the .core, resulting in an exception due to a conflict between the cryptography version and the pyjwt version.",
        "Issue_preprocessed_content":"Title: warning while loading the; Content: hi, i have installed the using below environment yml, installation happened without any issues but when i import the i am getting exception. conda environment yml exception import sdk version please help. thanks"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Issue_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Issue_label":[
            "bug",
            "Pipelines",
            "ADO"
        ],
        "Issue_creation_time":1624292044000,
        "Issue_closed_time":1626719342000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  pipelines: expected a steprun object but received <class '.core.run.run'> instead.; Content: i am running a lightly edited version of this pipeline example: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `expected a steprun object but received <class '.core.run.run'> instead.`\r\n\r\ni am also getting this same warning in other pipelines i make and i cannot figure out what is causing it.\r\n\r\nhere is a slightly reduced mwe for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom .core import workspace, datastore, dataset, experiment\r\nfrom .core.authentication import serviceprincipalauthentication\r\nfrom .core.runconfig import runconfiguration, default_cpu_image\r\nfrom .core.conda_dependencies import condadependencies\r\nfrom .core.compute import computetarget, amlcompute\r\nfrom .core.compute_target import computetargetexception\r\nfrom .data import outputfiledatasetconfig\r\nfrom .pipeline.steps import pythonscriptstep\r\nfrom .pipeline.core import pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = serviceprincipalauthentication(tenant_id=os.environ['aml_tenant_id'],\r\n                                    service_principal_id=os.environ['aml_principal_id'],\r\n                                    service_principal_password=os.environ['aml_principal_pass'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = pythonscriptstep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=runconfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=true\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom .core import run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=true)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nexpected a steprun object but received <class '.core.run.run'> instead.\r\nthis usually indicates a package conflict with one of the dependencies of -core or -pipeline-core.\r\nplease check for package conflicts in your python environment\r\n```\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they received an error message indicating a package conflict with one of the dependencies of -core or -pipeline-core when running a pipeline.",
        "Issue_preprocessed_content":"Title: pipelines expected a steprun object but received instead.; Content: i am running a lightly edited version of this pipeline example and it is yielding me this error i am also getting this same warning in other pipelines i make and i cannot figure out what is causing it. here is a slightly reduced mwe for clarity"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Issue_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Issue_label":[
            "bug",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1622675126000,
        "Issue_closed_time":1631108652000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: 'clientrequesterror' when trying to use azure computer vision api from  notebook; Content: i'm trying to use azure computer vision's ocr api in an  notebook. however there seems to be an error when trying to call the computer vision api from an  notebook. the same code works when i'm running it on a local machine.\r\ni'm following azure computer vision's ocr quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nwhen running the following code, `computervision_client.read(read_image_url, raw=true)` does not return but throws an exception.\r\nexception:\r\n`clientrequesterror: error occurred in request., connectionerror: httpsconnectionpool(host='some-host.cognitiveservices.azure.com', port=443): max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingorder=basic (caused by newconnectionerror('<urllib3.connection.httpsconnection object at 0x7f59e0102820>: failed to establish a new connection: [errno -3] temporary failure in name resolution'))`\r\n\r\ncode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import computervisionclient\r\nfrom azure.cognitiveservices.vision.computervision.models import operationstatuscodes\r\nfrom azure.cognitiveservices.vision.computervision.models import visualfeaturetypes\r\nfrom msrest.authentication import cognitiveservicescredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom pil import image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nauthenticate\r\nauthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"computervision_key\"]\r\nendpoint = os.environ[\"computervision_url\"]\r\n\r\ncomputervision_client = computervisionclient(endpoint, cognitiveservicescredentials(subscription_key))\r\n\r\n'''\r\nocr: read file using the read api, extract text - remote\r\nthis example will extract text in an image, then print results, line by line.\r\nthis api call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== read file - remote =====\")\r\n# get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/microsoftdocs\/azure-docs\/master\/articles\/cognitive-services\/computer-vision\/images\/readsample.jpg\"\r\n\r\n# call api with url and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=true) # <- throws exception\r\n```\r\n\r\nused azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\n-automl-core                           1.29.0\r\n-contrib-dataset                       1.29.0\r\n-core                                  1.29.0.post1\r\n-dataprep                              2.15.1\r\n-dataprep-native                       33.0.0\r\n-dataprep-rslex                        1.13.0\r\n-dataset-runtime                       1.29.0\r\n-pipeline-core                         1.29.0\r\n-pipeline-steps                        1.29.0\r\n-telemetry                             1.29.0\r\n-train-automl-client                   1.29.0\r\n-train-core                            1.29.0\r\n-train-restclients-hyperdrive          1.29.0\r\n-widgets                               1.29.0.post1\r\n```\r\n\r\nmaybe related to #1107",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to use the Azure Computer Vision API from an  notebook, despite the same code working on a local machine.",
        "Issue_preprocessed_content":"Title: 'clientrequesterror' when trying to use azure computer vision api from notebook; Content: i'm trying to use azure computer vision's ocr api in an notebook. however there seems to be an error when trying to call the computer vision api from an notebook. the same code works when i'm running it on a local machine. i'm following azure computer vision's ocr quickstart when running the following code, does not return but throws an exception. exception code used azure packages maybe related to"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1341",
        "Issue_title":"azureml-defaults not described ",
        "Issue_label":[
            "doc-bug",
            "product-issue",
            "MLOps",
            "ADO"
        ],
        "Issue_creation_time":1613523098000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"\r\n[Enter feedback here]\r\n\r\nWe need details description of `azureml-defaults`. \r\n\r\nWe need this when deployment. In training, we usually use `azureml-core`. In deployment, `azureml-defaults` is necessary (only `azureml-core` is not enough to deploy). I heard `azureml-defaults` includes `azureml-core`. But it is not documented.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: -defaults not described ; Content: \r\n[enter feedback here]\r\n\r\nwe need details description of `-defaults`. \r\n\r\nwe need this when deployment. in training, we usually use `-core`. in deployment, `-defaults` is necessary (only `-core` is not enough to deploy). i heard `-defaults` includes `-core`. but it is not documented.\r\n\r\n---\r\n#### document details\r\n\r\n\u26a0 *do not edit this section. it is required for docs.microsoft.com \u279f github issue linking.*\r\n\r\n* id: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* version independent id: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n*  [install the  sdk for python -  python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* content source: [-docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/install.md)\r\n* service: **machine-learning**\r\n* sub-service: **core**\r\n* github login: @harneetvirk\r\n* microsoft alias: **harnvir**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the lack of documentation regarding the `-defaults` option when deploying the Azure Machine Learning SDK for Python, which is necessary for deployment but not included in the training process.",
        "Issue_preprocessed_content":"Title: defaults not described; Content: enter feedback here we need details description of . we need this when deployment. in training, we usually use . in deployment, is necessary . i heard includes . but it is not documented. document details do not edit this section. it is required for github issue id e e a b b db efb version independent id e a ac b a cc d a cb b b content content source service machine learning sub service core github login microsoft alias harnvir"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1417",
        "Issue_title":"python package azureml-contrib-pipeline-steps 1.20.0 not working ",
        "Issue_label":[
            "bug",
            "Compute",
            "ADO"
        ],
        "Issue_creation_time":1611092820000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"## Describe the issue\r\n\r\nversion 1.20.0 of python package azureml-contrib-pipeline-steps throws (works fine on version 1.19 or 1.18)\r\n\r\n File \"C:\/Users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run\r\n    pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\core\\_experiment_method.py\", line 97, in wrapper\r\n    return init_func(self, *args, **kwargs)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\pipeline.py\", line 177, in __init__\r\n    self._graph = self._graph_builder.build(self._name, steps, finalize=False)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1481, in build\r\n    graph = self.construct(name, steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1503, in construct\r\n    self.process_collection(steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1539, in process_collection\r\n    builder.process_collection(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1830, in process_collection\r\n    self._base_builder.process_collection(item)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1533, in process_collection\r\n    return self.process_step(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1577, in process_step\r\n    node = step.create_node(self._graph, self._default_datastore, self._context)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\steps\\python_script_step.py\", line 243, in create_node\r\n    return super(PythonScriptStep, self).create_node(\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node\r\n    self._set_compute_params_to_node(node,\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node\r\n    self._module_param_provider.set_params_to_node(\r\nTypeError: _set_params_to_node_hook() got an unexpected keyword argument 'command'\r\n\r\n## Minimal example\r\n\r\n```python\r\nfrom azureml.core import Workspace\r\n\r\nws = Workspace.from_config()\r\n\r\n\r\nsplit_step = PythonScriptStep(\r\n        name=\"Train Test Split\",\r\n        script_name=\"obj_segment_step_data_process.py\",\r\n        arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\r\n                   \"--train-split\", train_split_data, \"--test-split\", test_split_data,\r\n                   \"--test-size\", 50],\r\n        compute_target=compute_target,\r\n        runconfig=aml_run_config,\r\n        source_directory=source_directory,\r\n        allow_reuse=False\r\n    )\r\n\r\npipeline_steps = [split_step ]\r\n\r\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n```\r\n\r\n## Additional context\r\nI am using aml sdk 1.20. no type errors with version 1.19\/1.18 of azureml-contrib-pipeline-steps.\r\n-\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: python package -contrib-pipeline-steps 1.20.0 not working ; Content: ## describe the issue\r\n\r\nversion 1.20.0 of python package -contrib-pipeline-steps throws (works fine on version 1.19 or 1.18)\r\n\r\n file \"c:\/users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run\r\n    pipeline = pipeline(workspace=ws, steps=pipeline_steps)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\core\\_experiment_method.py\", line 97, in wrapper\r\n    return init_func(self, *args, **kwargs)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\pipeline.py\", line 177, in __init__\r\n    self._graph = self._graph_builder.build(self._name, steps, finalize=false)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1481, in build\r\n    graph = self.construct(name, steps)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1503, in construct\r\n    self.process_collection(steps)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1539, in process_collection\r\n    builder.process_collection(collection)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1830, in process_collection\r\n    self._base_builder.process_collection(item)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1533, in process_collection\r\n    return self.process_step(collection)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1577, in process_step\r\n    node = step.create_node(self._graph, self._default_datastore, self._context)\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\steps\\python_script_step.py\", line 243, in create_node\r\n    return super(pythonscriptstep, self).create_node(\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node\r\n    self._set_compute_params_to_node(node,\r\n  file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node\r\n    self._module_param_provider.set_params_to_node(\r\ntypeerror: _set_params_to_node_hook() got an unexpected keyword argument 'command'\r\n\r\n## minimal example\r\n\r\n```python\r\nfrom .core import workspace\r\n\r\nws = workspace.from_config()\r\n\r\n\r\nsplit_step = pythonscriptstep(\r\n        name=\"train test split\",\r\n        script_name=\"obj_segment_step_data_process.py\",\r\n        arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\r\n                   \"--train-split\", train_split_data, \"--test-split\", test_split_data,\r\n                   \"--test-size\", 50],\r\n        compute_target=compute_target,\r\n        runconfig=aml_run_config,\r\n        source_directory=source_directory,\r\n        allow_reuse=false\r\n    )\r\n\r\npipeline_steps = [split_step ]\r\n\r\npipeline = pipeline(workspace=ws, steps=pipeline_steps)\r\n```\r\n\r\n## additional context\r\ni am using aml sdk 1.20. no type errors with version 1.19\/1.18 of -contrib-pipeline-steps.\r\n-\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue with version 1.20.0 of python package -contrib-pipeline-steps, which threw a typeerror when running a pipeline, while versions 1.19 and 1.18 of the package worked fine.",
        "Issue_preprocessed_content":"Title: python package contrib pipeline steps not working; Content: describe the issue version of python package contrib pipeline steps throws file line , in run pipeline pipeline file line , in wrapper return args, kwargs file line , in steps, finalize false file line , in build graph steps file line , in construct file line , in file line , in file line , in return file line , in node file line , in return super file line , in file line , in typeerror got an unexpected keyword argument 'command' minimal example additional context i am using aml sdk no type errors with version of contrib pipeline steps."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1797",
        "Issue_title":"connecting to VScode to AzureML",
        "Issue_label":[
            "customer-issue",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1668197837000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: connecting to vscode to ; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.20.0\r\nos: win32\r\nos release: 10.0.19044\r\nproduct: visual studio code\r\nproduct version: 1.72.2\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue connecting to VSCode, with an unknown error retrieving subscriptions from the Azure Account Extension, while using Visual Studio Code version 1.72.2 on Windows 10.",
        "Issue_preprocessed_content":"Title: connecting to vscode to; Content: does this occur consistently? repro steps . . action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1714",
        "Issue_title":"I keep on getting this error continuously for Azure Machine Learning extension",
        "Issue_label":[
            "customer-issue",
            "triage-needed",
            "t-unknown-sub-error"
        ],
        "Issue_creation_time":1662929331000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Azure sign in\r\n2. Sign in using Azure portal. You get the sign in successful, you may close the window message, but Azure asks to sign in again.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.17.2022090809\r\nOS: win32\r\nOS Release: 10.0.19042\r\nProduct: Visual Studio Code - Insiders\r\nProduct Version: 1.72.0-insider\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: i keep on getting this error continuously for  extension; Content: <!-- important: please be sure to remove any private information before submitting. -->\r\n\r\ndoes this occur consistently? <!-- todo: type yes or no -->\r\nrepro steps:\r\n<!-- todo: share the steps needed to reliably reproduce the problem. please include actual and expected results. -->\r\n\r\n1. azure sign in\r\n2. sign in using azure portal. you get the sign in successful, you may close the window message, but azure asks to sign in again.\r\n\r\naction: azureaccount.onsessionschanged\r\nerror type: 123\r\nerror message: unknown error retrieving susbcriptions from azure account extension\r\n\r\n\r\nversion: 0.17.2022090809\r\nos: win32\r\nos release: 10.0.19042\r\nproduct: visual studio code - insiders\r\nproduct version: 1.72.0-insider\r\nlanguage: en\r\n\r\n<details>\r\n<summary>call stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when signing into Azure using the Visual Studio Code - Insiders extension, with an unknown error retrieving subscriptions from the Azure Account extension.",
        "Issue_preprocessed_content":"Title: i keep on getting this error continuously for extension; Content: does this occur consistently? repro steps . azure sign in . sign in using azure portal. you get the sign in successful, you may close the window message, but azure asks to sign in again. action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code insiders product version language en call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1627",
        "Issue_title":"AzureML Prompts twice to login when VS Code (Insiders) loads",
        "Issue_label":[
            "bug",
            "area-sign-in"
        ],
        "Issue_creation_time":1659626460000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Expected Behavior\r\nIf the user is logged out, the AML extension should not prompt to login until the user specifically tries to run an AzureML command. Prompting when VS Code loads is disruptive and unnecessary, and no other extensions for AWS or Azure do this.\r\n\r\n## Actual Behavior\r\nIf you are signed out of the Azure ML extension and reload VS Code, you are prompted to login when it loads (Issue #1). If you click cancel, you are prompted again (#2). \r\n\r\n## Steps to Reproduce the Problem\r\n  1. Install the Azure ML Extension\r\n  2. Login\r\n  3. Logout\r\n  4. Reload VS Code\r\n  5. Click \"Cancel\" when prompted to login\r\n\r\n\r\n## Specifications\r\nAzure ML Extension Version 0.16.0\r\n \r\nVersion: 1.70.0-insider (Universal)\r\nCommit: da76f93349a72022ca4670c1b84860304616aaa2\r\nDate: 2022-08-03T05:55:27.651Z (1 day ago)\r\nElectron: 18.3.5\r\nChromium: 100.0.4896.160\r\nNode.js: 16.13.2\r\nV8: 10.0.139.17-electron.0\r\nOS: Darwin x64 21.6.0\r\n\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  prompts twice to login when vs code (insiders) loads; Content: ## expected behavior\r\nif the user is logged out, the aml extension should not prompt to login until the user specifically tries to run an  command. prompting when vs code loads is disruptive and unnecessary, and no other extensions for aws or azure do this.\r\n\r\n## actual behavior\r\nif you are signed out of the  extension and reload vs code, you are prompted to login when it loads (issue #1). if you click cancel, you are prompted again (#2). \r\n\r\n## steps to reproduce the problem\r\n  1. install the  extension\r\n  2. login\r\n  3. logout\r\n  4. reload vs code\r\n  5. click \"cancel\" when prompted to login\r\n\r\n\r\n## specifications\r\n extension version 0.16.0\r\n \r\nversion: 1.70.0-insider (universal)\r\ncommit: da76f93349a72022ca4670c1b84860304616aaa2\r\ndate: 2022-08-03t05:55:27.651z (1 day ago)\r\nelectron: 18.3.5\r\nchromium: 100.0.4896.160\r\nnode.js: 16.13.2\r\nv8: 10.0.139.17-electron.0\r\nos: darwin x64 21.6.0\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the AML extension prompts twice to login when VS Code (Insiders) loads, even after clicking \"cancel\" the first time.",
        "Issue_preprocessed_content":"Title: prompts twice to login when vs code loads; Content: expected behavior if the user is logged out, the aml extension should not prompt to login until the user specifically tries to run an command. prompting when vs code loads is disruptive and unnecessary, and no other extensions for aws or azure do this. actual behavior if you are signed out of the extension and reload vs code, you are prompted to login when it loads . if you click cancel, you are prompted again . steps to reproduce the problem . install the extension . login . logout . reload vs code . click cancel when prompted to login specifications extension version version commit da f a ca c b aaa date electron chromium v os darwin x"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Issue_title":"Can not add AzureML extention on openshift cluster ",
        "Issue_label":[
            "bug",
            "triage"
        ],
        "Issue_creation_time":1653563264000,
        "Issue_closed_time":1654438640000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can not add  extention on openshift cluster ; error when adding extetnion \r\naz k8s-extension create --name -extension --extension-type microsoft..kubernetes --config enabletraining= cluster-type conneced--cluster-name <your-aks-cluster-name> --resource-group <your-rg-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"creating\"}\r\ncli.azure.cli.core.sdk.policies: request url: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: request method: 'get'\r\ncli.azure.cli.core.sdk.policies: request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'commandname': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'parametersetname': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'user-agent': 'azurecli\/2.36.0 (msi) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 python\/3.10.4 (windows-10-10.0.19044-sp0)'\r\ncli.azure.cli.core.sdk.policies:     'authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: request body:\r\ncli.azure.cli.core.sdk.policies: this request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3a%2f%2fmanagement.azure.com%2f&data=05%7c01%7cjohan.andolf%40microsoft.com%7c37b5d083c3d7447f133208da3e347a4a%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c637890692414003835%7cunknown%7ctwfpbgzsb3d8eyjwijoimc4wljawmdailcjqijoiv2lumziilcjbtii6ik1hawwilcjxvci6mn0%3d%7c3000%7c%7c%7c&sdata=1kwoqv7fwagqmyol4w7wfzrbf%2bctkz9xucdbe%2fkggka%3d&reserved=0) \"get \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-version=2022-03-01 http\/1.1\" 200 none\r\ncli.azure.cli.core.sdk.policies: response status: 200\r\ncli.azure.cli.core.sdk.policies: response headers:\r\ncli.azure.cli.core.sdk.policies:     'cache-control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'transfer-encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'content-type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'content-encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'vary': 'accept-encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'strict-transport-security': 'max-age=31536000; Content: includesubdomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'x-content-type-options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'swedencentral:20220525t095135z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'date': 'wed, 25 may 2022 09:51:34 gmt'\r\ncli.azure.cli.core.sdk.policies: response \r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"failed\",\"error\":{\"code\":\"extensioncreationfailed\",\"message\":\" error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: traceback (most recent call last):\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.operationfailed: operation failed or canceled\r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\ntraceback (most recent call last):\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.httpresponseerror: (extensioncreationfailed)  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\ncode: extensioncreationfailed\r\nmessage:  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (extensioncreationfailed)  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\ncode: extensioncreationfailed\r\nmessage:  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (extensioncreationfailed)  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\ncode: extensioncreationfailed\r\nmessage:  error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: event: cli.postexecute [<function azclilogging.deinit_cmd_metadata_logging at 0x0387c190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: save telemetry record of length 3581 in cache\r\ntelemetry.check: returns positive.\r\ntelemetry.main: begin creating telemetry upload process.\r\ntelemetry.process: creating upload process: \"c:\\program files (x86)\\microsoft sdks\\azure\\cli2\\python.exe c:\\program files (x86)\\microsoft sdks\\azure\\cli2\\lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc c:\\users\\ropa04\\.azure\"\r\ntelemetry.process: return from creating process\r\ntelemetry.main: finish creating telemetry upload process.",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to add an extension to an OpenShift cluster, resulting in an error due to an inability to get the status from the local crd.",
        "Issue_preprocessed_content":"Title: can not add extention on openshift cluster; Content: error when adding extetnion az k s extension create name extension extension type config enabletraining cluster type conneced cluster name resource group scope cluster request url request method 'get' request headers 'x ms client request id' 'f bf c dc d ec a c abda e d' 'commandname' 'k s extension create' 'parametersetname' ' name extension type cluster type cluster name resource group name auto upgrade scope debug config' 'user agent' ' 'authorization' ' ' request body this request has no body get none response status response headers 'cache control' 'no cache' 'pragma' 'no cache' 'transfer encoding' 'chunked' 'content type' charset utf ' 'content encoding' 'gzip' 'expires' ' ' 'vary' 'accept encoding' 'x ms ratelimit remaining subscription reads' ' ' 'strict transport security' 'max age ; includesubdomains' 'api supported versions' ' preview, preview, preview, , preview, preview, , preview' 'x content type options' 'nosniff' 'x ms request id' ' d b f d d c c bef d ' 'x ms correlation request id' ' d b f d d c c bef d ' 'x ms routing request id' 'swedencentral t z d b f d d c c bef d ' 'date' 'wed, may gmt' response content is called with an exception traceback file line , in run file line , in operation failed or canceled during handling of the above exception, another exception occurred traceback file line , in invoke file line , in execute file line , in file line , in file line , in file line , in file line , in result file line , in file line , in wait file line , in file line , in run error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error event exit code command ran in seconds save telemetry record of length in cache returns positive. begin creating telemetry upload process. creating upload process files files return from creating process finish creating telemetry upload process."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Issue_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Issue_label":[
            "bug",
            "dev workflow"
        ],
        "Issue_creation_time":1652108924000,
        "Issue_closed_time":1657547192000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: models that override  crossval_count with a value bigger than 1 automatically switch to train on  even if user overrides --=false; Content: models that override  crossval_count with a value bigger than 1 automatically switch to train on  even if user overrides --=false\r\n\r\nthis behaviour is a bit confusing and i had to debug the code to understand what was happening. i would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what i want that is train locally.\r\n\r\nrepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.deepsmilecrck \r\n\r\nalso the histopathology.deepsmilecrck is not trainable because it does not have a default encoder type. should we flag base classes as not trainable and throw an error?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where models that override crossval_count with a value bigger than 1 automatically switch to train on even if user overrides --=false, which is confusing and unexpected.",
        "Issue_preprocessed_content":"Title: models that override with a value bigger than automatically switch to train on even if user overrides false; Content: models that override with a value bigger than automatically switch to train on even if user overrides false this behaviour is a bit confusing and i had to debug the code to understand what was happening. i would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what i want that is train locally. repro with also the is not trainable because it does not have a default encoder type. should we flag base classes as not trainable and throw an error?"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/410",
        "Issue_title":"[BUG] Some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_azureml notebook",
        "Issue_label":[
            "bug",
            "bug-bash"
        ],
        "Issue_creation_time":1573072566000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nThe introduction section of the 11_exploring_hyperparameters_on_azureml notebook under object detection includes two broken links:\r\n` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)`\r\n`[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)`\r\n\r\nThe master branch of this repo (which I am working from...please tell me that was intended...) does not contain these notebooks. \r\n\r\n### In which platform does it happen?\r\nAll.\r\n\r\n### How do we replicate the issue?\r\nClick the links\r\n\r\n### Expected behavior (i.e. solution)\r\nNotebooks are present or links are removed\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_ notebook; Content: ### description\r\n\r\nthe introduction section of the 11_exploring_hyperparameters_on_ notebook under object detection includes two broken links:\r\n` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)`\r\n`[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)`\r\n\r\nthe master branch of this repo (which i am working from...please tell me that was intended...) does not contain these notebooks. \r\n\r\n### in which platform does it happen?\r\nall.\r\n\r\n### how do we replicate the issue?\r\nclick the links\r\n\r\n### expected behavior (i.e. solution)\r\nnotebooks are present or links are removed\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered broken links in the introduction section of the 11_exploring_hyperparameters_on_ notebook under object detection, which were not present in the master branch of the repository.",
        "Issue_preprocessed_content":"Title: some links to notebooks in introduction are broken in notebook; Content: description the introduction section of the notebook under object detection includes two broken links the master branch of this repo does not contain these notebooks. in which platform does it happen? all. how do we replicate the issue? click the links expected behavior notebooks are present or links are removed other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/409",
        "Issue_title":"[FEATURE_REQUEST] Provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_azureml notebook",
        "Issue_label":[
            "bug-bash"
        ],
        "Issue_creation_time":1573072237000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nUsers need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. Some guidance within the notebook on how to obtain these values and fill in the strings would be helpful.\r\n\r\nIt would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on.\r\n\r\n### Expected behavior with the suggested feature\r\n\r\nUsers who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. Novice users receive some guidance on how to obtain their Azure subscription id without having to reference other notebooks.\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [feature_request] provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_ notebook; Content: ### description\r\n\r\nusers need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. some guidance within the notebook on how to obtain these values and fill in the strings would be helpful.\r\n\r\nit would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on.\r\n\r\n### expected behavior with the suggested feature\r\n\r\nusers who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. novice users receive some guidance on how to obtain their azure subscription id without having to reference other notebooks.\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges in obtaining a subscription ID and filling in the strings in the third code cell of the 11_exploring_hyperparameters_on_ notebook, and requested guidance on how to do so, as well as an error message if the strings are not filled in.",
        "Issue_preprocessed_content":"Title: provide guidance on how to obtain a subscription id in notebook; Content: description users need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. some guidance within the notebook on how to obtain these values and fill in the strings would be helpful. it would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to later on. expected behavior with the suggested feature users who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. novice users receive some guidance on how to obtain their azure subscription id without having to reference other notebooks. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/408",
        "Issue_title":"[BUG] Link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_azureml notebook is broken",
        "Issue_label":[
            "bug",
            "bug-bash"
        ],
        "Issue_creation_time":1573071661000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nThe 11_exploring_hyperparameters_on_azureml notebook contains the following link in markdown:\r\n`[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)`\r\n\r\nThe link does not resolve properly -- it appears the relative location of the notebook has changed.\r\n\r\n### In which platform does it happen?\r\nAll\r\n\r\n### How do we replicate the issue?\r\nClick the link\r\n\r\n### Expected behavior (i.e. solution)\r\nLink works\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_ notebook is broken; Content: ### description\r\n\r\nthe 11_exploring_hyperparameters_on_ notebook contains the following link in markdown:\r\n`[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)`\r\n\r\nthe link does not resolve properly -- it appears the relative location of the notebook has changed.\r\n\r\n### in which platform does it happen?\r\nall\r\n\r\n### how do we replicate the issue?\r\nclick the link\r\n\r\n### expected behavior (i.e. solution)\r\nlink works\r\n\r\n### other comments\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the link to the 20_azure_workspace_setup.ipynb in the 11_exploring_hyperparameters_on_ notebook was broken.",
        "Issue_preprocessed_content":"Title: link to in notebook is broken; Content: description the notebook contains the following link in markdown the link does not resolve properly it appears the relative location of the notebook has changed. in which platform does it happen? all how do we replicate the issue? click the link expected behavior link works other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/404",
        "Issue_title":"[FEATURE_REQUEST] AzureML may need to be updated 1.0.30->1.0.72?",
        "Issue_label":[
            "bug-bash"
        ],
        "Issue_creation_time":1573068707000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\nThe current version of AzureML is a little dated \r\nhttps:\/\/github.com\/microsoft\/ComputerVision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#L41\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [feature_request]  may need to be updated 1.0.30->1.0.72?; Content: ### description\r\nthe current version of  is a little dated \r\nhttps:\/\/github.com\/microsoft\/computervision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#l41\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with needing to update the version of a feature from 1.0.30 to 1.0.72.",
        "Issue_preprocessed_content":"Title: may need to be updated; Content: description the current version of is a little dated"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/396",
        "Issue_title":"[FEATURE_REQUEST] Install utils_cv as a pip wheel in AzureML",
        "Issue_label":[
            "bug-bash"
        ],
        "Issue_creation_time":1573065169000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\nIn https:\/\/github.com\/microsoft\/ComputerVision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_azureml.ipynb\r\nyou copy the whole directory in order to make use of the utils_cv\r\nThis is a bit cumbersome and unecesarily copies things around. You can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-\r\n\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [feature_request] install utils_cv as a pip wheel in ; Content: ### description\r\nin https:\/\/github.com\/microsoft\/computervision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_.ipynb\r\nyou copy the whole directory in order to make use of the utils_cv\r\nthis is a bit cumbersome and unecesarily copies things around. you can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of having to copy a whole directory in order to make use of the utils_cv, and proposed creating a pip wheel package of the utils_cv and adding it as a dependency.",
        "Issue_preprocessed_content":"Title: install as a pip wheel in; Content: description in you copy the whole directory in order to make use of the this is a bit cumbersome and unecesarily copies things around. you can create a pip wheel package of your and add it as a dependency see here"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/332",
        "Issue_title":"[BUG] Error in o16n with AzureML notebooks",
        "Issue_label":[
            "bug",
            "test"
        ],
        "Issue_creation_time":1569349581000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nThis is the error, it looks it is related to the deployment of ACI and AKS resources. \r\n\r\n\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:35:17.380577', 'duration': 1113.334717, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [26]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-26-21aec20dbbb2> in <module>\r\nE                 1 # Deploy the web service\r\nE           ----> 2 service.wait_for_deployment(show_output=True)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<01:03,  1.01cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:44,  1.40cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:31,  1.92cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:04<01:13,  1.24s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<01:00,  1.07s\/cell]\r\nExecuting:  15%|\u2588\u258c        | 10\/65 [00:05<00:43,  1.27cell\/s]\r\nExecuting:  18%|\u2588\u258a        | 12\/65 [00:05<00:30,  1.72cell\/s]\r\nExecuting:  20%|\u2588\u2588        | 13\/65 [00:06<00:22,  2.26cell\/s]\r\nExecuting:  23%|\u2588\u2588\u258e       | 15\/65 [00:07<00:23,  2.15cell\/s]\r\nExecuting:  26%|\u2588\u2588\u258c       | 17\/65 [00:07<00:16,  2.87cell\/s]\r\nExecuting:  28%|\u2588\u2588\u258a       | 18\/65 [00:13<01:34,  2.00s\/cell]\r\nExecuting:  31%|\u2588\u2588\u2588       | 20\/65 [00:13<01:04,  1.43s\/cell]\r\nExecuting:  32%|\u2588\u2588\u2588\u258f      | 21\/65 [00:15<01:06,  1.51s\/cell]\r\nExecuting:  35%|\u2588\u2588\u2588\u258c      | 23\/65 [00:15<00:45,  1.08s\/cell]\r\nExecuting:  37%|\u2588\u2588\u2588\u258b      | 24\/65 [00:16<00:45,  1.11s\/cell]\r\nExecuting:  38%|\u2588\u2588\u2588\u258a      | 25\/65 [00:18<00:54,  1.37s\/cell]\r\nExecuting:  42%|\u2588\u2588\u2588\u2588\u258f     | 27\/65 [00:18<00:37,  1.01cell\/s]\r\nExecuting:  43%|\u2588\u2588\u2588\u2588\u258e     | 28\/65 [00:20<00:50,  1.37s\/cell]\r\nExecuting:  45%|\u2588\u2588\u2588\u2588\u258d     | 29\/65 [00:21<00:38,  1.07s\/cell]\r\nExecuting:  48%|\u2588\u2588\u2588\u2588\u258a     | 31\/65 [00:22<00:33,  1.01cell\/s]\r\nExecuting:  51%|\u2588\u2588\u2588\u2588\u2588     | 33\/65 [00:22<00:22,  1.39cell\/s]\r\nExecuting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 34\/65 [00:23<00:19,  1.61cell\/s]\r\nExecuting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 35\/65 [00:23<00:14,  2.11cell\/s]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 37\/65 [00:23<00:10,  2.76cell\/s]\r\nExecuting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 38\/65 [00:23<00:07,  3.41cell\/s]\r\nExecuting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 40\/65 [00:24<00:05,  4.18cell\/s]\r\nExecuting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 42\/65 [00:24<00:04,  5.02cell\/s]\r\nExecuting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 43\/65 [00:32<00:59,  2.70s\/cell]\r\nExecuting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 44\/65 [11:52<1:12:00, 205.75s\/cell]\r\nExecuting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 45\/65 [11:52<48:01, 144.08s\/cell]  \r\nExecuting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 46\/65 [11:53<32:00, 101.08s\/cell]\r\nExecuting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 47\/65 [11:53<21:14, 70.80s\/cell] \r\nExecuting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 48\/65 [11:53<14:03, 49.63s\/cell]\r\nExecuting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 49\/65 [11:53<09:16, 34.79s\/cell]\r\nExecuting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 50\/65 [11:53<06:05, 24.40s\/cell]\r\nExecuting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 51\/65 [11:56<04:07, 17.70s\/cell]\r\nExecuting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 52\/65 [11:56<02:41, 12.44s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:32<25:30, 127.58s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:33<04:12, 21.01s\/cell] \r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:58:40.389449', 'duration': 1402.445046, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [12]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-12-ea5338712650> in <module>\r\nE                 8         deployment_target = aks_target\r\nE                 9     )\r\nE           ---> 10     aks_service.wait_for_deployment(show_output = True)\r\nE                11     print(f\"The web service is {aks_service.state}\")\r\nE                12 else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n```\r\n\r\nFYI @PatrickBue @jiata any idea of what could be happening?\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] error in o16n with  notebooks; Content: ### description\r\n<!--- describe your issue\/bug\/request in detail -->\r\n\r\nthis is the error, it looks it is related to the deployment of aci and aks resources. \r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error in o16n with notebooks related to the deployment of aci and aks resources.",
        "Issue_preprocessed_content":"Title: error in o n with notebooks; Content: description this is the error, it looks it is related to the deployment of aci and aks resources. fyi any idea of what could be happening? in which platform does it happen? how do we replicate the issue? expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Issue_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1568285443000,
        "Issue_closed_time":1569234937000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] pipeline -notebook-test-linux-cpu failing",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the pipeline -notebook-test-linux-cpu failing.",
        "Issue_preprocessed_content":"Title: pipeline notebook test linux cpu failing; Content: description in which platform does it happen? how do we replicate the issue? expected behavior other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/InnerEye-DeepLearning\/issues\/389",
        "Issue_title":"Memory utilization metrics are not correctly visible in AzureML",
        "Issue_label":[
            "bug",
            "reporting and diagnostics"
        ],
        "Issue_creation_time":1612434474000,
        "Issue_closed_time":1613669349000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Run 2236 in experiment \"master\" in RadiomicsNN: \r\n- Only metrics for 3 out of the 4 GPUs are visible\r\n- The MemAllocated and MemReserved metrics are all zero and hence meaningless.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: memory utilization metrics are not correctly visible in ; Content: run 2236 in experiment \"master\" in radiomicsnn: \r\n- only metrics for 3 out of the 4 gpus are visible\r\n- the memallocated and memreserved metrics are all zero and hence meaningless.",
        "Issue_original_content_gpt_summary":"The user encountered challenges with memory utilization metrics not correctly visible in run 2236 in experiment \"master\" in radiomicsnn, with only metrics for 3 out of the 4 gpus visible and the memallocated and memreserved metrics all zero and hence meaningless.",
        "Issue_preprocessed_content":"Title: memory utilization metrics are not correctly visible in; Content: run in experiment master in radiomicsnn only metrics for out of the gpus are visible the memallocated and memreserved metrics are all zero and hence meaningless."
    },
    {
        "Issue_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/175",
        "Issue_title":"azure credentials module should lazy-import any azureml.core modules",
        "Issue_label":[
            "bug",
            "azure-provider"
        ],
        "Issue_creation_time":1589931356000,
        "Issue_closed_time":1589931676000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"A regression was introduced in https:\/\/github.com\/augerai\/a2ml\/commit\/c4f89d282fd951defe3e1d51d35386be2c55c7d9#diff-1cd4abe6fbca8804140fbb9b340e3cc8, where this import statement causes azureml.core.authentication to be loaded when it's not needed if you only have the default set of a2ml dependencies installed.\r\n\r\n```\r\n~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages\/a2ml\/api\/azure\/credentials.py\", line 4, in <module>\r\n    from azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\nModuleNotFoundError: No module named 'azureml'\r\n```\r\n\r\nThe following import statement could be added around L34, right before `InteractiveLoginAuthentication` is called:\r\n\r\n```python\r\nfrom azureml.core.authentication import InteractiveLoginAuthentication\r\n```\r\n\r\nThen this could be removed from the top:\r\n\r\n```python\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: azure credentials module should lazy-import any .core modules; Content: a regression was introduced in https:\/\/github.com\/augerai\/a2ml\/commit\/c4f89d282fd951defe3e1d51d35386be2c55c7d9#diff-1cd4abe6fbca8804140fbb9b340e3cc8, where this import statement causes .core.authentication to be loaded when it's not needed if you only have the default set of a2ml dependencies installed.\r\n\r\n```\r\n~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages\/a2ml\/api\/azure\/credentials.py\", line 4, in <module>\r\n    from .core.authentication import serviceprincipalauthentication, interactiveloginauthentication\r\nmodulenotfounderror: no module named ''\r\n```\r\n\r\nthe following import statement could be added around l34, right before `interactiveloginauthentication` is called:\r\n\r\n```python\r\nfrom .core.authentication import interactiveloginauthentication\r\n```\r\n\r\nthen this could be removed from the top:\r\n\r\n```python\r\nfrom .core.authentication import serviceprincipalauthentication, interactiveloginauthentication\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the azure credentials module should lazy-import any .core modules, and a regression was introduced in a commit which caused a ModuleNotFoundError when trying to import the interactiveloginauthentication module.",
        "Issue_preprocessed_content":"Title: azure credentials module should lazy import any modules; Content: a regression was introduced in where this import statement causes to be loaded when it's not needed if you only have the default set of a ml dependencies installed. the following import statement could be added around l , right before is called then this could be removed from the top"
    },
    {
        "Issue_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Issue_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Issue_label":[
            "bug",
            "azure-provider"
        ],
        "Issue_creation_time":1589926894000,
        "Issue_closed_time":1597072927000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: warning message about hyperdrive loading with _run_type_providers; if you run any command that uses  (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nfailure while loading _run_type_providers. failed to load entrypoint hyperdrive = .train.hyperdrive:hyperdriverun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), requirement.parse('flake8<=3.7.9,>=3.1.0; Content: python_version >= \"3.6\"')).\r\n```\r\n\r\n**expected behavior**\r\nno warning message should be printed.\r\n\r\n**steps to reproduce the issue**\r\n1. from latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. observe the warning message above.\r\n\r\n\r\n**environment details:**\r\n - os: macos 10.15\r\n - a2ml version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - python version: 3.7.7\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a strange warning message when running a command that uses _run_type_providers, which was not the expected behavior, and the issue was reproduced in a MacOS 10.15 environment with a2ml version from the master branch and Python version 3.7.7.",
        "Issue_preprocessed_content":"Title: warning message about hyperdrive loading with; Content: if you run any command that uses , it prints out this strange warning message expected behavior no warning message should be printed. steps to reproduce the issue . from latest master branch in a fresh virtualenv run . . . observe the warning message above. environment details os macos a ml version master branch rev fe a e fc efde c afbfb b d python version"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Issue_title":"Show lightgbm logs in the logs in AzureML",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1630081759000,
        "Issue_closed_time":1630110931000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: show lightgbm logs in the logs in ; Content: current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of not being able to view LightGBM logs in the logs, as the current execution lets LightGBM handle its own logs, which are likely printed in stdout, but do not show up in the logs.",
        "Issue_preprocessed_content":"Title: show lightgbm logs in the logs in; Content: current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in"
    },
    {
        "Issue_link":"https:\/\/github.com\/rapidsai\/cloud-ml-examples\/issues\/165",
        "Issue_title":"azureml-sdk downgrades pyarrow to 3.0.0 which breaks cudf",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1655998483000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Steps to reproduce\r\n\r\n1. Create a fresh RAPIDS conda environment <br\/> `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5`\r\n2. `conda activate rapids-22.06`\r\n3. `conda list | grep pyarrow` shows 7.0.0 installed\r\n4. Launch python\/ipython and `import cudf` should work\r\n5. `pip install azureml-sdk`\r\n6. Launch python\/ipython and `import cudf` fails\r\n7. `conda list | grep pyarrow` shows 3.0.0 installed\r\n\r\n#### Error:\r\n```\r\n$ python -m cudf\r\nTraceback (most recent call last):\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in <module>\r\n    from cudf import api, core, datasets, testing\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in <module>\r\n    from cudf._lib.transform import bools_to_mask\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in <module>\r\n    from . import (\r\n  File \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro\r\n  File \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column\r\n  File \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar\r\n  File \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop\r\nAttributeError: module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: -sdk downgrades pyarrow to 3.0.0 which breaks cudf; Content: ### steps to reproduce\r\n\r\n1. create a fresh rapids conda environment <br\/> `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5`\r\n2. `conda activate rapids-22.06`\r\n3. `conda list | grep pyarrow` shows 7.0.0 installed\r\n4. launch python\/ipython and `import cudf` should work\r\n5. `pip install -sdk`\r\n6. launch python\/ipython and `import cudf` fails\r\n7. `conda list | grep pyarrow` shows 3.0.0 installed\r\n\r\n#### error:\r\n```\r\n$ python -m cudf\r\ntraceback (most recent call last):\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _error)\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in <module>\r\n    from cudf import api, core, datasets, testing\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in <module>\r\n    from cudf._lib.transform import bools_to_mask\r\n  file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in <module>\r\n    from . import (\r\n  file \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro\r\n  file \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column\r\n  file \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar\r\n  file \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop\r\nattributeerror: module 'pyarrow.lib' has no attribute 'monthdaynanointervalarray'\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running the 'pip install -sdk' command downgraded pyarrow to 3.0.0, which caused cudf to fail.",
        "Issue_preprocessed_content":"Title: sdk downgrades pyarrow to which breaks cudf; Content: steps to reproduce . create a fresh rapids conda environment . . shows installed . launch and should work . . launch and fails . shows installed error"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Issue_title":"Training extremely slow with Azure Machine Learning",
        "Issue_label":[
            "bug",
            "user raised",
            "support",
            "Training Service"
        ],
        "Issue_creation_time":1617867548000,
        "Issue_closed_time":1662517763000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: training extremely slow with ; Content: **what issue meet, what's expected?**:\r\nthe mnist_pytorch example training with  is unreasonably slow, each trial take about 3 to 5 mins. the entire experiment took nearly 50 mins. i was expecting it to be much faster given that it's using standard_nc6 with gpu - 1 x nvidia tesla k80.\r\n\r\n**additional information**:\r\ntried adding gpunum: 1 and useactivegpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the mnist_pytorch example training with Azure Machine Learning, which was unreasonably slow, taking 3-5 minutes per trial and nearly 50 minutes for the entire experiment, despite using a standard_nc6 with a single NVIDIA Tesla K80 GPU.",
        "Issue_preprocessed_content":"Title: training extremely slow with; Content: environment nni version nni mode remote client os windows server os linux python version version is used? conda is running in docker? no log message dispatcher started took seconds tpe using trials took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss took seconds tpe using trials with best loss nnictl stdout and stderr experiment start time experiment start time node maxlistenersexceededwarning possible eventemitter memory leak detected. message listeners added. use to increase limit node maxlistenersexceededwarning possible eventemitter memory leak detected. error listeners added. use to increase limit node maxlistenersexceededwarning possible eventemitter memory leak detected. close listeners added. use to increase limit what issue meet, what's expected? the example training with is unreasonably slow, each trial take about to mins. the entire experiment took nearly mins. i was expecting it to be much faster given that it's using with gpu x nvidia tesla k . how to reproduce it? follow this doc additional tried adding gpunum and useactivegpu true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all trials in run, each trial take run."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/125",
        "Issue_title":"Child models need to copy the dict.*.txt files from the parent model when launching an experiment on ClearML",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1644785881000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If a parent model was trained with the Alignment Enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model.  Those files are not currently being copied into the \/tmp directory on the AQUA server when the experiment is launched through ClearML, so preprocessing fails on the child model.\r\n\r\nSample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: child models need to copy the dict.*.txt files from the parent model when launching an experiment on ; Content: if a parent model was trained with the alignment enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model.  those files are not currently being copied into the \/tmp directory on the aqua server when the experiment is launched through , so preprocessing fails on the child model.\r\n\r\nsample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the dict.*.txt files from the parent model were not being copied into the \/tmp directory on the aqua server when launching an experiment, resulting in preprocessing failure on the child model.",
        "Issue_preprocessed_content":"Title: child models need to copy the files from the parent model when launching an experiment on; Content: if a parent model was trained with the alignment enhanced architecture and the dictionary on, preprocessing for the child model will look for the files from the parent model. those files are not currently being copied into the directory on the aqua server when the experiment is launched through , so preprocessing fails on the child model. sample with this failure."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Issue_title":"Execute translate script without creating ClearML task",
        "Issue_label":[
            "bug",
            "pipeline 6: infer"
        ],
        "Issue_creation_time":1641546207000,
        "Issue_closed_time":1657980432000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: execute translate script without creating  task; Content: currently, the `silnlp.nmt.translate` script always creates a  task. this should be optional. by default, it should just execute locally.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of having to create a task when executing the `silnlp.nmt.translate` script, which should be optional and execute locally by default.",
        "Issue_preprocessed_content":"Title: execute translate script without creating task; Content: currently, the script always creates a task. this should be optional. by default, it should just execute locally."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Issue_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Issue_label":[
            "bug",
            "pipeline 6: infer"
        ],
        "Issue_creation_time":1637586010000,
        "Issue_closed_time":1637601038000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: translate is trying to use  even though it was not requested. preventing translation on local machine.; Content: i tried to translate with the following command line and trace.\r\nthe command is meant to run locally, but there is an error about  credentials. the  argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project gela3_2021_11_22 --book ot --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: i tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - info - using workspace: \/home\/david\/gutenberg_new as per environment variable sil_nlp_data_path.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - info - git commit: 12aca87cab\r\ntraceback (most recent call last):\r\n  file \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, none,\r\n  file \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self. = sil(\r\n  file \"<string>\", line 8, in __init__\r\n  file \"\/home\/david\/silnlp\/silnlp\/common\/.py\", line 27, in __post_init__\r\n    self.task = task.init(\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 164, in __init__\r\n    super(task, self).__init__(**kwargs)\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(task, self).__init__(id=task_id, session=session, log=log)\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 131, in __init__\r\n    super(idobjectbase, self).__init__(session, log, **kwargs)\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    interfacebase._default_session = session(\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise loginerror(\r\n.backend_api.session.session.loginerror: failed getting token (error 401 from https:\/\/api.pro.clear.ml): unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to translate locally, as an error about credentials was raised due to the lack of the argument being set in the command line.",
        "Issue_preprocessed_content":"Title: translate is trying to use even though it was not requested. preventing translation on local machine.; Content: i tried to translate with the following command line and trace. the command is meant to run locally, but there is an error about credentials. the argument was not set in the command line."
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Issue_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1655132901000,
        "Issue_closed_time":1662130932000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: \"-ml not installed\" error in trainer (despite -ml being installed); Content: ### system info\n\n```shell\n- `transformers` version: 4.19.4\r\n- platform: linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- python version: 3.9.6\r\n- huggingface_hub version: 0.4.0\r\n- pytorch version (gpu?): 1.11.0+cu102 (false)\r\n- tensorflow version (gpu?): 2.4.1 (false)\r\n- flax version (cpu?\/gpu?\/tpu?): 0.4.0 (cpu)\r\n- jax version: 0.3.4\r\n- jaxlib version: 0.3.2\r\n- using gpu in script?: no\r\n- using distributed or parallel set-up in script?: no\n```\n\n\n### who can help?\n\n@sg\n\n### information\n\n- [ ] the official example scripts\n- [x] my own modified scripts\n\n### tasks\n\n- [x] an officially supported task in the `examples` folder (such as glue\/squad, ...)\n- [ ] my own task or dataset (give details below)\n\n### reproduction\n\n1. install -ml (in my case -ml==3.31.3)\r\n2. create trainingarguments with `report-to='_ml'\r\n3. try to instantiate trainer\r\n\r\n\r\nthis can be reproduced by adding `report_to='_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/nielsrogge\/transformers-tutorials\/blob\/master\/bert\/fine_tuning_bert_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nfollowing error happens when creating the trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nruntimeerror                              traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = default_callbacks + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is none else default_callbacks + callbacks\r\n--> 446         self.callback_handler = callbackhandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_:\r\n--> 669             raise runtimeerror(\"callback requires -ml to be installed. run `pip install -ml`.\")\r\n    670         self._initialized = false\r\n    671         self._log_assets = false\r\n\r\nruntimeerror: callback requires -ml to be installed. run `pip install -ml`.\r\n```\n\n### expected behavior\n\n```shell\na trainer is successfully created with ml callback enabled.\n```\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to instantiate a trainer with the ml callback enabled, despite -ml being installed, resulting in a \"runtimeerror: callback requires -ml to be installed. run `pip install -ml`.\" error.",
        "Issue_preprocessed_content":"Title: ml not installed error in trainer; Content: system who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . install ml . create trainingarguments with to training arguments in this notebook following error happens when creating the trainer expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Issue_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Issue_label":[
            "bug",
            "duplicate",
            "logger",
            "logger: comet",
            "needs triage"
        ],
        "Issue_creation_time":1649790124000,
        "Issue_closed_time":1650063297000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to create  logger when using pytorch lightning cli.; Content: ## \ud83d\udc1b bug\r\n\r\nunable to create  logger when using pytorch lightning cli.\r\n\r\n### to reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cveyyhcekjunkpcgy39ofrinwnivydjv?usp=sharing\r\n\r\n### expected behavior\r\nrun model.\r\n\r\n### environment\r\n\r\n* cuda:\r\n\t- gpu:\r\n\t\t- tesla t4\r\n\t- available:         true\r\n\t- version:           11.1\r\n* packages:\r\n\t- numpy:             1.21.5\r\n\t- pytorch_debug:     false\r\n\t- pytorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* system:\r\n\t- os:                linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 smp tue dec 7 09:58:10 pst 2021\r\n\r\n### additional context\r\n\r\nerror message:\r\n```\r\nepoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]traceback (most recent call last):\r\n  file \"main.py\", line 48, in <module>\r\n    cli = lightningcli(boringmodel, litdataset, save_config_callback=none)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or dummyexperiment()\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\ntypeerror: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nfor some reason, `self._kwargs` there has `{'agg_key_funcs': none, 'agg_default_func': none}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @raalsky @blaizzy",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to create a logger when using pytorch lightning cli, resulting in a TypeError due to an unexpected keyword argument.",
        "Issue_preprocessed_content":"Title: unable to create logger when using pytorch lightning cli.; Content: bug unable to create logger when using pytorch lightning cli. to reproduce expected behavior run model. environment cuda gpu tesla t available true version packages numpy false pytorch lightning tqdm system os linux architecture bit processor python version smp tue dec pst additional context error message for some reason, there has . cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/11043",
        "Issue_title":"RichProgressBar doesn't display progress bar when using Comet logger.",
        "Issue_label":[
            "bug",
            "help wanted",
            "progress bar: rich",
            "logger: comet"
        ],
        "Issue_creation_time":1639406686000,
        "Issue_closed_time":1666742778000,
        "Issue_upvote_count":6,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: richprogressbar doesn't display progress bar when using  logger.; Content: ## \ud83d\udc1b bug\r\n\r\nrichprogressbar doesn't display progress bar when using  logger.\r\ni verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### to reproduce\r\n```python\r\nimport _ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import lightningmodule, trainer\r\nfrom torch.utils.data import dataloader, dataset\r\nfrom pytorch_lightning.loggers import logger\r\nfrom pytorch_lightning.callbacks import richprogressbar\r\n\r\n\r\nclass randomdataset(dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass boringmodel(lightningmodule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # an arbitrary loss to have a loss that updates the model weights during `trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> none:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> none:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> none:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.sgd(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.steplr(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return dataloader(randomdataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return dataloader(randomdataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return dataloader(randomdataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return dataloader(randomdataset(32, 64))\r\n\r\n\r\nmodel = boringmodel()\r\n\r\nlogger = logger(api_key=os.environ.get(\"_api_token\"))\r\n\r\ntrainer = trainer(logger=logger, max_epochs=100, callbacks=[richprogressbar()])\r\n# trainer = trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### environment\r\n- pytorch lightning version 1.5.5\r\n- pytorch version 1.10.0\r\n- python version 3.8\r\n- os ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @seannaren",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the richprogressbar does not display a progress bar when using a logger in pytorch lightning.",
        "Issue_preprocessed_content":"Title: richprogressbar doesn't display progress bar when using logger.; Content: bug richprogressbar doesn't display progress bar when using logger. i verified it works correctly with tensorboard and wandb. to reproduce environment pytorch lightning version pytorch version python version os ubuntu cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Issue_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Issue_label":[
            "bug",
            "help wanted",
            "won't fix",
            "accelerator: tpu",
            "3rd party",
            "logger: wandb"
        ],
        "Issue_creation_time":1633792312000,
        "Issue_closed_time":1642181493000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: \"dumps computation\" at the start of validation loop when using wandb\/.ml logger during multi-core tpu training; Content: ## \ud83d\udc1b bug\r\n\r\ni am training a resnet model on multi core tpus on kaggle. i get this error:\r\n```\r\ndumping computation:\r\n```\r\nthis text goes on and on for several pages.\r\n\r\nthe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nnote that this only happens when using a logger (wandb or .ml) and everything works fine when i do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> i have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### to reproduce\r\n\r\nsee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu--ml) with .ml.\r\n\r\n### expected behavior\r\n\r\ntraining should run normally with no issues and logging should work.\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the training of a ResNet model on multi-core TPUs crashed and printed an error message of \"dumping computation\" at the start of the validation loop when using a logger (Wandb or .ml).",
        "Issue_preprocessed_content":"Title: dumps computation at the start of validation loop when using logger during multi core tpu training; Content: bug i am training a resnet model on multi core tpus on kaggle. i get this error this text goes on and on for several pages. the first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output. note that this only happens when using a logger and everything works fine when i do or normal as evident in this . > i have also tried adding very small batch sizes so this probably isn't a memory issue to reproduce see this that uses wandb and with expected behavior training should run normally with no issues and logging should work. environment cuda gpu available false version none packages numpy false pytorch lightning tqdm pytorch xla system os linux architecture bit processor python additional context none cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Issue_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Issue_label":[
            "bug",
            "help wanted",
            "accelerator: tpu",
            "priority: 2"
        ],
        "Issue_creation_time":1623138830000,
        "Issue_closed_time":1636988013000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger doesn't seem to log with tpu_cores=8; Content: ## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n\r\n## please reproduce using the boringmodel\r\n\r\n\r\n<!-- please paste your boringmodel colab link here. -->\r\n\r\n### to reproduce\r\n\r\nuse following [**boringmodel**](https:\/\/colab.research.google.com\/drive\/1hvwvvtk8j2nj52qu4q4ycyzom0_alqf3?usp=sharing) and post here\r\n\r\n<!-- if you could not reproduce using the boringmodel and still think there's a bug, please post here -->\r\n\r\n### expected behavior\r\n\r\n<!-- fill in -->\r\n\r\n### environment\r\n\r\n**note**: `bugs with code` are solved faster ! `colab notebook` should be made `public` !\r\n\r\n* `ide`: please, use our python [bug_report_model.py](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `colab notebook`: please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nyou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# for security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - pytorch version (e.g., 1.0):\r\n - os (e.g., linux):\r\n - how you installed pytorch (`conda`, `pip`, source):\r\n - build command you used (if compiling from source):\r\n - python version:\r\n - cuda\/cudnn version:\r\n - gpu models and configuration:\r\n - any other relevant information:\r\n\r\n### additional context\r\n\r\n<!-- add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger does not seem to log with tpu_cores=8, and provided a BoringModel Colab link to reproduce the issue, as well as their environment details.",
        "Issue_preprocessed_content":"Title: logger doesn't seem to log with; Content: bug please reproduce using the boringmodel to reproduce use following and post here expected behavior environment note are solved faster ! should be made ! please, use our python template. please copy and paste the output from our . you can get the script and run it with pytorch version os how you installed pytorch build command you used python version version gpu models and configuration any other relevant additional context cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Issue_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Issue_label":[
            "bug",
            "help wanted",
            "won't fix",
            "logger",
            "priority: 1"
        ],
        "Issue_creation_time":1621374020000,
        "Issue_closed_time":1631600832000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: upgrading from 1.2.4 to 1.3.1 causes the pytorch  logger to produce multiple experiments.; Content: ## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n\r\nwhen running a ddp multi-gpu experiment on a slurm cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple  experiments, one for each gpu. only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"screen shot 2021-05-18 at 2 00 40 pm\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"screen shot 2021-05-18 at 1 59 26 pm\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nhere is an experiment from the 'main' gpu, the one that actually logs the metrics.\r\nhttps:\/\/www..ml\/bw4sz\/everglades\/view\/syqjplzx3sbwvfg27mojv0b8p\r\n\r\nhere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www..ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## please reproduce using the boringmodel\r\n\r\n### to reproduce\r\n\r\n<!-- if you could not reproduce using the boringmodel and still think there's a bug, please post here -->\r\n\r\ni do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a  authentication, which i cannot paste here.\r\n\r\n### expected behavior\r\n\r\na single  experiment for a single call to trainer.fit(). this was the behavior in lightning 1.2.4.\r\n\r\n### environment\r\n\r\n**note**: `bugs with code` are solved faster ! `colab notebook` should be made `public` !\r\n\r\n* `ide`: please, use our python [bug_report_model.py](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `colab notebook`: please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nyou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# for security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - pytorch version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - os (e.g., linux): linux\r\n - how you installed pytorch (`conda`, `pip`, source): pip\r\n - build command you used (if compiling from source):\r\n - python version: python 3.8.8\r\n - cuda\/cudnn version: 10\r\n - gpu models and configuration: geforce 2080ti\r\n\r\n--\r\n\r\n<br class=\"apple-interchange-newline\">\r\n - any other relevant information:\r\n slurm hpc cluster, single node.\r\n\r\n### additional context\r\nproblem appears after upgrading to 1.3.1 from 1.2.4. i believe it is related to the thought behind this so post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Issue_original_content_gpt_summary":"The user encountered a bug when upgrading from PyTorch Lightning 1.2.4 to 1.3.1, causing the pytorch logger to produce multiple experiments when running a DDP multi-GPU experiment on a Slurm cluster.",
        "Issue_preprocessed_content":"Title: upgrading from to causes the pytorch logger to produce multiple experiments.; Content: bug when running a ddp multi gpu experiment on a slurm cluster, but not creates multiple experiments, one for each gpu. only one of them logs any metrics, the others just sit. here is an experiment from the 'main' gpu, the one that actually logs the metrics. here is the same run, a gpu that just announces itself and does not log anything else please reproduce using the boringmodel to reproduce i do not know how to make a reproducible example, since you cannot do multi gpu ddp in colab and would need a authentication, which i cannot paste here. expected behavior a single experiment for a single call to this was the behavior in lightning environment note are solved faster ! should be made ! please, use our python template. please copy and paste the output from our . you can get the script and run it with pytorch version os linux how you installed pytorch pip build command you used python version python version gpu models and configuration geforce ti any other relevant slurm hpc cluster, single node. additional context problem appears after upgrading to from i believe it is related to the thought behind this so post"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Issue_title":"CometLogger can modify logged metrics in-place ",
        "Issue_label":[
            "bug",
            "help wanted",
            "good first issue",
            "logger",
            "priority: 2"
        ],
        "Issue_creation_time":1618430187000,
        "Issue_closed_time":1630398077000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger can modify logged metrics in-place ; Content: when `logger.log_metrics(metrics)` is called with a `logger`, `metrics` may be modified in-place. this can lead to confusing errors. e.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the cpu and their gradients detached, leading to an error like `runtimeerror: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nnone of the other loggers change `metrics` in-place when `log_metrics` is called. all of them except neptune say that they just accept `metrics: dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.tensor`s or other types as well.\r\n\r\nthe `csvlogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nthe `tensorboardlogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nin the `logger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\ni'm happy to submit a pr to fix this so that the `logger` doesn't modify the original `metrics` dictionary. i just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. should i keep the current tensor conversion behavior for `logger` (`val.cpu().detach()`) or switch to using `val.item()`? my preference would be the latter, though this does change the behavior (see at the end).\r\n2. should i update the other loggers to all accept `metrics: dict[str, union[float, torch.tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `dict[str, float]`?\r\n3. * i don't know the other loggers, so i'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n*  sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. however, i don't think anybody would be using this behavior on purpose. if you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get ` warning: cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. the metric itself doesn't even appear in the web interface for ml, so i assume you can only access it if you query for it directly through their api.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger can modify logged metrics in-place, leading to confusing errors.",
        "Issue_preprocessed_content":"Title: logger can modify logged metrics in place; Content: when is called with a , may be modified in place. this can lead to confusing errors. if the user does then will have all the tensors moved to the cpu and their gradients detached, leading to an error like when backprop is attempted. none of the other loggers change in place when is called. all of them except neptune say that they just accept , though some others have code to handle s or other types as well. the uses the following for handling tensors the similarly has in the , the current tensor conversion code is but then the entire dictionary is copied later in the function anyway, so it doesn't really make sense to do in place modification then copy everything. i'm happy to submit a pr to fix this so that the doesn't modify the original dictionary. i just wanted to ask for a couple of opinions before changing things . should i keep the current tensor conversion behavior for .detach or switch to using ? my preference would be the latter, though this does change the behavior . . should i update the other loggers to all accept and have them all use the same method to convert to a ? . i don't know the other loggers, so i'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third party code vs sort of has support for tensors with > element, so using the first method will make logging such tensors valid while the second method would throw an error. however, i don't think anybody would be using this behavior on purpose. if you do , you get . the metric itself doesn't even appear in the web interface for ml, so i assume you can only access it if you query for it directly through their api."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Issue_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Issue_label":[
            "bug",
            "help wanted",
            "logger"
        ],
        "Issue_creation_time":1612502289000,
        "Issue_closed_time":1615221269000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: must manually import `_ml` before `logger` to avoid import error; Content: ## \ud83d\udc1b bug\r\na few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `logger`. however,  requires for `_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. if not, you get the following error:\r\n```\r\nimporterror: you must import  before these modules: torch, tensorboard\r\n```\r\n\r\nbefore the imports reordering, 's import requirements could be met by importing `logger` before torch and tensorboard. however, since the refactoring, torch is now imported before  in `loggers\/.py` itself. this forces users to manually add an unused import for `_ml` before importing `logger` to avoid the above `importerror`.\r\n\r\n### to reproduce\r\nthis [**boringmodel**](https:\/\/colab.research.google.com\/drive\/1u7ve02v40rcebexg1515kmucxvelacnf?usp=sharing) example reproduces the `importerror`.\r\n\r\n### expected behavior\r\nusers should not have to manually import `_ml` before `logger` to avoid triggering the `importerror`. the `_ml` import inside `loggers\/.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where they must manually import `_ml` before `logger` to avoid an import error when using PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: must manually import before to avoid import error; Content: bug a few weeks ago, a changed the ordering of imports for the . however, requires for to be imported before some other dependencies, torch and tensorboard, to work properly. if not, you get the following error before the imports reordering, 's import requirements could be met by importing before torch and tensorboard. however, since the refactoring, torch is now imported before in itself. this forces users to manually add an unused import for before importing to avoid the above . to reproduce this example reproduces the . expected behavior users should not have to manually import before to avoid triggering the . the import inside should exceptionally come before the import, even if it violates usual import ordering."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4229",
        "Issue_title":"Comet logger overrides COMET_EXPERIMENT_KEY env variable",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1603104554000,
        "Issue_closed_time":1603809056000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"After https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. It starts using `COMET_EXPERIMENT_KEY`. But it doesn't respect it if it is set already.\r\nSo the bug is in the following.\r\nI already set this variable \r\nThen logger overwrites my value here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L189\r\nThen it deletes this variable at all here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L215\r\nThis way it ignores my variable and deletes it at all later\r\nMoreover in version function it also ignores my set variable\r\nI will create a pull request to fix it ",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger overrides _experiment_key env variable; Content: after https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. it starts using `_experiment_key`. but it doesn't respect it if it is set already.\r\nso the bug is in the following.\r\ni already set this variable \r\nthen logger overwrites my value here https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l189\r\nthen it deletes this variable at all here https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l215\r\nthis way it ignores my variable and deletes it at all later\r\nmoreover in version function it also ignores my set variable\r\ni will create a pull request to fix it ",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the PyTorch Lightning logger which overrides the _experiment_key environment variable and deletes it, ignoring the user's set variable.",
        "Issue_preprocessed_content":"Title: logger overrides env variable; Content: after there is a changed logger behavior. it starts using . but it doesn't respect it if it is set already. so the bug is in the following. i already set this variable then logger overwrites my value here then it deletes this variable at all here this way it ignores my variable and deletes it at all later moreover in version function it also ignores my set variable i will create a pull request to fix it"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Issue_title":"CometLogger failing without save_dir",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1599655027000,
        "Issue_closed_time":1599658056000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger failing without save_dir; Content: <!-- \r\n### common bugs:\r\n1. tensorboard not showing in jupyter-notebook see [issue 79](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/issues\/79).    \r\n2. pytorch 1.1.0 vs 1.2.0 support [see faq](https:\/\/github.com\/pytorchlightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b bug\r\n\r\nmllogger with api key and  without save dir results in error.\r\nthis happens due to this if https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nthis can be fixed by setting _save_dir to none. i will supply pr in a moment\r\n\r\n### to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n```\r\n    model = lightningmodel({})\r\n    _logger = logger(\r\n        api_key=key,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = trainer(logger=_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- if you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- a clear and concise description of what you expected to happen. -->\r\n\r\n\r\ntraceback (most recent call last):\r\ntrainer.fit(model)\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nfile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### additional context\r\n\r\n<!-- add any other context about the problem here. -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the mllogger with an API key and without a save directory resulted in an error due to the lack of a save directory.",
        "Issue_preprocessed_content":"Title: logger failing without; Content: bug mllogger with api key and without save dir results in error. this happens due to this if is not set and later train loop tries to read it and fails. this can be fixed by setting to none. i will supply pr in a moment to reproduce steps to reproduce the behavior traceback file line , in result fn file line , in fit results file line , in train results file line , in file line , in train file line , in file line , in return fn file line , in or file line , in return additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Issue_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1588434434000,
        "Issue_closed_time":1591023634000,
        "Issue_upvote_count":6,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error running on ddp (can't pickle local object 'summarytopic) with  logger; Content: i have the following problem running on ddp mode with logger.\r\nwhen i detach the logger from the trainer (i.e deleting`logger=_logger`) the code runs.\r\n```\r\nexception has occurred: attributeerror\r\ncan't pickle local object 'summarytopic.__init__.<locals>.default'\r\n  file \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    forkingpickler(file, protocol).dump(obj)\r\n  file \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  file \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  file \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  file \"\/path\/multiprocessing\/context.py\", line 284, in _popen\r\n    return popen(process_obj)\r\n  file \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._popen(self)\r\n  file \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  file \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  file \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  file \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  file \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  file \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  file \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  file \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered an error running on ddp mode with a logger, where an AttributeError occurred due to an inability to pickle a local object.",
        "Issue_preprocessed_content":"Title: error running on ddp with logger; Content: i have the following problem running on ddp mode with logger. when i detach the logger from the trainer the code runs."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Issue_title":"Comet logger cannot be pickled after creating an experiment",
        "Issue_label":[
            "bug",
            "help wanted",
            "logger"
        ],
        "Issue_creation_time":1588303817000,
        "Issue_closed_time":1591023635000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger cannot be pickled after creating an experiment; ## \ud83d\udc1b bug \r\n\r\nthe  logger cannot be pickled after an experiment (at least an offlineexperiment) has been created.\r\n\r\n### to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import logger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = logger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a trainer object with the logger (works fine)\r\n```\r\ntrainer = trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the offlineexperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> typeerror: can't pickle _thread.lock objects\r\n```\r\n\r\n### expected behavior\r\n\r\nwe should be able to pickle loggers for distributed training.\r\n\r\n### environment\r\n\r\n* cuda:\r\n        - gpu:\r\n        - available:         false\r\n        - version:           none\r\n* packages:\r\n        - numpy:             1.18.1\r\n        - pytorch_debug:     false\r\n        - pytorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* system:\r\n        - os:                darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           darwin kernel version 19.3.0: thu jan  9 20:58:23 pst 2020; Content: root:xnu-6153.81.5~1\/release_x86_64\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger object cannot be pickled after an experiment has been created, resulting in a TypeError.",
        "Issue_preprocessed_content":"Title: logger cannot be pickled after creating an experiment; Content: bug the logger cannot be pickled after an experiment has been created. to reproduce steps to reproduce the behavior initialize the logger object initialize a trainer object with the logger access the attribute which creates the offlineexperiment object expected behavior we should be able to pickle loggers for distributed training. environment cuda gpu available false version none packages numpy false pytorch lightning tensorboard tqdm system os darwin architecture bit processor i python version darwin kernel version thu jan pst ;"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Issue_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1586647457000,
        "Issue_closed_time":1586910754000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: test metrics are no longer pushed to .ml (and perhaps others); Content: ## \ud83d\udc1b bug\r\n\r\npytorch lightning 0.7.2 used to publish test metrics to .ml.  commit https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n\r\nrun fast-run of training and observe test metrics not being submitted to .ml (and possibly other logging destinations).\r\n\r\n### environment\r\n\r\n```\r\ncuda:\r\n        gpu:\r\n                tesla t4\r\n        available:           true\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pytorch_debug:       false\r\n        pytorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        os:                  linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-ubuntu smp thu mar 26 02:17:29 utc 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where test metrics are no longer pushed to .ml (and perhaps others) when using pytorch lightning 0.7.2, and provided steps to reproduce the behavior and their environment.",
        "Issue_preprocessed_content":"Title: test metrics are no longer pushed to; Content: bug pytorch lightning used to publish test metrics to commit has broken this functionality. to reproduce steps to reproduce the behavior run fast run of training and observe test metrics not being submitted to . environment cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Issue_title":"Test metrics not logging to Comet after training",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1580225794000,
        "Issue_closed_time":1582760093000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: test metrics not logging to  after training; Content: ## \ud83d\udc1b bug\r\n\r\nwhen testing a model with `trainer.test` metrics are not logged to  if the model was previously trained using `trainer.fit`. while training metrics are logged correctly.\r\n\r\n\r\n#### code sample\r\n```\r\n    _logger = logger()\r\n    trainer = trainer(logger=_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # metrics are logged to \r\n    trainer.test(model) # no metrics are logged to \r\n```\r\n\r\n### expected behavior\r\n\r\ntest metrics should also be logged in to .\r\n\r\n### environment\r\n\r\n```\r\n- pytorch version: 1.3.0\r\nis debug build: no\r\ncuda used to build pytorch: 10.1.243\r\n\r\nos: ubuntu 18.04.3 lts\r\ngcc version: (ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\ncmake version: version 3.10.2\r\n\r\npython version: 3.7\r\nis cuda available: yes\r\ncuda runtime version: 10.1.168\r\ngpu models and configuration:\r\ngpu 0: geforce gtx 1080 ti\r\ngpu 1: geforce gtx 1080 ti\r\ngpu 2: geforce gtx 1080 ti\r\ngpu 3: geforce gtx 1080 ti\r\ngpu 4: geforce gtx 1080 ti\r\ngpu 5: geforce gtx 1080 ti\r\ngpu 6: geforce gtx 1080 ti\r\ngpu 7: geforce gtx 1080 ti\r\n\r\nnvidia driver version: 418.67\r\ncudnn version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nversions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] could not collect\r\n```\r\n\r\n### additional context\r\n\r\ni believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#l366), `logger.finalize(\"success\")` is called. this in turn calls `experiment.end()` inside the logger and the `experiment` object doesn't expect to send more information after this.\r\n\r\nan alternative is to create another `trainer` object, with another logger but this means that the metrics will be logged into a different  experiment from the original. this issue can be solved using the `existingexperiment` object form the  sdk, but the solution seems a little hacky and the `logger` currently doesn't support this kind of experiment.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where metrics were not logging to  after training a model with `trainer.test` if the model was previously trained using `trainer.fit`.",
        "Issue_preprocessed_content":"Title: test metrics not logging to after training; Content: bug when testing a model with metrics are not logged to if the model was previously trained using . while training metrics are logged correctly. code sample expected behavior test metrics should also be logged in to . environment additional context i believe the issue is caused because at the , is called. this in turn calls inside the logger and the object doesn't expect to send more after this. an alternative is to create another object, with another logger but this means that the metrics will be logged into a different experiment from the original. this issue can be solved using the object form the sdk, but the solution seems a little hacky and the currently doesn't support this kind of experiment."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Issue_title":"Comet PAPI Depreciated",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1575967432000,
        "Issue_closed_time":1576023863000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  papi depreciated; use of the  api logger reports an unecessary depreciation warning relating to the use of _ml.papi, rather than the newer _ml.api.\r\n\r\nexample:\r\n` warning: you have imported _ml.papi; Content: this interface is deprecated. please use _ml.api instead. for more information, see: https:\/\/www..ml\/docs\/python-sdk\/releases\/#release-300`",
        "Issue_original_content_gpt_summary":"The user encountered a depreciation warning when using the API logger, which indicated that the older _ml.papi interface should be replaced with the newer _ml.api interface.",
        "Issue_preprocessed_content":"Title: papi depreciated; Content: use of the api logger reports an unecessary depreciation warning relating to the use of rather than the newer example"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Issue_title":"CometLogger does not implement name() and version() class methods",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1573092782000,
        "Issue_closed_time":1573531232000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger does not implement name() and version() class methods; Content: explicitly creating a logger instance and passing it to trainer using trainer(logger=my__logger) raises a notimplementederror because logger does not implement the name() and version() class methods.\r\n\r\nbelow is the traceback:\r\n`\r\ntraceback (most recent call last):\r\n  file \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not none and self.logger.version is not none:\r\n  file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise notimplementederror(\"sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where explicitly creating a logger instance and passing it to trainer using trainer(logger=my__logger) raised a NotImplementedError because the logger did not implement the name() and version() class methods.",
        "Issue_preprocessed_content":"Title: logger does not implement name class methods; Content: explicitly creating a logger instance and passing it to trainer using raises a notimplementederror because logger does not implement the name class methods. below is the traceback"
    },
    {
        "Issue_link":"https:\/\/github.com\/ultralytics\/yolov5\/issues\/10301",
        "Issue_title":"Comet Bug: Unable to train on Window 11",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669476859000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":14.0,
        "Issue_body":"### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions.\n\n\n### Question\n\nI am unable to train alway the same error:\r\n\r\npython train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt\r\ntrain: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5\r\nYOLOv5  2022-11-26 Python-3.9.13 torch-1.13.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Using 'C:\\\\Users\\\\telem\\\\Desktop\\\\Yolo\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nCOMET WARNING: Native output logging mode is not available, falling back to basic output logging\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 633, in <module>\r\n    main(opt)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 95, in train\r\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\__init__.py\", line 132, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(COMET_PREFIX):\r\nKeyError: 'path'\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO: ----------------------------------\r\nCOMET WARNING: Experiment Name is generated at upload time for Offline Experiments unless set explicitly with Experiment.set_name\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Starting saving the offline archive\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload C:\\Users\\telem\\Desktop\\Yolo\\.cometml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip\r\n\r\nI have tested many dataset and alway the same error any advice ?\r\n\n\n### Additional\n\n_No response_",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  bug: unable to train on window 11; Content: ### search before asking\n\n- [x] i have searched the yolov5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions.\n\n\n### question\n\ni am unable to train alway the same error:\r\n\r\npython train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt\r\ntrain: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=false, resume=false, nosave=false, noval=false, noautoanchor=false, noplots=false, evolve=none, bucket=, cache=none, image_weights=false, device=, multi_scale=false, single_cls=false, optimizer=sgd, sync_bn=false, workers=8, project=runs\\train, name=exp, exist_ok=false, quad=false, cos_lr=false, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=none, upload_dataset=false, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5\r\nyolov5  2022-11-26 python-3.9.13 torch-1.13.0+cpu cpu\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nclearml: run 'pip install clearml' to automatically track, visualize and remotely train yolov5  in clearml\r\ntensorboard: start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/\r\n warning:  credentials have not been set.  will default to offline logging. please set your credentials to enable online logging.\r\n warning:  has disabled auto-logging functionality as it has been imported after the following ml modules: tensorboard, torch. metrics and hyperparameters can still be logged using _ml.log_metrics() and _ml.log_parameters()\r\n info: using 'c:\\\\users\\\\telem\\\\desktop\\\\yolo\\\\.ml-runs' path as offline directory. pass 'offline_directory' parameter into constructor or set the '_offline_directory' environment variable to manually choose where to store offline experiment archives.\r\n warning: native output logging mode is not available, falling back to basic output logging\r\ntraceback (most recent call last):\r\n  file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 633, in <module>\r\n    main(opt)\r\n  file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 95, in train\r\n    loggers = loggers(save_dir, weights, opt, hyp, logger)  # loggers instance\r\n  file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\__init__.py\", line 132, in __init__\r\n    self._logger = logger(self.opt, self.hyp)\r\n  file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\\\__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\\\__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(_prefix):\r\nkeyerror: 'path'\r\n info: ----------------------------------\r\n info: .ml offlineexperiment summary\r\n info: ----------------------------------\r\n info:   data:\r\n info:     display_summary_level : 1\r\n info:     url                   : [offlineexperiment will get url after upload]\r\n info:   others:\r\n info:     offline_experiment : true\r\n info:   uploads:\r\n info:     environment details : 1\r\n info:     installed packages  : 1\r\n info: ----------------------------------\r\n warning: experiment name is generated at upload time for offline experiments unless set explicitly with experiment.set_name\r\n warning:  has disabled auto-logging functionality as it has been imported after the following ml modules: tensorboard, torch. metrics and hyperparameters can still be logged using _ml.log_metrics() and _ml.log_parameters()\r\n info: still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\n info: starting saving the offline archive\r\n info: to upload this offline experiment, run:\r\n     upload c:\\users\\telem\\desktop\\yolo\\.ml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip\r\n\r\ni have tested many dataset and alway the same error any advice ?\r\n\n\n### additional\n\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with training on Windows 11, where they received a KeyError and were unable to find a similar issue in the Yolov5 issues and discussions.",
        "Issue_preprocessed_content":"Title: bug unable to train on window; Content: search before asking i have searched the yolov and and found no similar questions. question i am unable to train alway the same error python img batch epochs data weights train cfg , epochs , imgsz , rect false, resume false, nosave false, noval false, noautoanchor false, noplots false, evolve none, bucket , cache none, device , optimizer sgd, workers , name exp, quad false, patience , freeze , seed , entity none, github skipping check , for updates see yolov cpu hyperparameters clearml run 'pip install clearml' to automatically track, visualize and remotely train yolov in clearml tensorboard start with 'tensorboard logdir view at warning credentials have not been set. will default to offline logging. please set your credentials to enable online logging. warning has disabled auto logging functionality as it has been imported after the following ml modules tensorboard, torch. metrics and hyperparameters can still be logged using and using path as offline directory. pass parameter into constructor or set the environment variable to manually choose where to store offline experiment archives. warning native output logging mode is not available, falling back to basic output logging traceback file line , in main file line , in main opt, device, callbacks file line , in train loggers weights, opt, hyp, logger loggers instance file line , in file line , in file line , in if keyerror 'path' offlineexperiment summary data url others true uploads environment details installed packages warning experiment name is generated at upload time for offline experiments unless set explicitly with warning has disabled auto logging functionality as it has been imported after the following ml modules tensorboard, torch. metrics and hyperparameters can still be logged using and still saving offline stats to messages file before program termination starting saving the offline archive to upload this offline experiment, run upload i have tested many dataset and alway the same error any advice ? additional"
    },
    {
        "Issue_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/733",
        "Issue_title":"The learning rate plot in Comet is not the expected one",
        "Issue_label":[
            "looking into it"
        ],
        "Issue_creation_time":1591889615000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Hi! I've been trying the comet.ml integration and I must say this has been a great addition to the framework. \ud83d\ude4c\r\n\r\nI wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that I expected, especially when trying the learning_rate_warmup_epochs option, which I set to 6 as suggested. The learning rate that is plot on comet is the one set in learning_rate, and it's constant for the first epochs.\r\n\r\nCould this be related to this error?\r\n\r\n`COMET ERROR: Failed to extract parameters from Optimizer.init()\r\n`\r\n\r\n**To Reproduce**\r\n1. Setup comet\r\n2. Set  learning_rate_warmup_epochs option to 6\r\n\r\n**Expected behavior**\r\nI expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as I set also reduce_learning_rate_on_plateau .\r\n\r\n**Actual behavior**\r\nThe lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: the learning rate plot in  is not the expected one; Content: hi! i've been trying the .ml integration and i must say this has been a great addition to the framework. \ud83d\ude4c\r\n\r\ni wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that i expected, especially when trying the learning_rate_warmup_epochs option, which i set to 6 as suggested. the learning rate that is plot on  is the one set in learning_rate, and it's constant for the first epochs.\r\n\r\ncould this be related to this error?\r\n\r\n` error: failed to extract parameters from optimizer.init()\r\n`\r\n\r\n**to reproduce**\r\n1. setup \r\n2. set  learning_rate_warmup_epochs option to 6\r\n\r\n**expected behavior**\r\ni expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as i set also reduce_learning_rate_on_plateau .\r\n\r\n**actual behavior**\r\nthe lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the learning rate plot in TensorBoard was not the expected one, despite setting the learning_rate_warmup_epochs option to 6.",
        "Issue_preprocessed_content":"Title: the learning rate plot in is not the expected one; Content: hi! i've been trying the integration and i must say this has been a great addition to the framework. i wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that i expected, especially when trying the option, which i set to as suggested. the learning rate that is plot on is the one set in and it's constant for the first epochs. could this be related to this error? to reproduce . setup . set option to expected behavior i expected to see the lr increase in the first epochs, reach the lr set in and eventually decrease, as i set also . actual behavior the lr is equal to the set in the first epochs, and eventually decreases due to ."
    },
    {
        "Issue_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Issue_title":"Logging issue when activating Comet contrib",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1557825272000,
        "Issue_closed_time":1559077203000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logging issue when activating  contrib; Content: **describe the bug**\r\n\r\nwhen activating the  contrib, most of ludwig log message disappears.\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n\r\nlaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --`\r\n\r\nyou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ v  v \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - experiment\r\n\r\nexperiment name: experiment\r\nmodel name: run\r\noutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**expected behavior**\r\n\r\nthe log messages should be displayed when the  contrib is activated.\r\n\r\n**environment (please complete the following information):**\r\n - os: fedora\r\n - version 28\r\n- python version: 3.6.8\r\n- ludwig version: 0.1.2\r\n\r\n**additional context**\r\n\r\ni think the issue is that ludwig is using the root-level logger configured through `logging.basicconfig`. the  contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/.py#l56.\r\n\r\nthose calls happen before any `basicconfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#l461.\r\n\r\nthe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicconfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/lib\/logging\/__init__.py#l2065. the direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a streamhandler pointing to `\/dev\/stderr`.\r\n\r\nthe unfortunate side-effect is that calling `basicconfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\ni would recommend moving from using the root logger and configure the logger through `basicconfig` to using a `ludwig` logger and configure it manually, it's not that more complex. i can help if wanted.\r\n\r\none last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. that includes requests and is polluting the output. using a separate logger would also solve this issue.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a logging issue when activating the contrib, where most of Ludwig's log messages disappeared due to a misconfiguration of the root logger.",
        "Issue_preprocessed_content":"Title: logging issue when activating contrib; Content: describe the bug when activating the contrib, most of ludwig log message disappears. to reproduce steps to reproduce the behavior launch you won't see the following output expected behavior the log messages should be displayed when the contrib is activated. environment os fedora version python version ludwig version additional context i think the issue is that ludwig is using the root level logger configured through . the contrib integration contains some logging calls, for example, those calls happen before any call the issue with calling the root level , and so on is that they will call on their own if the root logger is not configured yet the direct effect is that the first call to will configure the root logger with no configuration which will create a streamhandler pointing to . the unfortunate side effect is that calling will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device. i would recommend moving from using the root logger and configure the logger through to using a logger and configure it manually, it's not that more complex. i can help if wanted. one last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. that includes requests and is polluting the output. using a separate logger would also solve this issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Issue_title":"Comet \"Reproduce\" feature doesn't work",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1595861398000,
        "Issue_closed_time":1624956881000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"It fails at the \"apply patch\" stage",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  \"reproduce\" feature doesn't work; Content: it fails at the \"apply patch\" stage",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"reproduce\" feature was not working, as it was failing at the \"apply patch\" stage.",
        "Issue_preprocessed_content":"Title: reproduce feature doesn't work; Content: it fails at the apply patch stage"
    },
    {
        "Issue_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Issue_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600305917000,
        "Issue_closed_time":1600309841000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: when using nn.dataparallel, the name of the model saved in .ml will be dataparallel.; Content: when using nn.dataparallel, the name of the model saved in .ml will be dataparallel.\r\n\r\n## expected behavior\r\n\r\n<!-- please write a clear and concise description of what you expected to happen. -->\r\n\r\n## environment\r\n\r\n- enchanter version: 0.7.0\r\n- python version: 3.6.6\r\n- os: ubuntu 18.04\r\n- (optional) other libraries and their versions:\r\n\r\n## error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## additional context (optional)\r\n\r\n<!-- please add any other context or screenshots about the problem here. -->",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using nn.dataparallel, where the name of the model saved in .ml was not as expected.",
        "Issue_preprocessed_content":"Title: when using the name of the model saved in will be dataparallel.; Content: when using the name of the model saved in will be dataparallel. expected behavior environment enchanter version python version os ubuntu other libraries and their versions error messages, stack traces, or logs steps to reproduce . . . reproducible examples additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Issue_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600151670000,
        "Issue_closed_time":1600153381000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  warning: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...); enchanter v0.7.0 raise ` warning: log_asset_data(..., file_name=...) is deprecated; Content: use log_asset_data(..., name=...)` when using context api\r\n\r\n## expected behavior\r\n\r\n<!-- please write a clear and concise description of what you expected to happen. -->\r\n\r\n## environment\r\n\r\n- enchanter version: v0.7.0\r\n- python version: ?\r\n- os: linux\r\n- (optional) other libraries and their versions: google colab with gpu\r\n\r\n## error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## reproducible examples (optional)\r\n\r\n```python\r\nrunner = classificationrunner(\r\n    net, optimizer, criterion, experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.gradscaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## additional context (optional)\r\n\r\n<!-- please add any other context or screenshots about the problem here. -->",
        "Issue_original_content_gpt_summary":"The user encountered a warning when using the context API in enchanter v0.7.0, which stated that log_asset_data(..., file_name=...) is deprecated and should be replaced with log_asset_data(..., name=...).",
        "Issue_preprocessed_content":"Title: warning is deprecated; use; Content: enchanter raise when using context api expected behavior environment enchanter version python version ? os linux other libraries and their versions google colab with gpu error messages, stack traces, or logs steps to reproduce . . . reproducible examples additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Issue_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Issue_label":[
            "\ud83d\udc1b bug fix",
            "\ud83e\udd89 dvc"
        ],
        "Issue_creation_time":1625148874000,
        "Issue_closed_time":1626683431000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fix the definition of pipelines\/sentence_embedding\/.yaml; Content: ## \ud83d\udc1b bug description\r\n\r\nthe pipeline `sentence_embedding\/.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nthis creates the following issues:\r\n  - the evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - the training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## to reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport pipeline=data_and_models\/pipelines\/sentence_embedding\/.yaml\r\n pull -d $pipeline\r\n repro -f $pipeline\r\n```\r\n\r\nthis will give the error:\r\n```\r\nrunning stage 'data_and_models\/pipelines\/sentence_embedding\/.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nattributeerror: path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nafter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nrunning stage 'data_and_models\/pipelines\/sentence_embedding\/.yaml:evaluation@tf_idf':\r\n...\r\nfilenotfounderror: [errno 2] no such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## expected behavior\r\n\r\n` pull -d` and ` repro -f` should run without errors about missing files.",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the definition of the pipeline `sentence_embedding\/.yaml` which caused errors when running `pull -d` and `repro -f` commands.",
        "Issue_preprocessed_content":"Title: fix the definition of; Content: bug description the pipeline is not correctly defined for . this creates the following issues the evaluation stage does not know how to pull the model . the training stage does not know it has to run before the evaluation stage for the models and . to reproduce this will give the error after manually pulling , this will give the error expected behavior and should run without errors about missing files."
    },
    {
        "Issue_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Issue_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Issue_label":[
            "\ud83d\udc1b bug fix"
        ],
        "Issue_creation_time":1620385977000,
        "Issue_closed_time":1620393602000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  eval crashes \"int64 not json serializable\"; Content: the  evaluation is crashing. after investigation, the bug was introduced by #348.\r\n\r\nthe bug:\r\n```\r\ntraceback (most recent call last):\r\n  file \"eval.py\", line 111, in <module>\r\n    main()\r\n  file \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  file \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise typeerror(f'object of type {o.__class__.__name__} '\r\ntypeerror: object of type int64 is not json serializable\r\n```\r\n\r\nto reproduce:\r\n\r\n```\r\n# for the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# for the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport commit=...\r\n\r\ngit clone https:\/\/github.com\/bluebrain\/search\r\ncd search\/\r\n\r\n# change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/dockerfile --build-arg bbs_revision=$commit -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $commit\r\ngit checkout -- data_and_models\/pipelines\/ner\/.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\n pull --with-deps evaluation@organism\r\n repro -fs evaluation@organism\r\n```\r\n\r\n_originally posted by @pafonta in https:\/\/github.com\/bluebrain\/search\/issues\/335#issuecomment-833506692_",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the evaluation process which caused it to crash due to an int64 not being json serializable, which was introduced by a commit in the repository.",
        "Issue_preprocessed_content":"Title: eval crashes int not json serializable; Content: the evaluation is crashing. after investigation, the bug was introduced by . the bug to reproduce posted by in"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/435",
        "Issue_title":"Combine `zn.params` and `dvc.params` might not work",
        "Issue_label":[
            "bug",
            "p0-critical"
        ],
        "Issue_creation_time":1668011905000,
        "Issue_closed_time":1668012691000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: combine `zn.params` and `.params` might not work; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to combine `zn.params` and `.params` which did not work.",
        "Issue_preprocessed_content":"Title: combine and might not work; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/348",
        "Issue_title":"znNodes not working with `dvc.<...>`",
        "Issue_label":[
            "bug",
            "p1-important",
            "needs-tests"
        ],
        "Issue_creation_time":1658850596000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"- [ ] fix docstring\r\n- [ ] test with a Node that has `dvc.params` and `dvc.outs`\r\n\r\nhttps:\/\/github.com\/zincware\/ZnTrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#L11-L28",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: znnodes not working with `.<...>`; Content: - [ ] fix docstring\r\n- [ ] test with a node that has `.params` and `.outs`\r\n\r\nhttps:\/\/github.com\/zincware\/zntrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#l11-l28",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with znnodes not working with `.<...>` and had to fix the docstring and test with a node that has `.params` and `.outs`.",
        "Issue_preprocessed_content":"Title: znnodes not working with; Content: fix docstring test with a node that has and"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Issue_title":"zn.Method does not add params to `dvc.yaml`",
        "Issue_label":[
            "bug",
            "p0-critical"
        ],
        "Issue_creation_time":1643228171000,
        "Issue_closed_time":1643235530000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: zn.method does not add params to `.yaml`; Content: when only `zn.method` without `zn.params` is used in a node the `.yaml` will not depend on the `params.yaml`.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using only zn.method without zn.params in a node caused the .yaml file to not depend on the params.yaml.",
        "Issue_preprocessed_content":"Title: does not add params to; Content: when only without is used in a node the will not depend on the ."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Issue_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Issue_label":[
            ":bug: bug"
        ],
        "Issue_creation_time":1619681675000,
        "Issue_closed_time":1621495987000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error: '.lock' is git-ignored.; Content: ```\r\n$  repro run_benchmarks\r\nerror: '.lock' is git-ignored.\r\n```\r\n\r\n`..lock` in `.gitignore` causes exceptions at running benchmark. delete this line solves this problem. and because of #168 maybe we need some better ways to deal with `.lock`.",
        "Issue_original_content_gpt_summary":"The user encountered an error when running a benchmark due to a '.lock' file being git-ignored, and needed to delete the line from the .gitignore file to solve the problem.",
        "Issue_preprocessed_content":"Title: error is git ignored.; Content: in causes exceptions at running benchmark. delete this line solves this problem. and because of maybe we need some better ways to deal with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Issue_title":"requirements: update dvc",
        "Issue_label":[
            ":bug: bug",
            "p1-important"
        ],
        "Issue_creation_time":1616672577000,
        "Issue_closed_time":1628758546000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: requirements: update ; Content: after https:\/\/github.com\/iterative\/\/pull\/5265\r\nwe do not allow ignoring lockfile. `-bench` is running currently on some older version of ``, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of updating the requirements to ensure that the '-bench' command works with versions greater than 2.0.0, while still not allowing the ignoring of the lockfile.",
        "Issue_preprocessed_content":"Title: requirements update; Content: after we do not allow ignoring lockfile. is running currently on some older version of , though it would be good to adjust it so that it works with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/149",
        "Issue_title":"dvc tries to launch updater using asv script",
        "Issue_label":[
            ":bug: bug",
            "p0-critical"
        ],
        "Issue_creation_time":1593808834000,
        "Issue_closed_time":1594041670000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"In every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 DEBUG: Trying to spawn '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 DEBUG: Spawned '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               Unknown mode daemon\r\n```\r\nwe clearly need to take more care on dvc-side, but a good enough workaround is to set CI or DVC_TEST env var to make dvc skip launching the updater.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  tries to launch updater using asv script; Content: in every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 debug: trying to spawn '['\/home\/efiop\/git\/-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 debug: spawned '['\/home\/efiop\/git\/-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               unknown mode daemon\r\n```\r\nwe clearly need to take more care on -side, but a good enough workaround is to set ci or _test env var to make  skip launching the updater.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to launch an updater using an asv script, requiring them to take more care on the side and set a ci or _test env var to make it skip launching the updater.",
        "Issue_preprocessed_content":"Title: tries to launch updater using asv script; Content: in every run you can see we clearly need to take more care on side, but a good enough workaround is to set ci or env var to make skip launching the updater."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Issue_title":"Various issues in `example-dvc-experiments`",
        "Issue_label":[
            "bug",
            "priority-p0"
        ],
        "Issue_creation_time":1638880026000,
        "Issue_closed_time":1642605521000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: various issues in `example--experiments`; Content: > these are reported by @tapadipti (thanks). i'm moving here to discuss and follow: \r\n\r\ni was running experiments by following the docs (https:\/\/.org\/doc\/start\/experiments) and encountered the following issues. sharing here for any required action.\r\n1.  is not installed by `pip install -r requirements.txt`. so, if someone is trying to use a new virtual env, they need to install  separately. would be good to include `` in `requirements.txt`.\r\n2. ` pull` gave this error:\r\n   ```\r\n   error: failed to pull data from the cloud - checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   is your cache up to date?\r\n   <https:\/\/error..org\/missing-files>\r\n   ```\r\n\r\n3. ` exp run` lists all the image when running the `extract` stage. would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `if you used  repro before` section in the doc is a little unclear. does ` exp run` replace ` repro`? if yes, can we state this clearly? also would be great to change this statement `we use  repro to run the pipeline...` to ` repro runs the pipeline...`",
        "Issue_original_content_gpt_summary":"The user encountered various issues while running experiments by following the documentation, such as not being able to install a required package, errors when pulling data from the cloud, listing all images when running the extract stage, and confusion about the use of repro.",
        "Issue_preprocessed_content":"Title: various issues in; Content: > these are reported by . i'm moving here to discuss and follow i was running experiments by following the docs and encountered the following issues. sharing here for any required action. . is not installed by . so, if someone is trying to use a new virtual env, they need to install separately. would be good to include in . . gave this error . lists all the image when running the stage. would be good to remove from . section in the doc is a little unclear. does replace ? if yes, can we state this clearly? also would be great to change this statement to"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/17",
        "Issue_title":"example-get-started is broken with latest DVC",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1606072868000,
        "Issue_closed_time":1606074573000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"> From https:\/\/github.com\/iterative\/dvc.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$ dvc fetch\r\nERROR: failed to fetch data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n```",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: example-get-started is broken with latest ; Content: > from https:\/\/github.com\/iterative\/.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$  fetch\r\nerror: failed to fetch data from the cloud - lockfile '.lock' is corrupted.\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the example-get-started repository, where an error was thrown when attempting to fetch data from the cloud due to a corrupted lockfile.",
        "Issue_preprocessed_content":"Title: example get started is broken with latest; Content: > from"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/12",
        "Issue_title":"need to rebuild get-started with the latest DVC version",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1582914224000,
        "Issue_closed_time":1588739140000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Experience is broken since every DVC command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: need to rebuild get-started with the latest  version; Content: experience is broken since every  command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Issue_original_content_gpt_summary":"The user experienced broken functionality due to changes in the `.gitignore` file when jumping between branches, making it difficult to rebuild the get-started process with the latest version.",
        "Issue_preprocessed_content":"Title: need to rebuild get started with the latest version; Content: experience is broken since every command changes now makes it very annoying to jump between branches."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/121",
        "Issue_title":"fds fails to pull dvc on windows",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1647838452000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When running pull command on DagsHub remote. I receive dvc pull failure, so I have to manually pull dvc again. \n\nThis issue permanent issue on windows. \n\n```bash\nfds clone <remote> \n\n```\n\nIt is not urgent issue, but in annoyance category. ",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fds fails to pull  on windows; Content: when running pull command on dagshub remote. i receive  pull failure, so i have to manually pull  again. \n\nthis issue permanent issue on windows. \n\n```bash\nfds clone <remote> \n\n```\n\nit is not urgent issue, but in annoyance category. ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running the 'pull' command on Dagshub Remote resulted in a pull failure, requiring them to manually pull again, which is a permanent issue on Windows.",
        "Issue_preprocessed_content":"Title: fds fails to pull on windows; Content: when running pull command on dagshub remote. i receive pull failure, so i have to manually pull again. this issue permanent issue on windows. it is not urgent issue, but in annoyance category."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Issue_title":"DVC and Git services don't correctly detect the repo root directory",
        "Issue_label":[
            "bug",
            "good first issue",
            "breaking"
        ],
        "Issue_creation_time":1629832721000,
        "Issue_closed_time":1630227884000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  and git services don't correctly detect the repo root directory; Content: it seems they both assume that the current working dir is where they can find the `.git` and `.` dirs.\r\nwe should correctly detect those paths, as it affects all our logic to e.g. automatically  init on behalf of the user.\r\n\r\nrelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/.org\/doc\/command-reference\/root",
        "Issue_original_content_gpt_summary":"The user encountered challenges with git and git services not correctly detecting the repository root directory, which affects the logic to automatically initialize on behalf of the user.",
        "Issue_preprocessed_content":"Title: and git services don't correctly detect the repo root directory; Content: it seems they both assume that the current working dir is where they can find the and dirs. we should correctly detect those paths, as it affects all our logic to automatically init on behalf of the user. relevant resources . ."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Issue_title":"fsd clone for non-DVC repos throws an error",
        "Issue_label":[
            "bug",
            "enhancement"
        ],
        "Issue_creation_time":1628503684000,
        "Issue_closed_time":1630576282000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fsd clone for non- repos throws an error; Content: when using `fds clone` for non- repo it throws the following error:\r\n\r\n`error: you are not inside of a  repository (checked up to mount point '\/')`\r\n\r\ncloning a non- repo using fds can be a common use case, e.g., cloning a dagshub repo containing many files, but none of them are tracked by  nur the repo contains  config files. \r\n\r\ni suggest that after cloning the git server, fds will check if the repo contains  files. \r\n\r\nif it contains  files:\r\n  - echo 'starting  clone...`\r\n  - fds will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - fds will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nit doesn't contain  files:\r\n  - fds will initialize \r\n  \r\n    if the git server url is dagshub's:\r\n      - fds will set dagshub storage as the remote using the git url (replacing`.git` with `.`).\r\n      - fds will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - fds will start a wizard asking do you want to set a  remote\r\n       if yes:\r\n           - with the wizard, the user will set the remote url, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using `fds clone` for non- repo, as it throws an error, and suggests a solution to check if the repo contains files and set the user name and password for each remote storage in the local config.",
        "Issue_preprocessed_content":"Title: fsd clone for non repos throws an error; Content: when using for non repo it throws the following error cloning a non repo using fds can be a common use case, cloning a dagshub repo containing many files, but none of them are tracked by nur the repo contains config files. i suggest that after cloning the git server, fds will check if the repo contains files. if it contains files echo 'starting fds will start a wizard to set the user name and password for each remote storage in the local config. fds will pull all the files from the remotes and show a progress bar it doesn't contain files fds will initialize if the git server url is dagshub's fds will set dagshub storage as the remote using the git url . fds will start a wizard to set the remote user name, password, and name. else fds will start a wizard asking do you want to set a remote if yes with the wizard, the user will set the remote url, name, username, and password."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/39",
        "Issue_title":"Fails to add files to DVC tracking",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1622120972000,
        "Issue_closed_time":1622139051000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When running the `fds add` command for data files it tries to add them to DVC tracking but fails.\r\n\r\nIn my case I tried to add the raw-data directory that contains the following image files:\r\n```\r\n$ tree data\/raw-data\r\ndata\/raw-data\r\n\u251c\u2500\u2500 IM-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0003-0001.jpeg\r\n\u251c\u2500\u2500 IM-0005-0001.jpeg\r\n\u251c\u2500\u2500 IM-0006-0001.jpeg\r\n\u251c\u2500\u2500 IM-0007-0001.jpeg\r\n\u251c\u2500\u2500 IM-0009-0001.jpeg\r\n\u251c\u2500\u2500 IM-0010-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0002.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001.jpeg\r\n\u251c\u2500\u2500 IM-0013-0001.jpeg\r\n\u251c\u2500\u2500 IM-0015-0001.jpeg\r\n\u251c\u2500\u2500 IM-0016-0001.jpeg\r\n\u251c\u2500\u2500 IM-0017-0001.jpeg\r\n....\r\n```\r\nBut fds failed to execute the add command:\r\n```\r\n$ fds add data\/raw-data\r\n========== Make your selection, Press \"h\" for help ==========\r\n\r\nDVC add failed to execute\r\n```",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fails to add files to  tracking; Content: when running the `fds add` command for data files it tries to add them to  tracking but fails.\r\n\r\nin my case i tried to add the raw-data directory that contains the following image files:\r\n```\r\n$ tree data\/raw-data\r\ndata\/raw-data\r\n\u251c\u2500\u2500 im-0001-0001.jpeg\r\n\u251c\u2500\u2500 im-0003-0001.jpeg\r\n\u251c\u2500\u2500 im-0005-0001.jpeg\r\n\u251c\u2500\u2500 im-0006-0001.jpeg\r\n\u251c\u2500\u2500 im-0007-0001.jpeg\r\n\u251c\u2500\u2500 im-0009-0001.jpeg\r\n\u251c\u2500\u2500 im-0010-0001.jpeg\r\n\u251c\u2500\u2500 im-0011-0001-0001.jpeg\r\n\u251c\u2500\u2500 im-0011-0001-0002.jpeg\r\n\u251c\u2500\u2500 im-0011-0001.jpeg\r\n\u251c\u2500\u2500 im-0013-0001.jpeg\r\n\u251c\u2500\u2500 im-0015-0001.jpeg\r\n\u251c\u2500\u2500 im-0016-0001.jpeg\r\n\u251c\u2500\u2500 im-0017-0001.jpeg\r\n....\r\n```\r\nbut fds failed to execute the add command:\r\n```\r\n$ fds add data\/raw-data\r\n========== make your selection, press \"h\" for help ==========\r\n\r\n add failed to execute\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running the `fds add` command for data files, which failed to add them to tracking.",
        "Issue_preprocessed_content":"Title: fails to add files to tracking; Content: when running the command for data files it tries to add them to tracking but fails. in my case i tried to add the raw data directory that contains the following image files but fds failed to execute the add command"
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Issue_title":"Only display the DVC add prompt if there is anything to add",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1622117855000,
        "Issue_closed_time":1622551859000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: only display the  add prompt if there is anything to add; Content: currently, it will display always display\r\n`========== make your selection, press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/dagshub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/_service.py#l119",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the prompt to add a file was always displayed, even when the list of files was empty.",
        "Issue_preprocessed_content":"Title: only display the add prompt if there is anything to add; Content: currently, it will display always display even if there is no selection to make since the list of files is empty"
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Issue_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Issue_label":[
            "bug",
            "good first issue"
        ],
        "Issue_creation_time":1621771875000,
        "Issue_closed_time":1621785253000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: markdown in  install prompt isn't rendered as markdown; Content: `should we install [https:\/\/.org\/] (`pip install  <3`) for you right now?`\r\nhttps:\/\/github.com\/dagshub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#l27",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the markdown in an install prompt was not rendered as markdown.",
        "Issue_preprocessed_content":"Title: markdown in install prompt isn't rendered as markdown; Content: pip install <"
    },
    {
        "Issue_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/79",
        "Issue_title":"Use DVC remove instead of just removing the base image file",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1643114302000,
        "Issue_closed_time":1643645434000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The problem described in this issue es very similar to #77 .\r\n\r\nCurrently, the \"delete\" action just removes the base image file. This is not correct for some reasons:\r\n\r\n- The base images are under control by DVC. The right way to remove a file that has been previously added to DVC is using its remove command, which removes the file pointer.\r\n- The deletion of the base image is not needed because it is not actually in the repository: it is pushed to the DVC remote storage during the base image generation and does not persist after this finishes. In case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nTo summarize: the right way to do the deletion would be using DVC _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: use  remove instead of just removing the base image file; Content: the problem described in this issue es very similar to #77 .\r\n\r\ncurrently, the \"delete\" action just removes the base image file. this is not correct for some reasons:\r\n\r\n- the base images are under control by . the right way to remove a file that has been previously added to  is using its remove command, which removes the file pointer.\r\n- the deletion of the base image is not needed because it is not actually in the repository: it is pushed to the  remote storage during the base image generation and does not persist after this finishes. in case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nto summarize: the right way to do the deletion would be using  _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"delete\" action was not correctly removing the base image file, and instead the remove command from the wrapper should be used to properly delete the file.",
        "Issue_preprocessed_content":"Title: use remove instead of just removing the base image file; Content: the problem described in this issue es very similar to . currently, the delete action just removes the base image file. this is not correct for some reasons the base images are under control by . the right way to remove a file that has been previously added to is using its remove command, which removes the file pointer. the deletion of the base image is not needed because it is not actually in the repository it is pushed to the remote storage during the base image generation and does not persist after this finishes. in case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow. to summarize the right way to do the deletion would be using command, which is already available in the wrapper, and is how it must be implemented in the action."
    },
    {
        "Issue_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/77",
        "Issue_title":"Use DVC move instead of system's mv in rename action",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642662749000,
        "Issue_closed_time":1643122571000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The current implementation of the rename action (to be triggered when the \"rename\" section of the DVC diff contains elements) includes the actual rename of the base image using the shutils' mv command. \r\n\r\n```python\r\nguard_that_base_image_exists(base_filename_old)\r\ncreate_output_folder(base_filename_new)\r\nmove(f\"{base_filename_old}\", f\"{base_filename_new}\")\r\n```\r\n\r\nThis rename is to be committed afterwards so that the rename of the base image is applied to the main branch.\r\n\r\nHowever, this approach is invalid:\r\n\r\n- If only the actual file is renamed, when a DVC pull is performed, the file with the previous name will be pulled. We will get two identical files with different names.\r\n- Nor can we just rename the pointer (.dvc file), as the pointer file name is irrelevant to DVC. The _path_ property inside the pointer is what determines the filename of the pulled file.\r\n\r\nThe right, convenient way to implement the file rename action is using the **dvc rename** command that performs all these actions:\r\n\r\n- Rename the actual file\r\n- Rename the pointer\r\n- Update the _path_ property\r\n\r\nFor consistency, we should use our DVC wrapper. If the move command is not wrapped there, we can do it as part of this issue.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: use  move instead of system's mv in rename action; Content: the current implementation of the rename action (to be triggered when the \"rename\" section of the  diff contains elements) includes the actual rename of the base image using the shutils' mv command. \r\n\r\n```python\r\nguard_that_base_image_exists(base_filename_old)\r\ncreate_output_folder(base_filename_new)\r\nmove(f\"{base_filename_old}\", f\"{base_filename_new}\")\r\n```\r\n\r\nthis rename is to be committed afterwards so that the rename of the base image is applied to the main branch.\r\n\r\nhowever, this approach is invalid:\r\n\r\n- if only the actual file is renamed, when a  pull is performed, the file with the previous name will be pulled. we will get two identical files with different names.\r\n- nor can we just rename the pointer (. file), as the pointer file name is irrelevant to . the _path_ property inside the pointer is what determines the filename of the pulled file.\r\n\r\nthe right, convenient way to implement the file rename action is using the ** rename** command that performs all these actions:\r\n\r\n- rename the actual file\r\n- rename the pointer\r\n- update the _path_ property\r\n\r\nfor consistency, we should use our  wrapper. if the move command is not wrapped there, we can do it as part of this issue.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to rename a base image using the shutils' mv command, as this approach was invalid due to potential conflicts with the pull command, and the right way to implement the file rename action was to use the rename command.",
        "Issue_preprocessed_content":"Title: use move instead of system's mv in rename action; Content: the current implementation of the rename action includes the actual rename of the base image using the shutils' mv command. this rename is to be committed afterwards so that the rename of the base image is applied to the main branch. however, this approach is invalid if only the actual file is renamed, when a pull is performed, the file with the previous name will be pulled. we will get two identical files with different names. nor can we just rename the pointer , as the pointer file name is irrelevant to . the property inside the pointer is what determines the filename of the pulled file. the right, convenient way to implement the file rename action is using the rename command that performs all these actions rename the actual file rename the pointer update the property for consistency, we should use our wrapper. if the move command is not wrapped there, we can do it as part of this issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Issue_title":"data loading bug with dvc",
        "Issue_label":[
            "bug",
            "in progress"
        ],
        "Issue_creation_time":1641729327000,
        "Issue_closed_time":1642070875000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: data loading bug with ; Content: load_dataset function from hugging face can't access the  tracked data directory \r\n--> oserror: [errno 30] read-only file system: '\/data'",
        "Issue_original_content_gpt_summary":"The user encountered a data loading bug with the load_dataset function from hugging face, which resulted in an OSError due to a read-only file system in the tracked data directory.",
        "Issue_preprocessed_content":"Title: data loading bug with; Content: function from hugging face can't access the tracked data directory > oserror read only file system"
    },
    {
        "Issue_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Issue_title":"Missing params field for evaluate stage in dvc.yaml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1638704752000,
        "Issue_closed_time":1638706309000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: missing params field for evaluate stage in .yaml; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the evaluate stage in the .yaml file was missing the params field.",
        "Issue_preprocessed_content":"Title: missing params field for evaluate stage in; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/csia-pme\/csia-pme\/issues\/39",
        "Issue_title":"Resolve DVC Bad request with Minio",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1670317034000,
        "Issue_closed_time":1670919923000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: resolve  bad request with minio; Content: ![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a \"Bad Request\" error when attempting to use Minio, and needed to find a solution to resolve the issue.",
        "Issue_preprocessed_content":"Title: resolve bad request with minio"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/28",
        "Issue_title":"\"dvc-cc init\" just take three letters for the dvc folder name?",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1584006633000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"CALL DVC-CC INIT just takes the first three letters of the repo name???\r\n\r\nHere you can enter the folder where you want to store the DVC files on the DVC Storage Server.\r\n\tThe remote DVC folder that you want use (default: ~\/*****\/***\/TES): \r\nThe username with that you can access the DVC storage server \"dt1.f4.htw-berlin.de\".\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: \"-cc init\" just take three letters for the  folder name?; Content: call -cc init just takes the first three letters of the repo name???\r\n\r\nhere you can enter the folder where you want to store the  files on the  storage server.\r\n\tthe remote  folder that you want use (default: ~\/*****\/***\/tes): \r\nthe username with that you can access the  storage server \"dt1.f4.htw-berlin.de\".\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"-cc init\" command only took the first three letters of the repository name when attempting to store files on a remote storage server.",
        "Issue_preprocessed_content":"Title: cc init just take three letters for the folder name?; Content: call cc init just takes the first three letters of the repo name??? here you can enter the folder where you want to store the files on the storage server. the remote folder that you want use the username with that you can access the storage server"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/27",
        "Issue_title":"\"dvc pull\" does not work in the result branch if a sshfs connection is mounted",
        "Issue_label":[
            "bug",
            "Important"
        ],
        "Issue_creation_time":1583006053000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n> Entsprechend dem Tutorial habe ich mit sshfs den data Ordner\r\n> gemountet, um externe Daten verwenden zu k\u00f6nnen, was soweit auch\r\n> funktioniert.\r\n> Wenn ich dann aber nach erfolgreich abgeschlossenem Job die Ergebnisse\r\n> ansehen will (git pull, git checkout rcc_00XX_ergebnis_branch, dvc\r\n> pull), bekomme ich eine Fehlermeldung:\r\n> \r\n> rmdir: data: Das Ger\u00e4t oder die Ressource ist belegt\r\n> \r\n> Wenn ich vorher mit fusermount -u data den Dataordner wieder unmounte,\r\n> funktioniert alles wie erwartet. Ist das das zu erwartende Verhalten?\r\n> Muss ich also \"data\" unmounten, um die Ergebnisse ansehen zu k\u00f6nnen?\r\n> Und dann erneut mounten, um einen neuen Job zu starten?\r\n\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: \" pull\" does not work in the result branch if a sshfs connection is mounted; Content: **describe the bug**\r\n> entsprechend dem tutorial habe ich mit sshfs den data ordner\r\n> gemountet, um externe daten verwenden zu k\u00f6nnen, was soweit auch\r\n> funktioniert.\r\n> wenn ich dann aber nach erfolgreich abgeschlossenem job die ergebnisse\r\n> ansehen will (git pull, git checkout rcc_00xx_ergebnis_branch, \r\n> pull), bekomme ich eine fehlermeldung:\r\n> \r\n> rmdir: data: das ger\u00e4t oder die ressource ist belegt\r\n> \r\n> wenn ich vorher mit fusermount -u data den dataordner wieder unmounte,\r\n> funktioniert alles wie erwartet. ist das das zu erwartende verhalten?\r\n> muss ich also \"data\" unmounten, um die ergebnisse ansehen zu k\u00f6nnen?\r\n> und dann erneut mounten, um einen neuen job zu starten?\r\n\r\n>  -v 0.87.0\r\n> faice -v 9.1.0\r\n> -cc -v 0.8.66",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where \"pull\" does not work in the result branch if a sshfs connection is mounted.",
        "Issue_preprocessed_content":"Title: pull does not work in the result branch if a sshfs connection is mounted; Content: describe the bug > entsprechend dem tutorial habe ich mit sshfs den data ordner > gemountet, um externe daten verwenden zu knnen, was soweit auch > funktioniert. > wenn ich dann aber nach erfolgreich abgeschlossenem job die ergebnisse > ansehen will , bekomme ich eine fehlermeldung > > rmdir data das gert oder die ressource ist belegt > > wenn ich vorher mit fusermount u data den dataordner wieder unmounte, > funktioniert alles wie erwartet. ist das das zu erwartende verhalten? > muss ich also data unmounten, um die ergebnisse ansehen zu knnen? > und dann erneut mounten, um einen neuen job zu starten? > v > faice v > cc v"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/26",
        "Issue_title":"dvc servername and url not found by calling \"dvc-cc run\"",
        "Issue_label":[
            "bug",
            "Important"
        ],
        "Issue_creation_time":1583005907000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nIf the dvc\/config file is created with whitespaces dvc-cc cann't read the config file.\r\n\r\n**To Reproduce**\r\nCreate a dvc\/config file like this:\r\n`[core]\r\n    remote = dvc_connection\r\n['remote \"dvc_connection\"']\r\n    url = ...............\r\n    ask_password = true\r\n\r\n**Additional context**\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  servername and url not found by calling \"-cc run\"; Content: **describe the bug**\r\nif the \/config file is created with whitespaces -cc cann't read the config file.\r\n\r\n**to reproduce**\r\ncreate a \/config file like this:\r\n`[core]\r\n    remote = _connection\r\n['remote \"_connection\"']\r\n    url = ...............\r\n    ask_password = true\r\n\r\n**additional context**\r\n>  -v 0.87.0\r\n> faice -v 9.1.0\r\n> -cc -v 0.8.66\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the servername and url were not found when calling \"-cc run\" due to whitespaces in the \/config file.",
        "Issue_preprocessed_content":"Title: servername and url not found by calling cc run; Content: describe the bug if the file is created with whitespaces cc cann't read the config file. to reproduce create a file like this ` remote 'remote url true additional context > v > faice v > cc v"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20",
        "Issue_title":"DVC View and Plots don't load in `vscode-dvc`",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1655687938000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"UPDATE: Summary in https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20#issuecomment-1164570090\r\n\r\nI cloned https:\/\/github.com\/iterative\/dvc-checkpoints-mnist. I setup the IDE workspace so the extension is active.\r\n\r\nI haven't run any experiments:\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png)\r\n\r\nI check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/tree\/make_checkpoint) branch. The DVC view and Plots Dashboard never load.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png)\r\n\r\nThe Experiments Table says \"No Experiments to Display.\"\r\n\r\nOther components do load.\r\n\r\nDVC virtual env is loaded via MS Python extension.\r\n\r\n```console\r\n$ dvc version\r\nDVC version: 2.11.0 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.13 on macOS-12.4-arm64-arm-64bit\r\nSupports:\r\n        webhdfs (fsspec = 2022.5.0),\r\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\r\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)\r\nCache types: <https:\/\/error.dvc.org\/no-dvc-cache>\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk3s1s1\r\nRepo: dvc, git\r\n```\r\n\r\n---\r\n\r\n~~p.s. the same happens in the included `demo\/` project if I set up the extension with `\"dvc.dvcPath\": \"demo\/.env\/bin\/dvc\"` in .vscode\/settings.json (no MS Python extension).~~",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  view and plots don't load in `vscode-`; Content: update: summary in https:\/\/github.com\/iterative\/-checkpoints-mnist\/issues\/20#issuecomment-1164570090\r\n\r\ni cloned https:\/\/github.com\/iterative\/-checkpoints-mnist. i setup the ide workspace so the extension is active.\r\n\r\ni haven't run any experiments:\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png)\r\n\r\ni check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/-checkpoints-mnist\/tree\/make_checkpoint) branch. the  view and plots dashboard never load.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png)\r\n\r\nthe experiments table says \"no experiments to display.\"\r\n\r\nother components do load.\r\n\r\n virtual env is loaded via ms python extension.\r\n\r\n```console\r\n$  version\r\n version: 2.11.0 (pip)\r\n---------------------------------\r\nplatform: python 3.9.13 on macos-12.4-arm64-arm-64bit\r\nsupports:\r\n        webhdfs (fsspec = 2022.5.0),\r\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\r\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)\r\ncache types: <https:\/\/error..org\/no--cache>\r\ncaches: local\r\nremotes: none\r\nworkspace directory: apfs on \/dev\/disk3s1s1\r\nrepo: , git\r\n```\r\n\r\n---\r\n\r\n~~p.s. the same happens in the included `demo\/` project if i set up the extension with `\".path\": \"demo\/.env\/bin\/\"` in .vscode\/settings.json (no ms python extension).~~",
        "Issue_original_content_gpt_summary":"The user encountered challenges with loading view and plots in the `vscode-` extension while cloning the `iterative\/-checkpoints-mnist` repository, despite other components loading correctly.",
        "Issue_preprocessed_content":"Title: view and plots don't load in; Content: update summary in i cloned i setup the ide workspace so the extension is active. i haven't run any experiments i check out the branch. the view and plots dashboard never load. the experiments table says no experiments to other components do load. virtual env is loaded via ms python extension. the same happens in the included project if i set up the extension with in ."
    },
    {
        "Issue_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Issue_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1634634148000,
        "Issue_closed_time":1668173479000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: remote storage is not publicly accessible ( pull fails); Content: `error: unexpected error - forbidden: an error occurred (403) when calling the headobject operation: forbidden`\r\n\r\n` pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the remote storage was not publicly accessible, causing a `pull` to fail, and requiring the bucket to be made public and read-only in order to allow readers to access the content.",
        "Issue_preprocessed_content":"Title: remote storage is not publicly accessible; Content: needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only."
    },
    {
        "Issue_link":"https:\/\/github.com\/adamtupper\/cookiecutter-lvsn-workflow\/issues\/9",
        "Issue_title":"Post-gen hook shouldn't configure a DVC remote if no name is provided",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1623947751000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If the DVC remote name is left blank, the post-gen hook shouldn't try to set one. Currently this raises a (non-fatal) error.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: post-gen hook shouldn't configure a  remote if no name is provided; Content: if the  remote name is left blank, the post-gen hook shouldn't try to set one. currently this raises a (non-fatal) error.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the post-gen hook was attempting to configure a remote even when the remote name was left blank, resulting in a non-fatal error.",
        "Issue_preprocessed_content":"Title: post gen hook shouldn't configure a remote if no name is provided; Content: if the remote name is left blank, the post gen hook shouldn't try to set one. currently this raises a error."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Issue_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1633694389000,
        "Issue_closed_time":1633697794000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: attributeerror: module 'live' has no attribute 'log'; Content: i try to follow this checkpoints tutorial and documentation page https:\/\/.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nhowever, after adding `live` in the train.py file with this code: \r\n\r\n import the live package with the other imports:\r\n\r\n```python\r\nimport live\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('epoch %s: %s=%s'%(i, k, v))\r\n        live.log(k, v)\r\n    live.next_step()\r\n```\r\ni got an error: \r\n```bash \r\n\u276f  exp run\r\nmodified checkpoint experiment based on 'exp-defaa' will be created   \r\nrunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nepoch 1: loss=0.1541447937488556\r\ntraceback (most recent call last):\r\n  file \"[user-path]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  file \"[user-path]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    live.log(name=k, val=v)\r\nattributeerror: module 'live' has no attribute 'log'\r\n\r\nfile:\/\/\/[user-path]\/checkpoints-tutorial\/live.html\r\nerror: failed to reproduce '.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\ni only could run the example with the following trick: \r\n```python\r\nfrom live import live \r\nlive = live()\r\n```\r\nare there any updated in `live` api? \r\n\r\nsystem info\r\n```bash \r\n\u276f  doctor\r\n version: 2.6.4 (pip)\r\n---------------------------------\r\nplatform: python 3.9.4 on macos-11.6-x86_64-i386-64bit\r\nsupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\ncache types: reflink, hardlink, symlink\r\ncache directory: apfs on \/dev\/disk1s1s1\r\ncaches: local\r\nremotes: none\r\nworkspace directory: apfs on \/dev\/disk1s1s1\r\nrepo: , git\r\n```\r\n\r\nfiy @flippedcoder @daavoo ",
        "Issue_original_content_gpt_summary":"The user encountered an attributeerror when attempting to use the live package in their train.py file while following a Checkpoints tutorial and documentation page, and was able to resolve the issue with a workaround, but was curious if there had been any updates to the live API.",
        "Issue_preprocessed_content":"Title: attributeerror module 'live' has no attribute 'log'; Content: i try to follow this checkpoints tutorial and documentation page however, after adding in the file with this code import the live package with the other imports i got an error i only could run the example with the following trick are there any updated in api? system fiy"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Issue_title":"kedro mlflow ui gets a FileNotFoundError",
        "Issue_label":[
            "bug",
            "question"
        ],
        "Issue_creation_time":1664539296000,
        "Issue_closed_time":1664786016000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  mlflow ui gets a filenotfounderror; firstly i'd like to apologize if this is a dummy question.\r\ni'm following the tutorial to get introduced to  mlflow,; Content: after running the command \" mlflow init\" i tried to run the command \" mlflofw ui\" but i get an error:\r\n\r\ninfo     the 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). it is converted to a valid uri: 'file:\/\/\/c:\/users\/e107338\/pycharmprojects\/mlflow\/-mlflow-example\/mlruns'                                                   _mlflow_config.py:202\r\n\r\nafter the traceback i get an error: filenotfounderrror\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to run the command \"mlflow ui\" after running the command \"mlflow init\" while following a tutorial to get introduced to MLflow.",
        "Issue_preprocessed_content":"Title: mlflow ui gets a filenotfounderror; Content: firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to mlflow,; after running the command mlflow init i tried to run the command mlflofw ui but i get an error the key in is relative . it is converted to a valid uri after the traceback i get an error filenotfounderrror"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Issue_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1656532075000,
        "Issue_closed_time":1657139268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  mlflow init displays a wrong sucess message when the env folder does not exist; Content: ## description\r\n\r\nwhen running `` mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. we should move this code : \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/_mlflow\/framework\/cli\/cli.py#l116-l122\r\n\r\ninside the \"try\" block above.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running \"mlflow init --env=xxx\" displays a success message even if the env \"xxx\" folder does not exist, instead of an error message.",
        "Issue_preprocessed_content":"Title: mlflow init displays a wrong sucess message when the env folder does not exist; Content: description when running , a success message is displayed even if the env xxx folder does not exist, instead of an error message. we should move this code inside the try block above."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Issue_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1652380533000,
        "Issue_closed_time":1652640252000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: -mlflow is broken with ==0.18.1; Content: ## description\r\n\r\nthe plugin does not work with projects created with ``==0.18.1``\r\n\r\n## context\r\n\r\ntry to launch `` run`` in a project with ``==0.18.1`` and -mlflow installed.\r\n\r\n\r\n## steps to reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install ==0.18.1 -mlflow==0.9.0\r\n new --starter=pandas-iris\r\ncd pandas-iris\r\n mlflow init\r\n run\r\n```\r\n\r\n## expected result\r\n\r\nthis should run the pipeleine and log the parameters.\r\n\r\n## actual result\r\n\r\nthis raises the following error:\r\n\r\n```bash\r\nattributeerror: module '.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## your environment\r\n\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `` and `-mlflow` version used (`pip show ` and `pip show -mlflow`): ``==0.18.1`` and ``-mlflow<=0.9.0``\r\n* python version used (`python -v`): all\r\n* operating system and version: all\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## solution\r\n\r\ncurrently, -mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/_mlflow\/config\/_mlflow_config.py#l233-l247) inside a hook. \r\n\r\nwith ==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nretrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/_mlflow\/framework\/hooks\/pipeline_hook.py#l108-l109",
        "Issue_original_content_gpt_summary":"The user encountered a bug when trying to launch a pipeline with -mlflow installed in a project created with ``==0.18.1``, which was caused by the removal of the private ``_active_session`` global variable in ==0.18.1 and was solved by moving the configuration retrieval and setup to the ``after_context_created`` hook.",
        "Issue_preprocessed_content":"Title: mlflow is broken with; Content: description the plugin does not work with projects created with context try to launch in a project with and mlflow installed. steps to reproduce expected result this should run the pipeleine and log the parameters. actual result this raises the following error your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used all operating system and version all does the bug also happen with the last version on master? yes solution currently, mlflow uses inside a hook. with this private attribute was removed and the new recommandation is to use the hook. retrieving the configuration and set it up should be moved to this new hook"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/273",
        "Issue_title":"KedroPipelineModel requires unnecessary pipeline input dependencies to be executed",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1640015142000,
        "Issue_closed_time":1644791409000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi @Galileo-Galilei\r\n\r\n## Description\r\nthe KedroPipelineModel has a `initial_catalog` property which causes some problems. This `initial_catalog` can contain some Kedro Datasets but it's not necessary to log them when you train your model. because of this property I can't load my model anymore. I have to train it again.\r\n\r\nI explain : when I trained my model I used a kedro home-made plugin to load a specific dataset (which has no impact for my model). After that, I updated this plugin independently of my ML project. Today, I want to load my model but I can't because the load function uses the old Kedro Catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## Context\r\nIt would be great if we can update the kedro-catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## Possible Implementation\r\nLog in Mlflow what is only necessary.\r\n\r\nI hope my issue is clear.\r\n\r\nthank you",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pipelinemodel requires unnecessary pipeline input dependencies to be executed; Content: hi @galileo-galilei\r\n\r\n## description\r\nthe pipelinemodel has a `initial_catalog` property which causes some problems. this `initial_catalog` can contain some  datasets but it's not necessary to log them when you train your model. because of this property i can't load my model anymore. i have to train it again.\r\n\r\ni explain : when i trained my model i used a  home-made plugin to load a specific dataset (which has no impact for my model). after that, i updated this plugin independently of my ml project. today, i want to load my model but i can't because the load function uses the old  catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## context\r\nit would be great if we can update the -catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## possible implementation\r\nlog in mlflow what is only necessary.\r\n\r\ni hope my issue is clear.\r\n\r\nthank you",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the pipelinemodel requiring unnecessary pipeline input dependencies to be executed in order to load the model, preventing them from updating the catalog without having to retrain the model.",
        "Issue_preprocessed_content":"Title: pipelinemodel requires unnecessary pipeline input dependencies to be executed; Content: hi description the pipelinemodel has a property which causes some problems. this can contain some datasets but it's not necessary to log them when you train your model. because of this property i can't load my model anymore. i have to train it again. i explain when i trained my model i used a home made plugin to load a specific dataset . after that, i updated this plugin independently of my ml project. today, i want to load my model but i can't because the load function uses the old catalog with my old plugin version which is not in my environnement anymore. context it would be great if we can update the catalog without having to retrain our models. possible implementation log in mlflow what is only necessary. i hope my issue is clear. thank you"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Issue_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Issue_label":[
            "bug",
            "good first issue"
        ],
        "Issue_creation_time":1619193727000,
        "Issue_closed_time":1619987466000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: -mlflow cli is unavailable inside a  project; ## description\r\n\r\ni try to reproduce the minimal example from the docs: a  project using the starter `pandas-iris` using the `-mlflow` functinality. i do not arrive at initializing the -mlflow project, since the cli commands are not available.\r\n\r\n## context\r\n\r\nit is unclear to me if this is connected to #157 \r\ni wanted to start looking into -mlflow, but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## steps to reproduce\r\n\r\n```\r\nconda create -n _mlflow python=3.8\r\nconda activate _mlflow\r\npip install -mlflow\r\n mlflow -h\r\n new --starter=pandas-iris\r\ncd mlflow_test\/\r\n mlflow -h\r\n> error \"no such command 'mlflow'\"\r\n```\r\n\r\n## expected result\r\n\r\n` mlflow` is available in a project directory, i.e. ` mlflow -h` gives the same output inside the folder as before\r\n\r\n## actual result\r\n\r\ninside the project folder the `mlflow` command is unknown to \r\n\r\n```\r\n...\/miniconda3\/envs\/_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: deprecationwarning: use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: deprecationwarning: `np.object` is a deprecated alias for the builtin `object`. to silence this warning, use `object` by itself. doing this will not modify any behavior and is safe. \r\ndeprecated in numpy 1.20; Content: for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"binarytype\", np.object)\r\n2021-04-23 17:49:52,197 - root - info - registered hooks from 2 installed plugin(s): -mlflow-0.7.1\r\nusage:  [options] command [args]...\r\ntry ' -h' for help.\r\n\r\nerror: no such command 'mlflow'.\r\n\r\n```\r\n\r\n## your environment\r\n\r\nubuntu 18.04.5\r\n\r\n-  0.17.3\r\n- -mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the -mlflow CLI commands were unavailable inside a project, despite the user following the minimal example from the documentation and using the correct versions of -mlflow and Python.",
        "Issue_preprocessed_content":"Title: mlflow cli is unavailable inside a project; Content: description i try to reproduce the minimal example from the docs a project using the starter using the functinality. i do not arrive at initializing the mlflow project, since the cli commands are not available. context it is unclear to me if this is connected to i wanted to start looking into mlflow, but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. steps to reproduce expected result is available in a project directory, gives the same output inside the folder as before actual result inside the project folder the command is unknown to your environment ubuntu mlflow python mlflow does the bug also happen with the last version on master? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Issue_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1617627646000,
        "Issue_closed_time":1618006798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  mlflow ui does not use arguments from mlflow.yml; Content: ## description\r\n\r\nas described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run--mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## context & steps to reproduce\r\n\r\n- create a  project\r\n- call ` mlflow init`\r\n- modify the port in `mlflow.yml` to 5001\r\n- launch ` mlflow ui`\r\n\r\n## expected result\r\n\r\nthe mlflow ui should open in port 5001.\r\n\r\n## actual result\r\n\r\nit opens on port 5000 (the default).\r\n\r\n## your environment\r\n\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `` version: 0.17.0\r\n* `-mlflow` version: 0.6.0\r\n* python version used (`python -v`): 3.6.8\r\n* operating system and version: windows\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## solution\r\n\r\nwe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/_mlflow\/framework\/cli\/cli.py#l149-l151",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the mlflow ui command does not use the options specified in the mlflow.yml file, despite the expected result being that it should.",
        "Issue_preprocessed_content":"Title: mlflow ui does not use arguments from; Content: description as described in , the command does not use the options context & steps to reproduce create a project call modify the port in to launch expected result the mlflow ui should open in port . actual result it opens on port . your environment include as many relevant details about the environment in which you experienced the bug version version python version used operating system and version windows does the bug also happen with the last version on master? yes solution we should pass the arguments in the command"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Issue_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1610404594000,
        "Issue_closed_time":1615716614000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  mlflow cli is broken if configuration is declared in pyproject.toml; Content: ## description\r\n\r\n enable to declare configuration either in ``..yml`` or in ``pyproject.toml`` (in the ``[tool.]`` section). we claim to support both, but the cli commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## steps to reproduce\r\n\r\ncall `` mlflow init`` inside a project with no ``..yml`` file but only a ``pyproject.toml``.\r\n\r\n## expected result\r\n\r\nthe cli commands should be available (``init``)\r\n\r\n## actual result\r\nonly the ``new`` command is available. this is not considered as a  project.\r\n\r\n```\r\n-- separate them if you have more than one.\r\n```\r\n\r\n## your environment\r\n\r\n* `` and `-mlflow` version used (`pip show ` and `pip show -mlflow`): ==16.6, -mlflow==0.4.1\r\n* python version used (`python -v`): 3.7.9\r\n* operating system and version: windows 7\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes\r\n\r\n## solution\r\n\r\nthe error comes from the ``is__project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``..yml``.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the mlflow cli commands were not accessible if the project only contained a pyproject.toml file, despite the fact that it was declared in the [tool.] section.",
        "Issue_preprocessed_content":"Title: mlflow cli is broken if configuration is declared in; Content: description enable to declare configuration either in or in . we claim to support both, but the cli commands are not accessible if the project contains only a . steps to reproduce call inside a project with no file but only a . expected result the cli commands should be available actual result only the command is available. this is not considered as a project. your environment and version used python version used operating system and version windows does the bug also happen with the last version on develop? yes solution the error comes from the function which does not consider that a folder is the root of a kdro project if it does not contain a ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Issue_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605983313000,
        "Issue_closed_time":1606599848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: a pipelinemodel cannot be loaded from mlflow if its catalog contains non deepcopy-able datasets; Content: ## description\r\n\r\ni tried to load a pipelinemodel from mlflow, and i got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## context\r\n\r\ni cannot load a previously saved pipelinemodel generated by pipeline_ml_factory.\r\n\r\n## steps to reproduce\r\n\r\nsave a pipelinemodel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## expected result\r\n\r\nthe model should be loaded\r\n\r\n## actual result\r\n\r\nan error is raised\r\n\r\n## your environment\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `` and `-mlflow` version used: 0.16.5 and 0.4.0\r\n* python version used (`python -v`): 3.6.8\r\n* windows 10 & centos were tested\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes\r\n\r\n# potential solution\r\n\r\nthe faulty line is:\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_mlflow\/mlflow\/_pipeline_model.py#l45",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to load a previously saved pipelinemodel generated by pipeline_ml_factory due to the presence of an object which cannot be deepcopied in the dataset.",
        "Issue_preprocessed_content":"Title: a pipelinemodel cannot be loaded from mlflow if its catalog contains non deepcopy able datasets; Content: description i tried to load a pipelinemodel from mlflow, and i got a cannot pickle context artifacts error, which is due do the context i cannot load a previously saved pipelinemodel generated by steps to reproduce save a pipelinemodel with a dataset that contains an object which cannot be deepcopied expected result the model should be loaded actual result an error is raised your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used windows & centos were tested does the bug also happen with the last version on develop? yes potential solution the faulty line is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Issue_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605982845000,
        "Issue_closed_time":1606515096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: runstatus of mlflow run is \"finished\" instead of \"failed\" when the  run fails; Content: ## description\r\n\r\nwhen i launch ` run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## context\r\n\r\ni cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## steps to reproduce\r\n\r\nlaunch a failing pipeline with  run.\r\n\r\n## expected result\r\n\r\nthe mlflow ui should display the run with a red cross\r\n\r\n## actual result\r\n\r\nthe mlflow ui displays the run with a green tick\r\n\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes.\r\n\r\n## potential solution: \r\n\r\nreplace these lines:\r\n\r\n`https:\/\/github.com\/galileo-galilei\/-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_mlflow\/framework\/hooks\/pipeline_hook.py#l193-l194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.runstatus.failed)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runstatus of an mlflow run was incorrectly displayed as \"finished\" instead of \"failed\" when the run failed.",
        "Issue_preprocessed_content":"Title: runstatus of mlflow run is finished instead of failed when the run fails; Content: description when i launch and the run fails, the closes all the mlflow runs context i cannot distinguish failed runs from sucessful ones in the mlflow ui. steps to reproduce launch a failing pipeline with run. expected result the mlflow ui should display the run with a red cross actual result the mlflow ui displays the run with a green tick does the bug also happen with the last version on develop? yes. potential solution replace these lines with or even better, retrieve current run status from mlflow?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Issue_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605718283000,
        "Issue_closed_time":1605720463000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pipelineml objects in `hooks.py` breaks all -viz versions with  template>=0.16.5; Content: ## description\r\n\r\nif i create a pipelineml objects  and i return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass projecthooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> dict[str, pipeline]:\r\n        \"\"\"register the project's pipeline.\r\n        returns:\r\n            a mapping from a pipeline name to a ``pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n` run` command works fine, but ` viz` and ` pipeline list` fail.\r\n\r\n## context\r\n\r\ni was trying to visualise a pipeline with -viz==3.7.0 (i also tried 3.4.0 and 3.0.0), and ==0.16.6\r\n\r\n## steps to reproduce\r\n\r\n1. create a pipelineml object with pipeline_ml_factory in `hooks;py`\r\n2. launch ` viz` in terminal\r\n\r\n## expected result\r\n viz should be launched on localhost:5000\r\n\r\n## actual result\r\ntell us what happens instead.\r\n\r\n```\r\n-- if you received an error, place it here.\r\n```\r\n\r\n```\r\n-- separate them if you have more than one.\r\n```\r\n\r\n## your environment\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `` and `-mlflow` version used (`pip show ` and `pip show -mlflow`):\r\n* python version used (`python -v`):\r\n* operating system and version:\r\n\r\n*note: everything works fine with the older template (`<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes\r\n\r\n## potential solution: \r\n\r\nit seems the `__add__` method of the `pipelineml` class must be implemented.",
        "Issue_original_content_gpt_summary":"The user encountered a bug when attempting to visualise a pipeline with template>=0.16.5, where creating a pipelineml object in `hooks.py` and launching `viz` in the terminal resulted in an error, but the bug did not occur with the older template (`<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`, and the potential solution was to implement the `__add__` method of the `pipelineml` class.",
        "Issue_preprocessed_content":"Title: pipelineml objects in breaks all viz versions with; Content: description if i create a pipelineml objects and i return it in the run viz pipeline list hooks;py viz mlflow pip show pip show mlflow python v and the file instead of does the bug also happen with the last version on develop? yes potential solution it seems the method of the class must be implemented."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Issue_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1601890192000,
        "Issue_closed_time":1601893558000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: typeerror in _generate__command when debugging run in vscode; Content: `typeerror: object of type 'nonetype' has no len()` happens when suggested [vscode configuration for ](https:\/\/.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. the error is due to commandline arguments being `none` when running pipeline directly through `run.py`.\r\n\r\n```\r\ntraceback (most recent call last):\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else false,\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/-mlflow\/_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/-mlflow\/_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate__command\r\n    if len(from_inputs) > 0:\r\ntypeerror: object of type 'nonetype' has no len()\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when attempting to debug a run in VSCode using the suggested configuration, due to commandline arguments being `None` when running the pipeline directly through `run.py`.",
        "Issue_preprocessed_content":"Title: typeerror in when debugging run in vscode; Content: happens when suggested is used for debugging. the error is due to commandline arguments being when running pipeline directly through ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Issue_title":"Warning message appears when calling ``kedro mlflow init``",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1593379921000,
        "Issue_closed_time":1600718139000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: warning message appears when calling `` mlflow init``; Content: the warning claims that the project is not initialised yet, and that you must call `` mlflow init`` before calling any command while you are calling `` mlflow init``. it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command.",
        "Issue_original_content_gpt_summary":"The user encountered a warning message when calling `` mlflow init``, which was due to a bug caused by the dynamic creation of command, but could be safely ignored as the command worked as intended.",
        "Issue_preprocessed_content":"Title: warning message appears when calling; Content: the warning claims that the project is not initialised yet, and that you must call before calling any command while you are calling . it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command."
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/160",
        "Issue_title":"Add support for kedro namespaces in data catalog",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1658927398000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Since kedro 0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled.\r\n\r\nUnless the function to create kfp artifacts is disabled in `kubeflow.yaml` config:\r\n```yaml\r\nstore_kedro_outputs_as_kfp_artifacts: False\r\n```\r\nIt causes issues like:\r\n```\r\nValueError: Only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. Must begin with a letter. Got name: data_science.active_modelling_pipeline.X_train\r\n```\r\nwhen trying to run or update the pipeline.\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: add support for  namespaces in data catalog; Content: since  0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled.\r\n\r\nunless the function to create kfp artifacts is disabled in `kubeflow.yaml` config:\r\n```yaml\r\nstore__outputs_as_kfp_artifacts: false\r\n```\r\nit causes issues like:\r\n```\r\nvalueerror: only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. must begin with a letter. got name: data_science.active_modelling_pipeline.x_train\r\n```\r\nwhen trying to run or update the pipeline.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to add support for namespaces in the data catalog, as the function to create KFP artifacts was not properly handled and caused issues unless disabled in the 'kubeflow.yaml' config.",
        "Issue_preprocessed_content":"Title: add support for namespaces in data catalog; Content: since there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled. unless the function to create kfp artifacts is disabled in config it causes issues like when trying to run or update the pipeline."
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Issue_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1643989114000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: plugin only compatible with -mlflow<0.8.0; Content: ```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # noqa\r\n        from _mlflow.framework.context import get_mlflow_config  # noqa\r\n        return true\r\n    except importerror:\r\n        return false\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the plugin was only compatible with mlflow versions lower than 0.8.0, and the exception was thrown due to the context package being moved or refactored.",
        "Issue_preprocessed_content":"Title: plugin only compatible with; Content: alway throws exception since package has been moved or refactored"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/83",
        "Issue_title":"Kedro Telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669645759000,
        "Issue_closed_time":1670415546000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\nKedro Telemetry installed alongside a packaged and installed Kedro project breaks the project by assuming that the `pyproject.toml` file exists. The `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nThe problem was introduced with https:\/\/github.com\/kedro-org\/kedro-plugins\/pull\/62\r\n\r\n## Context\r\nWhen deploying Kedro projects and if you have installed Kedro Telemetry, it breaks your project.\r\n\r\n## Steps to Reproduce\r\n1. Create a Kedro project\r\n2. Add a dependency on kedro-telemetry\r\n3. Package it through `kedro package`\r\n4. Install it in a different environment\r\n5. Run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## Expected Result\r\nThe project should run.\r\n\r\n## Actual Result\r\nAn exception is thrown.\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.18.x\r\n* Kedro plugin and kedro plugin version used (`pip show kedro-telemetry`): 0.2.2 \r\n* Python version used (`python -V`): Not relevant\r\n* Operating system and version: Not relevant\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists; Content: ## description\r\n telemetry installed alongside a packaged and installed  project breaks the project by assuming that the `pyproject.toml` file exists. the `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nthe problem was introduced with https:\/\/github.com\/-org\/-plugins\/pull\/62\r\n\r\n## context\r\nwhen deploying  projects and if you have installed  telemetry, it breaks your project.\r\n\r\n## steps to reproduce\r\n1. create a  project\r\n2. add a dependency on -telemetry\r\n3. package it through ` package`\r\n4. install it in a different environment\r\n5. run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## expected result\r\nthe project should run.\r\n\r\n## actual result\r\nan exception is thrown.\r\n\r\n## your environment\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n*  version used (`pip show ` or ` -v`): 0.18.x\r\n*  plugin and  plugin version used (`pip show -telemetry`): 0.2.2 \r\n* python version used (`python -v`): not relevant\r\n* operating system and version: not relevant\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where telemetry installed alongside a packaged and installed project breaks the project by assuming that the `pyproject.toml` file exists.",
        "Issue_preprocessed_content":"Title: telemetry breaks packaged projects due to wrongly assuming exists; Content: description telemetry installed alongside a packaged and installed project breaks the project by assuming that the file exists. the is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases. the problem was introduced with context when deploying projects and if you have installed telemetry, it breaks your project. steps to reproduce . create a project . add a dependency on telemetry . package it through . install it in a different environment . run the project through in a folder where only the is expected result the project should run. actual result an exception is thrown. your environment include as many relevant details about the environment in which you experienced the bug version used plugin and plugin version used python version used not relevant operating system and version not relevant"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/64",
        "Issue_title":"pip installing kedro-datasets[option] causes different dependencies to installing kedro[option]",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666866011000,
        "Issue_closed_time":1667929584000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\nInstalling `kedro-datasets[option]` installs a different set of dependencies than `kedro[option]`. It appears that `kedro-datasets[option]` is installing the superset of requirements for all datasets.\n\n## Context\nThis is currently blocking https:\/\/github.com\/kedro-org\/kedro\/issues\/1495\n\n## Steps to Reproduce\n1. `pip install \"kedro[pandas.CSVDataSet]\"; pip freeze > requirements-kedro.txt`\n2. `pip install \"kedro-datasets[pandas.CSVDataSet]\"; pip freeze > requirements-kedro-datasets.txt`\n3. Compare the requirements\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pip installing -datasets[option] causes different dependencies to installing [option]; ## description\ninstalling `-datasets[option]` installs a different set of dependencies than `[option]`. it appears that `-datasets[option]` is installing the superset of requirements for all datasets.\n\n## context\nthis is currently blocking https:\/\/github.com\/-org\/\/issues\/1495\n\n## steps to reproduce\n1. `pip install \"[pandas.csvdataset]\"; pip freeze > requirements-.txt`\n2. `pip install \"-datasets[pandas.csvdataset]\"; Content: pip freeze > requirements--datasets.txt`\n3. compare the requirements\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where installing `-datasets[option]` installs a different set of dependencies than `[option]`, blocking a GitHub issue.",
        "Issue_preprocessed_content":"Title: pip installing datasets causes different dependencies to installing; Content: description installing installs a different set of dependencies than . it appears that is installing the superset of requirements for all datasets. context this is currently blocking steps to reproduce . . . compare the requirements"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/13",
        "Issue_title":"Kedro-Airflow not working with Astrocloud",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1648473272000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":12.0,
        "Issue_body":"Raised by @jweiss-ocurate:\r\n\r\n## Description\r\nI am trying to run a simple spaceflights example with Astrocloud. I wasn't sure if anyone has been able to get it to work. \r\n\r\nHere is the DockerFile:\r\nFROM quay.io\/astronomer\/astro-runtime:4.1.0\r\n\r\nRUN pip install --user new_kedro_project-0.1-py3-none-any.whl --ignore-requires-python\r\n\r\n## Context\r\nI am trying to use kedro-airflow with astrocloud.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Follow directions here https:\/\/kedro.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html\r\n2. Replace the DockerFile with the above mentioned image.\r\n\r\n## Expected Result\r\nComplete Kedro Run on local Airflow image.\r\n\r\n## Actual Result\r\nFailure in local Airflow image.\r\n[2022-02-26, 16:43:26 UTC] {store.py:32} INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\r\n[2022-02-26, 16:43:26 UTC] {session.py:78} WARNING - Unable to git describe \/usr\/local\/airflow\r\n[2022-02-26, 16:43:29 UTC] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment you experienced the bug in:\r\n\r\n* Kedro-Airflow plugin version used (get it by running `pip show kedro-airflow`): 0.4.1\r\n* Airflow version (`airflow --version`):\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.17.7\r\n* Python version used (`python -V`): > 2.0.0\r\n* Operating system and version: Ubuntu Linux 20.04",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: -airflow not working with astrocloud; Content: raised by @jweiss-ocurate:\r\n\r\n## description\r\ni am trying to run a simple spaceflights example with astrocloud. i wasn't sure if anyone has been able to get it to work. \r\n\r\nhere is the dockerfile:\r\nfrom quay.io\/astronomer\/astro-runtime:4.1.0\r\n\r\nrun pip install --user new__project-0.1-py3-none-any.whl --ignore-requires-python\r\n\r\n## context\r\ni am trying to use -airflow with astrocloud.\r\n\r\n## steps to reproduce\r\n\r\n1. follow directions here https:\/\/.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html\r\n2. replace the dockerfile with the above mentioned image.\r\n\r\n## expected result\r\ncomplete  run on local airflow image.\r\n\r\n## actual result\r\nfailure in local airflow image.\r\n[2022-02-26, 16:43:26 utc] {store.py:32} info - `read()` not implemented for `basesessionstore`. assuming empty store.\r\n[2022-02-26, 16:43:26 utc] {session.py:78} warning - unable to git describe \/usr\/local\/airflow\r\n[2022-02-26, 16:43:29 utc] {local_task_job.py:154} info - task exited with return code negsignal.sigkill\r\n\r\n## your environment\r\ninclude as many relevant details about the environment you experienced the bug in:\r\n\r\n* -airflow plugin version used (get it by running `pip show -airflow`): 0.4.1\r\n* airflow version (`airflow --version`):\r\n*  version used (`pip show ` or ` -v`): 0.17.7\r\n* python version used (`python -v`): > 2.0.0\r\n* operating system and version: ubuntu linux 20.04",
        "Issue_original_content_gpt_summary":"The user @jweiss-ocurate encountered challenges while trying to run a simple spaceflights example with astrocloud, resulting in failure in the local airflow image.",
        "Issue_preprocessed_content":"Title: airflow not working with astrocloud; Content: raised by description i am trying to run a simple spaceflights example with astrocloud. i wasn't sure if anyone has been able to get it to work. here is the dockerfile from run pip install user ignore requires python context i am trying to use airflow with astrocloud. steps to reproduce . follow directions here . replace the dockerfile with the above mentioned image. expected result complete run on local airflow image. actual result failure in local airflow image. , utc not implemented for . assuming empty store. , utc warning unable to git describe , utc task exited with return code your environment include as many relevant details about the environment you experienced the bug in airflow plugin version used airflow version version used python version used > operating system and version ubuntu linux"
    },
    {
        "Issue_link":"https:\/\/github.com\/quaseldoku\/QuaselDoku\/issues\/1",
        "Issue_title":"Running kedro-pipeline \"dp\" results in \"character maps to <undefined>\"-error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1654682698000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\nRunning the kedro-pipeline \"dp\" via kedro-cli with \"kedro run --pipeline dp\" results in the following error:\r\n```cmd\r\nkedro.io.core.DataSetError: Failed while loading data from data set TextDataSet(filepath=C:\/EEAA\/Repos\/QuaselDoku\/data\/01_raw\/Doku_v1\/Bedienung\/EasyInsert.html, protocol=file).\r\n'charmap' codec can't decode byte 0x81 in position 5899: character maps to <undefined>\r\n```\r\n\r\n### Steps to reproduce\r\ncatalog.yml:\r\n```yml\r\necu_test_doku:\r\n  type: PartitionedDataSet\r\n  path: data\/01_raw\/Doku_v1\r\n  dataset: text.TextDataSet\r\n  filename_suffix: html\r\n```\r\n\r\npython-function to parse documentation-data:\r\n```python\r\ndef filter_doku(partitioned_input: Dict[str, Callable[[], Any]], params: Dict) -> Dict[str, Callable[[], Any]]:\r\n    \"\"\"\r\n    flatten input where html files can occur on multiple levels, as well as filter out files that match certain string.\r\n    Return new Dictionary with filenames and load functions from which a PartioniedDataset can be created and persisted.\r\n\r\n    Args:\r\n        partitioned_input: A dictionary with partition ids (file path) as keys and load functions as values.\r\n\r\n    Returns:\r\n        Dictionary with the partitions to create.\r\n    \"\"\"\r\n\r\n    result = {}\r\n\r\n    print(\"filtering out relevant html files from doku ...\")\r\n    for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())):\r\n        \r\n        exclude = False\r\n        for string in params['exclude_docs']:\r\n            \r\n            if string in partition_key:\r\n                \r\n                exclude = True\r\n                break\r\n\r\n        if exclude:\r\n            continue\r\n\r\n        filename = partition_key.replace('\/', ' ')\r\n        filename += 'html'\r\n\r\n        # append new filename with load function to results dictionary\r\n        result[filename] = partition_load_func\r\n\r\n    return result\r\n```\r\n\r\nkedro  0.18.0\r\n\r\nThanks! :)\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: running -pipeline \"dp\" results in \"character maps to <undefined>\"-error; Content: ### description\r\nrunning the -pipeline \"dp\" via -cli with \" run --pipeline dp\" results in the following error:\r\n```cmd\r\n.io.core.dataseterror: failed while loading data from data set textdataset(filepath=c:\/eeaa\/repos\/quaseldoku\/data\/01_raw\/doku_v1\/bedienung\/easyinsert.html, protocol=file).\r\n'charmap' codec can't decode byte 0x81 in position 5899: character maps to <undefined>\r\n```\r\n\r\n### steps to reproduce\r\ncatalog.yml:\r\n```yml\r\necu_test_doku:\r\n  type: partitioneddataset\r\n  path: data\/01_raw\/doku_v1\r\n  dataset: text.textdataset\r\n  filename_suffix: html\r\n```\r\n\r\npython-function to parse documentation-data:\r\n```python\r\ndef filter_doku(partitioned_input: dict[str, callable[[], any]], params: dict) -> dict[str, callable[[], any]]:\r\n    \"\"\"\r\n    flatten input where html files can occur on multiple levels, as well as filter out files that match certain string.\r\n    return new dictionary with filenames and load functions from which a partionieddataset can be created and persisted.\r\n\r\n    args:\r\n        partitioned_input: a dictionary with partition ids (file path) as keys and load functions as values.\r\n\r\n    returns:\r\n        dictionary with the partitions to create.\r\n    \"\"\"\r\n\r\n    result = {}\r\n\r\n    print(\"filtering out relevant html files from doku ...\")\r\n    for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())):\r\n        \r\n        exclude = false\r\n        for string in params['exclude_docs']:\r\n            \r\n            if string in partition_key:\r\n                \r\n                exclude = true\r\n                break\r\n\r\n        if exclude:\r\n            continue\r\n\r\n        filename = partition_key.replace('\/', ' ')\r\n        filename += 'html'\r\n\r\n        # append new filename with load function to results dictionary\r\n        result[filename] = partition_load_func\r\n\r\n    return result\r\n```\r\n\r\n  0.18.0\r\n\r\nthanks! :)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the -pipeline \"dp\" via -cli with \"run --pipeline dp\" which resulted in a 'charmap' codec error.",
        "Issue_preprocessed_content":"Title: running pipeline dp results in character maps to error; Content: description running the pipeline dp via cli with run pipeline dp results in the following error steps to reproduce python function to parse documentation data thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/480",
        "Issue_title":"using mlflow without an mlflow writer configured appears to fail silently",
        "Issue_label":[
            "bug",
            "stale :zzz:"
        ],
        "Issue_creation_time":1647628309000,
        "Issue_closed_time":1655127386000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Summary\r\n\r\nProfiling with mlflow and without an mlflow writer fails silently. \r\n\r\n### Steps to Reproduce it\r\n\r\nuse mlflow with get_or_create_session and no files are written.\r\n\r\n### Example\r\n\r\nThere are examples of how to configure mlflow writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_mlflow.yaml\r\n\r\nwhylogs should mention the missing mlflow writer in a warning. Maybe we can automatically add the mlflow writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## What is the current *bug* behavior?\r\n\r\nlogging with mlflow and default configuration appears to fail silently.\r\n\r\n### What is the expected *correct* behavior?\r\n\r\nmlflow integration should write to mlflow by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: using  without an  writer configured appears to fail silently; Content: ### summary\r\n\r\nprofiling with  and without an  writer fails silently. \r\n\r\n### steps to reproduce it\r\n\r\nuse  with get_or_create_session and no files are written.\r\n\r\n### example\r\n\r\nthere are examples of how to configure  writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_.yaml\r\n\r\nwhylogs should mention the missing  writer in a warning. maybe we can automatically add the  writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## what is the current *bug* behavior?\r\n\r\nlogging with  and default configuration appears to fail silently.\r\n\r\n### what is the expected *correct* behavior?\r\n\r\n integration should write to  by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where profiling with WhyLogs and without an output writer configured fails silently, with no warning or indication of the missing configuration.",
        "Issue_preprocessed_content":"Title: using without an writer configured appears to fail silently; Content: summary profiling with and without an writer fails silently. steps to reproduce it use with and no files are written. example there are examples of how to configure writer config here whylogs should mention the missing writer in a warning. maybe we can automatically add the writer , so that it works and draws attention to where the behavior can be modified. what is the current bug behavior? logging with and default configuration appears to fail silently. what is the expected correct behavior? integration should write to by default and warn if missing or inconsistent config is set."
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Issue_title":"Support writing out dataset profiles as json format with mlflow",
        "Issue_label":[
            "bug",
            "stale :zzz:"
        ],
        "Issue_creation_time":1646156442000,
        "Issue_closed_time":1655127391000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: support writing out dataset profiles as json format with ; Content: ### summary\r\n\r\nyou can call .log_artifact directly and save the profile json:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\n.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to  writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to write out dataset profiles as json format with the .log_artifact method, as the format config specifying 'json' was not supported and instead used the protobuf bin format.",
        "Issue_preprocessed_content":"Title: support writing out dataset profiles as json format with; Content: summary you can call directly and save the profile json but if you pass a format config to writer specifying 'json' it isn't supported and instead uses the protobuf bin format."
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Issue_title":"MLflow example: close session error",
        "Issue_label":[
            "bug",
            "stale :zzz:"
        ],
        "Issue_creation_time":1641941774000,
        "Issue_closed_time":1655127397000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  example: close session error; Content: ### summary\r\n\r\n[<!-- summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/%20integration%20example.ipynb)\r\n### steps to reproduce it\r\n\r\nused binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nruntimeerror                              traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         .whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (optional). if not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is none:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is none:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise runtimeerror(\"session is already closed. cannot create more loggers\")\r\n    175 \r\n    176         # explicitly set the default timezone to utc if none was provided. helps with equality testing\r\n\r\nruntimeerror: session is already closed. cannot create more loggers\r\n```\r\n### example\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a runtime error when attempting to create a logger in a closed session.",
        "Issue_preprocessed_content":"Title: example close session error; Content: summary steps to reproduce it used binder to run the above notebook example"
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Issue_title":"MLFlow NameError",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1603138068000,
        "Issue_closed_time":1603222865000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  nameerror; Content: when i don't have the optional  dependency installed i get the following exception the first time i try to import the `numbertracker`.  the second time i run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nfailed to import \r\n---------------------------------------------------------------------------\r\nnameerror                                 traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from . import enable_\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/\/__init__.py in <module>\r\n----> 1 from .patcher import enable_\r\n      2 \r\n      3 __all__ = [\"enable_\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = .tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nnameerror: name '' is not defined\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a NameError when attempting to import the numbertracker module from the whylogs library, which was resolved by running the import a second time.",
        "Issue_preprocessed_content":"Title: nameerror; Content: when i don't have the optional dependency installed i get the following exception the first time i try to import the . the second time i run the import, everything works just fine."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Issue_title":"Project template mlflow secret bad name",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1650980403000,
        "Issue_closed_time":1664791991000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: project template  secret bad name; Content: currently the `.drone.yaml` is referencing the k8s secret `-server-secret` which doesn't exist by default.\r\n\r\nwe have noticed that `{{ .projectid }}--secret` secret is created when a `kdlproject` resource is created.\r\n\r\nto solve this issue the name of the `-server-secret` must be changed into `{{ .projectid }}--secret`",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the `.drone.yaml` was referencing a k8s secret `-server-secret` which didn't exist by default, and the solution was to change the name of the `-server-secret` to `{{ .projectid }}--secret`.",
        "Issue_preprocessed_content":"Title: project template secret bad name; Content: currently the is referencing the k s secret which doesn't exist by default. we have noticed that secret is created when a resource is created. to solve this issue the name of the must be changed into"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Issue_title":"Project operator mlflow image tag is set to \"latest\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1635429600000,
        "Issue_closed_time":1635871931000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: project operator  image tag is set to \"latest\"; Content: on chart release v0.13.2 the default value for projectoperator..image.tag is set to latest when it should be set to v0.13.2.\r\n\r\ncheck values.yml:\r\n\r\n```yaml\r\nprojectoperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullpolicy: ifnotpresent\r\n  :\r\n    image:\r\n      repository: konstellation\/\r\n      tag: latest\r\n      pullpolicy: ifnotpresent\r\n    volume:\r\n      storageclassname: standard\r\n      size: 1gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullpolicy: ifnotpresent\r\n```",
        "Issue_original_content_gpt_summary":"The encountered challenge was that the default value for projectoperator.image.tag was set to \"latest\" when it should have been set to \"v0.13.2\" in the chart release v0.13.2.",
        "Issue_preprocessed_content":"Title: project operator image tag is set to latest; Content: on chart release the default value for is set to latest when it should be set to check"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Issue_title":"Users can access to any MLflow project",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1619772559000,
        "Issue_closed_time":1620648494000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Only allow access to project members for the given MLflow.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: users can access to any  project; Content: only allow access to project members for the given .",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of allowing access to project members only for the given project, while still allowing access to any project.",
        "Issue_preprocessed_content":"Title: users can access to any project; Content: only allow access to project members for the given ."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Issue_title":"Bad MLflow artifact folder by default",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1619181542000,
        "Issue_closed_time":1623230636000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: bad  artifact folder by default; Content: the artifact folder by default is not reemplacing the `$artifacts_bucket` env var",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the artifact folder by default was not replacing the `$artifacts_bucket` environment variable.",
        "Issue_preprocessed_content":"Title: bad artifact folder by default; Content: the artifact folder by default is not reemplacing the env var"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Issue_title":"All MLflow experiments are visible for any user",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1619181390000,
        "Issue_closed_time":1623230615000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: all  experiments are visible for any user; Content: we should create a instance of  for each project in order to see the experiments related to the current project.\r\n\r\n- [x] create project operator to deploy a  instance for each project.\r\n- [x] update kdl app api to create the kdlproject custom resource in k8s.\r\n- [x] update kdlctl.sh adding project-operator docker image building.\r\n- [x] add project-operator to kdl server helm chart.\r\n- [x] add github workflows to publish the project-operator in docker hub.",
        "Issue_original_content_gpt_summary":"The user encountered challenges in creating a project operator to deploy a instance for each project, updating the KDL app API to create the KDLProject custom resource in K8s, updating the KDLctl.sh adding project-operator docker image building, adding project-operator to KDL server helm chart, and adding GitHub workflows to publish the project-operator in Docker Hub.",
        "Issue_preprocessed_content":"Title: all experiments are visible for any user; Content: we should create a instance of for each project in order to see the experiments related to the current project. create project operator to deploy a instance for each project. update kdl app api to create the kdlproject custom resource in k s. update adding project operator docker image building. add project operator to kdl server helm chart. add github workflows to publish the project operator in docker hub."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/150",
        "Issue_title":"Error loading MLFlow",
        "Issue_label":[
            "bug",
            "app-ui"
        ],
        "Issue_creation_time":1614760376000,
        "Issue_closed_time":1646221957000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error loading ",
        "Issue_original_content_gpt_summary":"The user encountered challenges when trying to load an error, resulting in a lack of access to the desired content.",
        "Issue_preprocessed_content":"Title: error loading"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Issue_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1580136891000,
        "Issue_closed_time":1580393757000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger: \"connection aborted.\" - remotedisconnected error; Content: **describe the bug**\r\ni try to do multi-label classification with \"doc_classification_multilabel.py\". it worked at first. however when it came to `\"train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, none)\r\n  file \"<string>\", line 3, in raise_from\r\n  file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  file \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  file \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  file \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise remotedisconnected(\"remote end closed connection without\"\r\nhttp.client.remotedisconnected: remote end closed connection without response\r\n......\r\nurllib3.exceptions.protocolerror: ('connection aborted.', remotedisconnected('remote end closed connection without response',))\r\n```\r\n\r\n  i have checked that the internet connection was ok. so i was confused why this error occured ?\r\n  \r\n\r\n**error message**\r\nerror that was thrown (if available)\r\n\r\n**expected behavior**\r\na clear and concise description of what you expected to happen.\r\n\r\n**additional context**\r\nadd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior\r\n\r\n**system:**\r\n - os: \r\n - gpu\/cpu:\r\n - farm version:\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a remotedisconnected error when attempting to do multi-label classification with \"doc_classification_multilabel.py\", despite having a working internet connection.",
        "Issue_preprocessed_content":"Title: logger connection remotedisconnected error; Content: describe the bug i try to do multi label classification with it worked at first. however when it came to , it stopped and report i have checked that the internet connection was ok. so i was confused why this error occured ? error message error that was thrown expected behavior a clear and concise description of what you expected to happen. additional context add any other context about the problem here, like type of downstream task, part of to reproduce steps to reproduce the behavior system os farm version"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/499",
        "Issue_title":"Permission denied when log models to mlflow on Mac",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1642469367000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"```\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py\", line 188, in log\r\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 584, in log_artifacts\r\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 977, in log_artifacts\r\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 334, in log_artifacts\r\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py\", line 57, in log_artifacts\r\n    mkdir(artifact_dir)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 113, in mkdir\r\n    raise e\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 110, in mkdir\r\n    os.makedirs(target)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  [Previous line repeated 2 more times]\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 225, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/var\/lib\/mlflow'\r\n```\r\n\r\nEnvironment:\r\n* mlflow==1.2\r\n* macOS 12.1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: permission denied when log models to  on mac; Content: ```\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/models\/model.py\", line 188, in log\r\n    .tracking.fluent.log_artifacts(local_path, artifact_path)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/fluent.py\", line 584, in log_artifacts\r\n    client().log_artifacts(run_id, local_dir, artifact_path)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/client.py\", line 977, in log_artifacts\r\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 334, in log_artifacts\r\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/store\/artifact\/local_artifact_repo.py\", line 57, in log_artifacts\r\n    mkdir(artifact_dir)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/utils\/file_utils.py\", line 113, in mkdir\r\n    raise e\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/utils\/file_utils.py\", line 110, in mkdir\r\n    os.makedirs(target)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  [previous line repeated 2 more times]\r\n  file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 225, in makedirs\r\n    mkdir(name, mode)\r\npermissionerror: [errno 13] permission denied: '\/var\/lib\/'\r\n```\r\n\r\nenvironment:\r\n* ==1.2\r\n* macos 12.1",
        "Issue_original_content_gpt_summary":"The user encountered a PermissionError when attempting to log models on MacOS, due to insufficient permissions to create a directory in the '\/var\/lib\/' directory.",
        "Issue_preprocessed_content":"Title: permission denied when log models to on mac; Content: environment macos"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Issue_title":"Can not create model in MLflowCatalog",
        "Issue_label":[
            "bug",
            "spark",
            "mlflow"
        ],
        "Issue_creation_time":1642187856000,
        "Issue_closed_time":1642206718000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: can not create model in catalog; Content: ```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> create model ssd1 using \":\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nerror: org.apache.hive.service.cli.hivesqlexception: error running query: org..tracking.httpexception: statuscode=404 reasonphrase=[not found] bodymessage=[{\"error_code\": \"resource_does_not_exist\", \"message\": \"registered model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.org$apache$spark$sql$hive$thriftserver$sparkexecutestatementoperation$$execute(sparkexecutestatementoperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.$anonfun$run$2(sparkexecutestatementoperation.scala:263)\r\n\tat scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties(sparkoperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperti\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to create a model in the catalog, resulting in an org.apache.hive.service.cli.hivesqlexception.",
        "Issue_preprocessed_content":"Title: can not create model in catalog"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Issue_title":"MlflowCatalog anonymous function is not registered ",
        "Issue_label":[
            "bug",
            "spark",
            "mlflow"
        ],
        "Issue_creation_time":1642186768000,
        "Issue_closed_time":1642203169000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: catalog anonymous function is not registered ; problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ml_predict(ssd, image) from coco limit 1;\r\nerror: org.apache.hive.service.cli.hivesqlexception: error running query: org.apache.spark.sql.analysisexception: undefined function: '<anonymous>'. this function is neither a registered temporary function nor a permanent function registered in the database 'default'.; Content: line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.org$apache$spark$sql$hive$thriftserver$sparkexecutestatementoperation$$execute(sparkexecutestatementoperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.$anonfun$run$2(sparkexecutestatementoperation.scala:263)\r\n\tat scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties(sparkoperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties$(sparkoperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.withlocalproperties(sparkexecutestatementoperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.run(sparkexecutestatementoperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.run(sparkexecutestatementoperation.scala:258)\r\n\tat java.security.accesscontroller.doprivileged(native method)\r\n\tat javax.security.auth.subject.doas(subject.java:422)\r\n\tat org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2.run(sparkexecutestatementoperation.scala:272)\r\n\tat java.util.concurrent.executors$runnableadapter.call(executors.java:511)\r\n\tat java.util.concurrent.futuretask.run(futuretask.java:266)\r\n\tat java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)\r\n```\r\n\r\nsteps to reproduce:\r\n\r\n1. register models into \r\n2. start spark thrift server\r\n3. use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `select ml_predict(ssd, image) from coco`",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to run an anonymous function in a Hive query, due to the function not being registered in the database.",
        "Issue_preprocessed_content":"Title: catalog anonymous function is not registered; Content: problem steps to reproduce . register models into . start spark thrift server . use to connec to the thrift server . run"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Issue_title":"Leaking mlflow dependency",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1617993087000,
        "Issue_closed_time":1617994925000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: leaking  dependency; Content: ```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import _logger as \r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/_logger.py:19: in <module>\r\n    import \r\ne   modulenotfounderror: no module named ''\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with a leaking dependency causing a ModuleNotFoundError when importing a module.",
        "Issue_preprocessed_content":"Title: leaking dependency"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/270",
        "Issue_title":"Pytorch Lightning 1.2.0 requires new MLflow version",
        "Issue_label":[
            "bug",
            "on hold"
        ],
        "Issue_creation_time":1613838919000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Keep it in mind before mindlessly updating\r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/4118",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pytorch lightning 1.2.0 requires new  version; Content: keep it in mind before mindlessly updating\r\n\r\nhttps:\/\/github.com\/\/\/pull\/4118",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where updating to the latest version of PyTorch Lightning 1.2.0 required a new version of PyTorch, and they needed to keep this in mind before mindlessly updating.",
        "Issue_preprocessed_content":"Title: pytorch lightning requires new version; Content: keep it in mind before mindlessly updating"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Issue_title":"Warning when training mlflow-pytorch 2.0.0",
        "Issue_label":[
            "bug",
            "major",
            "high-priority"
        ],
        "Issue_creation_time":1612379283000,
        "Issue_closed_time":1613342987000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: warning when training -pytorch 2.0.0; Content: `2021\/02\/03 19:07:05 warning .utils.autologging_utils: encountered unexpected error during autologging: float() argument must be a string or a number, not 'accuracy'`\r\n\r\nprinted after every epoch!",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected error during autologging when training with PyTorch 2.0.0, which printed a warning after every epoch.",
        "Issue_preprocessed_content":"Title: warning when training pytorch; Content: printed after every epoch!"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/171",
        "Issue_title":"subprocess.call and mlflow.log_artifact checks inconsistent in linter",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1608150046000,
        "Issue_closed_time":1613430703000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'mlflow.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nThose two linting functions caused the template create WFs (and sometimes even local) to fail\r\n\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThey should pass. We should discuss why they fail and how to fix!\r\nSo currently they are outcommented!\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: subprocess.call and .log_artifact checks inconsistent in linter; Content: **describe the bug**\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nthose two linting functions caused the template create wfs (and sometimes even local) to fail\r\n\r\n\r\n**expected behavior**\r\n<!-- a clear and concise description of what you expected to happen. -->\r\nthey should pass. we should discuss why they fail and how to fix!\r\nso currently they are outcommented!\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where two linting functions, `subprocess.call` and `.log_artifact`, caused the template create WFS (and sometimes even local) to fail, instead of passing as expected.",
        "Issue_preprocessed_content":"Title: and checks inconsistent in linter; Content: describe the bug those two linting functions caused the template create wfs to fail expected behavior they should pass. we should discuss why they fail and how to fix! so currently they are outcommented!"
    },
    {
        "Issue_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/548",
        "Issue_title":"MLFlow Error 409 when deploying --assets-only",
        "Issue_label":[
            "bug",
            "triage"
        ],
        "Issue_creation_time":1667484803000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Expected Behavior\r\nDeploy Jobs with --assets-only option\r\n## Current Behavior\r\nMLFlow API Request 409 Conflict \r\n## Steps to Reproduce (for bugs)\r\n[dbx][2022-11-03 12:30:40.370] \ud83d\udd0e Deployment file is not provided, searching in the conf directory\r\n[dbx][2022-11-03 12:30:40.375] \ud83d\udca1 Auto-discovery found deployment file conf\/deployment.json\r\n[dbx][2022-11-03 12:30:40.376] \ud83c\udd97 Deployment file conf\/deployment.json exists and will be used for deployment\r\n[dbx][2022-11-03 12:30:40.377] Starting new deployment for environment dev\r\n[dbx][2022-11-03 12:30:40.378] Using profile provided from the project file\r\n[dbx][2022-11-03 12:30:40.378] Found auth config from provider EnvironmentVariableConfigProvider, verifying it\r\n[dbx][2022-11-03 12:30:40.379] Found auth config from provider EnvironmentVariableConfigProvider, verification successful\r\n[dbx][2022-11-03 12:30:44.897] \r\n                Since v0.7.0 environment configurations should be nested under environments section.\r\n\r\n                Please nest environment configurations under this section to avoid potential issues while using \"build\"\r\n                configuration directive.\r\n            \r\n[dbx][2022-11-03 12:30:44.899] No build logic defined in the deployment file. Default pip-based build logic will be used.\r\n[dbx][2022-11-03 12:30:44.903] Usage of jobs keyword in deployment file is deprecated. Please use workflows instead (simply rename this section to workflows).\r\n[dbx][2022-11-03 12:30:44.906] Workflows ['add-on-chanel-AT', 'add-on-chanel-PL', 'add-on-PL'] were selected for further operations\r\n[dbx][2022-11-03 12:30:44.907] Following the provided build logic\r\n[dbx][2022-11-03 12:30:44.908] \ud83d\udc0d Building a Python-based project\r\n[dbx][2022-11-03 12:30:46.262] \u2705 Python-based project build finished\r\n[dbx][2022-11-03 12:30:46.264] Locating package file\r\n[dbx][2022-11-03 12:30:46.265] Package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl\r\n[dbx][2022-11-03 12:30:47.221] Starting the traversal process\r\n[dbx][2022-11-03 12:30:47.222] Processing libraries for workflow add-on-chanel-AT\r\n[dbx][2022-11-03 12:30:47.223] \u2705 Processing libraries for workflow add-on-chanel-AT - done\r\n[dbx][2022-11-03 12:30:47.224] Processing libraries for workflow add-on-chanel-PL\r\n[dbx][2022-11-03 12:30:47.225] \u2705 Processing libraries for workflow add-on-chanel-PL - done\r\n[dbx][2022-11-03 12:30:47.225] Processing libraries for workflow add-on-PL\r\n[dbx][2022-11-03 12:30:47.226] \u2705 Processing libraries for workflow add-on-PL - done\r\n[dbx][2022-11-03 12:30:47.227] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.412] \u2705 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.414] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma \u2502\r\n\u2502 nds\/deploy.py:157 in deploy                                                  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, ele \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   adjuster.traverse(deployable_workflows)                    \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   if not _assets_only:                                       \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, dep \u2502\r\n\u2502   [16](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:185 in traverse                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   182 \u2502   def traverse(self, workflows: Union[WorkflowList, List[str]]):     \u2502\r\n\u2502   183 \u2502   \u2502   dbx_echo(\"Starting the traversal process\")                     \u2502\r\n\u2502   184 \u2502   \u2502   self.property_adjuster.library_traverse(workflows, self.additi \u2502\r\n\u2502 \u2771 185 \u2502   \u2502   self.property_adjuster.file_traverse(workflows, self.file_adju \u2502\r\n\u2502   186 \u2502   \u2502   self.property_adjuster.property_traverse(workflows)            \u2502\r\n\u2502   187 \u2502   \u2502   self.property_adjuster.cluster_policy_traverse(workflows)      \u2502\r\n\u2502   188 \u2502   \u2502   dbx_echo(\"Traversal process finished, all provided references  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:168 in file_traverse                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   165 \u2502   \u2502   for element, parent, index in self.traverse(workflows):        \u2502\r\n\u2502   166 \u2502   \u2502   \u2502   if isinstance(element, str):                               \u2502\r\n\u2502   167 \u2502   \u2502   \u2502   \u2502   if element.startswith(\"file:\/\/\") or element.startswith \u2502\r\n\u2502 \u2771 168 \u2502   \u2502   \u2502   \u2502   \u2502   file_adjuster.adjust_file_ref(element, parent, ind \u2502\r\n\u2502   169                                                                        \u2502\r\n\u2502   [17](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0                                                                        \u2502\r\n\u2502   171 class Adjuster:                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/mixins\/file_reference.py:12 in adjust_file_ref                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    9 \u2502   \u2502   self._uploader = file_uploader                                  \u2502\r\n\u2502   10 \u2502                                                                       \u2502\r\n\u2502   11 \u2502   def adjust_file_ref(self, element: str, parent: Any, index: Any):   \u2502\r\n\u2502 \u2771 12 \u2502   \u2502   _uploaded = self._uploader.upload_and_provide_path(element)     \u2502\r\n\u2502   13 \u2502   \u2502   self.set_element_at_parent(_uploaded, parent, index)            \u2502\r\n\u2502   14                                                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \u2502\r\n\u2502 \/file_uploader.py:59 in upload_and_provide_path                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   56 \u2502   \u2502   \u2502   self._verify_fuse_support()                                 \u2502\r\n\u2502   57 \u2502   \u2502                                                                   \u2502\r\n\u2502   58 \u2502   \u2502   dbx_echo(f\":arrow_up: Uploading local file {local_file_path}\")  \u2502\r\n\u2502 \u2771 59 \u2502   \u2502   self._upload_file(local_file_path)                              \u2502\r\n\u2502   60 \u2502   \u2502   dbx_echo(f\":white_check_mark: Uploading local file {local_file_ \u2502\r\n\u2502   61 \u2502   \u2502   return self._postprocess_path(local_file_path, as_fuse)         \u2502\r\n\u2502   62                                                                         \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[19](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe                                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   196 \u2502   Wrapper around ``http_request`` that also verifies that the reques \u2502\r\n\u2502   197 \u2502   \"\"\"                                                                \u2502\r\n\u2502   198 \u2502   response = http_request(host_creds=host_creds, endpoint=endpoint,  \u2502\r\n\u2502 \u2771 199 \u2502   return verify_rest_response(response, endpoint)                    \u2502\r\n\u2502   [20](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0                                                                        \u2502\r\n\u2502   201                                                                        \u2502\r\n\u2502   202 def verify_rest_response(response, endpoint):                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[21](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   \u2502   endpoint,                                              \u2502\r\n\u2502   210 \u2502   \u2502   \u2502   \u2502   response.status_code,                                  \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502 \u2771 212 \u2502   \u2502   \u2502   raise MlflowException(                                     \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \"%s. Response body: '%s'\" % (base_msg, response.text), \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   \u2502   error_code=get_error_code(response.status_code),       \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nMlflowException: API request to endpoint \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f9[22](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n failed with error code 409 != 200. Response body: '<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text\/html;charset=ISO-8859-1\"\/>\r\n<title>Error 409 <\/title>\r\n<\/head>\r\n<body>\r\n<h2>HTTP ERROR: 409<\/h2>\r\n<p>Problem accessing \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f92[25](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n. Reason:\r\n<pre>    File already exists, cannot overwrite: \r\n&apos;\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874\r\n[28](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.p\r\ny&apos;<\/pre><\/p>\r\n<hr \/>\r\n<\/body>\r\n<\/html>\r\n'\r\nError: Process completed with exit code 1.\r\n\r\n## Context\r\nUpdated few jobs today using the latest dbx version, and at the jobless deployment cicd step I get the error above.\r\nMLFlow is only used to define a specific experiment path. No path related updates or changes here!\r\n## Your Environment\r\n\r\n* dbx version used: 0.8.x\r\n* Databricks Runtime version:  10.4 LTS (standard or ML)\r\n* Python version: 3.8.11",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  error 409 when deploying --assets-only; Content: ## expected behavior\r\ndeploy jobs with --assets-only option\r\n## current behavior\r\n api request 409 conflict \r\n## steps to reproduce (for bugs)\r\n[dbx][2022-11-03 12:30:40.370] \ud83d\udd0e deployment file is not provided, searching in the conf directory\r\n[dbx][2022-11-03 12:30:40.375] \ud83d\udca1 auto-discovery found deployment file conf\/deployment.json\r\n[dbx][2022-11-03 12:30:40.376] \ud83c\udd97 deployment file conf\/deployment.json exists and will be used for deployment\r\n[dbx][2022-11-03 12:30:40.377] starting new deployment for environment dev\r\n[dbx][2022-11-03 12:30:40.378] using profile provided from the project file\r\n[dbx][2022-11-03 12:30:40.378] found auth config from provider environmentvariableconfigprovider, verifying it\r\n[dbx][2022-11-03 12:30:40.379] found auth config from provider environmentvariableconfigprovider, verification successful\r\n[dbx][2022-11-03 12:30:44.897] \r\n                since v0.7.0 environment configurations should be nested under environments section.\r\n\r\n                please nest environment configurations under this section to avoid potential issues while using \"build\"\r\n                configuration directive.\r\n            \r\n[dbx][2022-11-03 12:30:44.899] no build logic defined in the deployment file. default pip-based build logic will be used.\r\n[dbx][2022-11-03 12:30:44.903] usage of jobs keyword in deployment file is deprecated. please use workflows instead (simply rename this section to workflows).\r\n[dbx][2022-11-03 12:30:44.906] workflows ['add-on-chanel-at', 'add-on-chanel-pl', 'add-on-pl'] were selected for further operations\r\n[dbx][2022-11-03 12:30:44.907] following the provided build logic\r\n[dbx][2022-11-03 12:30:44.908] \ud83d\udc0d building a python-based project\r\n[dbx][2022-11-03 12:30:46.262] \u2705 python-based project build finished\r\n[dbx][2022-11-03 12:30:46.264] locating package file\r\n[dbx][2022-11-03 12:30:46.265] package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl\r\n[dbx][2022-11-03 12:30:47.221] starting the traversal process\r\n[dbx][2022-11-03 12:30:47.222] processing libraries for workflow add-on-chanel-at\r\n[dbx][2022-11-03 12:30:47.223] \u2705 processing libraries for workflow add-on-chanel-at - done\r\n[dbx][2022-11-03 12:30:47.224] processing libraries for workflow add-on-chanel-pl\r\n[dbx][2022-11-03 12:30:47.225] \u2705 processing libraries for workflow add-on-chanel-pl - done\r\n[dbx][2022-11-03 12:30:47.225] processing libraries for workflow add-on-pl\r\n[dbx][2022-11-03 12:30:47.226] \u2705 processing libraries for workflow add-on-pl - done\r\n[dbx][2022-11-03 12:30:47.227] \u2b06 uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py\r\n[dbx][2022-11-03 12:30:50.412] \u2705 uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py\r\n[dbx][2022-11-03 12:30:50.414] \u2b06 uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma \u2502\r\n\u2502 nds\/deploy.py:157 in deploy                                                  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   wf_manager = workflowdeploymentmanager(api_client, ele \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   adjuster.traverse(deployable_workflows)                    \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   if not _assets_only:                                       \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   wf_manager = workflowdeploymentmanager(api_client, dep \u2502\r\n\u2502   [16](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:185 in traverse                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   182 \u2502   def traverse(self, workflows: union[workflowlist, list[str]]):     \u2502\r\n\u2502   183 \u2502   \u2502   dbx_echo(\"starting the traversal process\")                     \u2502\r\n\u2502   184 \u2502   \u2502   self.property_adjuster.library_traverse(workflows, self.additi \u2502\r\n\u2502 \u2771 185 \u2502   \u2502   self.property_adjuster.file_traverse(workflows, self.file_adju \u2502\r\n\u2502   186 \u2502   \u2502   self.property_adjuster.property_traverse(workflows)            \u2502\r\n\u2502   187 \u2502   \u2502   self.property_adjuster.cluster_policy_traverse(workflows)      \u2502\r\n\u2502   188 \u2502   \u2502   dbx_echo(\"traversal process finished, all provided references  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:168 in file_traverse                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   165 \u2502   \u2502   for element, parent, index in self.traverse(workflows):        \u2502\r\n\u2502   166 \u2502   \u2502   \u2502   if isinstance(element, str):                               \u2502\r\n\u2502   167 \u2502   \u2502   \u2502   \u2502   if element.startswith(\"file:\/\/\") or element.startswith \u2502\r\n\u2502 \u2771 168 \u2502   \u2502   \u2502   \u2502   \u2502   file_adjuster.adjust_file_ref(element, parent, ind \u2502\r\n\u2502   169                                                                        \u2502\r\n\u2502   [17](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0                                                                        \u2502\r\n\u2502   171 class adjuster:                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/mixins\/file_reference.py:12 in adjust_file_ref                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    9 \u2502   \u2502   self._uploader = file_uploader                                  \u2502\r\n\u2502   10 \u2502                                                                       \u2502\r\n\u2502   11 \u2502   def adjust_file_ref(self, element: str, parent: any, index: any):   \u2502\r\n\u2502 \u2771 12 \u2502   \u2502   _uploaded = self._uploader.upload_and_provide_path(element)     \u2502\r\n\u2502   13 \u2502   \u2502   self.set_element_at_parent(_uploaded, parent, index)            \u2502\r\n\u2502   14                                                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \u2502\r\n\u2502 \/file_uploader.py:59 in upload_and_provide_path                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   56 \u2502   \u2502   \u2502   self._verify_fuse_support()                                 \u2502\r\n\u2502   57 \u2502   \u2502                                                                   \u2502\r\n\u2502   58 \u2502   \u2502   dbx_echo(f\":arrow_up: uploading local file {local_file_path}\")  \u2502\r\n\u2502 \u2771 59 \u2502   \u2502   self._upload_file(local_file_path)                              \u2502\r\n\u2502   60 \u2502   \u2502   dbx_echo(f\":white_check_mark: uploading local file {local_file_ \u2502\r\n\u2502   61 \u2502   \u2502   return self._postprocess_path(local_file_path, as_fuse)         \u2502\r\n\u2502   62                                                                         \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[19](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe                                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   196 \u2502   wrapper around ``http_request`` that also verifies that the reques \u2502\r\n\u2502   197 \u2502   \"\"\"                                                                \u2502\r\n\u2502   198 \u2502   response = http_request(host_creds=host_creds, endpoint=endpoint,  \u2502\r\n\u2502 \u2771 199 \u2502   return verify_rest_response(response, endpoint)                    \u2502\r\n\u2502   [20](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0                                                                        \u2502\r\n\u2502   201                                                                        \u2502\r\n\u2502   202 def verify_rest_response(response, endpoint):                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[21](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   \u2502   endpoint,                                              \u2502\r\n\u2502   210 \u2502   \u2502   \u2502   \u2502   response.status_code,                                  \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502 \u2771 212 \u2502   \u2502   \u2502   raise exception(                                     \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \"%s. response body: '%s'\" % (base_msg, response.text), \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   \u2502   error_code=get_error_code(response.status_code),       \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nexception: api request to endpoint \r\n\/dbfs\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f9[22](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py\r\n failed with error code 409 != 200. response body: '<html>\r\n<head>\r\n<meta http-equiv=\"content-type\" content=\"text\/html;charset=iso-8859-1\"\/>\r\n<title>error 409 <\/title>\r\n<\/head>\r\n<body>\r\n<h2>http error: 409<\/h2>\r\n<p>problem accessing \r\n\/dbfs\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f92[25](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py\r\n. reason:\r\n<pre>    file already exists, cannot overwrite: \r\n&apos;\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874\r\n[28](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.p\r\ny&apos;<\/pre><\/p>\r\n<hr \/>\r\n<\/body>\r\n<\/html>\r\n'\r\nerror: process completed with exit code 1.\r\n\r\n## context\r\nupdated few jobs today using the latest dbx version, and at the jobless deployment cicd step i get the error above.\r\n is only used to define a specific experiment path. no path related updates or changes here!\r\n## your environment\r\n\r\n* dbx version used: 0.8.x\r\n* databricks runtime version:  10.4 lts (standard or ml)\r\n* python version: 3.8.11",
        "Issue_original_content_gpt_summary":"The user encountered an error 409 conflict when deploying with the --assets-only option, due to a file already existing in the specified path.",
        "Issue_preprocessed_content":"Title: error when deploying assets only; Content: expected behavior deploy jobs with assets only option current behavior api request conflict steps to reproduce dbx deployment file is not provided, searching in the conf directory dbx auto discovery found deployment file dbx deployment file exists and will be used for deployment dbx starting new deployment for environment dev dbx using profile provided from the project file dbx found auth config from provider environmentvariableconfigprovider, verifying it dbx found auth config from provider environmentvariableconfigprovider, verification successful dbx since environment configurations should be nested under environments section. please nest environment configurations under this section to avoid potential issues while using build configuration directive. dbx no build logic defined in the deployment file. default pip based build logic will be used. dbx usage of jobs keyword in deployment file is deprecated. please use workflows instead . dbx workflows were selected for further operations dbx following the provided build logic dbx building a python based project dbx python based project build finished dbx locating package file dbx package file located in dbx starting the traversal process dbx processing libraries for workflow add on chanel at dbx processing libraries for workflow add on chanel at done dbx processing libraries for workflow add on chanel pl dbx processing libraries for workflow add on chanel pl done dbx processing libraries for workflow add on pl dbx processing libraries for workflow add on pl done dbx uploading local file dbx uploading local file dbx uploading local file traceback in deploy ele else if not dep in traverse def traverse the traversal process process finished, all provided references in for element, parent, index in if isinstance if or parent, ind class adjuster in def element str, parent any, index any parent, index in uploading local file uploading local file return in wrapper around that also verifies that the reques response endpoint endpoint, return endpoint def endpoint in endpoint, raise exception , exception api request to endpoint failed with error code ! . response body ' error http error problem accessing . reason file already exists, cannot overwrite ' error process completed with exit code . context updated few jobs today using the latest dbx version, and at the jobless deployment cicd step i get the error above. is only used to define a specific experiment path. no path related updates or changes here! your environment dbx version used databricks runtime version lts python version"
    },
    {
        "Issue_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Issue_title":"dbx deploy fails due to mlflow experiment not found",
        "Issue_label":[
            "bug",
            "triage"
        ],
        "Issue_creation_time":1660398516000,
        "Issue_closed_time":1661539227000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: dbx deploy fails due to  experiment not found; Content: ## expected behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## current behavior\r\nthe command returns \r\n`.exceptions.restexception: invalid_parameter_value: experiment with id '2170254243754186' does not exist.`\r\n\r\n## steps to reproduce (for bugs)\r\nfollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## context\r\ntrying to set up dbx for the first time.\r\n\r\n## your environment\r\nmac os m1 2021 with macos monterey 12.5\r\n\r\n* dbx version used: databricks extensions aka dbx, version ~> 0.6.11\r\n* databricks runtime version: version 0.17.1",
        "Issue_original_content_gpt_summary":"The user encountered an issue when trying to deploy with dbx, where the command returned an error stating that the experiment with the specified ID did not exist.",
        "Issue_preprocessed_content":"Title: dbx deploy fails due to experiment not found; Content: expected behavior succeeds current behavior the command returns steps to reproduce follow the instructions at context trying to set up dbx for the first time. your environment mac os m with macos monterey dbx version used databricks extensions aka dbx, version > databricks runtime version version"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/576",
        "Issue_title":"[BUG]: Helm fetch command for ai-engine,sdk-helper and mlflow includes the 22.09 release instead of 22.11",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1671535506000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Version\r\n\r\n22.11\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\n_No response_\r\n\r\n### Describe the bug.\r\n\r\nai-engine fetch command at the 22.11 guide:\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-mlflow-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\n### Minimum reproducible example\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Full env printout\r\n\r\n_No response_\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]: helm fetch command for ai-engine,sdk-helper and  includes the 22.09 release instead of 22.11; Content: ### version\r\n\r\n22.11\r\n\r\n### which installation method(s) does this occur on?\r\n\r\n_no response_\r\n\r\n### describe the bug.\r\n\r\nai-engine fetch command at the 22.11 guide:\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$api_key --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$api_key --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus--22.09.tgz --username='$oauthtoken' --password=$api_key --untar\r\n\r\n### minimum reproducible example\r\n\r\n_no response_\r\n\r\n### relevant log output\r\n\r\n_no response_\r\n\r\n### full env printout\r\n\r\n_no response_\r\n\r\n### other\/misc.\r\n\r\n_no response_\r\n\r\n### code of conduct\r\n\r\n- [x] i agree to follow morpheus' code of conduct\r\n- [x] i have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/morpheus\/issues?q=is%3aopen+is%3aissue+label%3abug) and have found no duplicates for this bug report",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the helm fetch command for ai-engine, sdk-helper and includes the 22.09 release instead of 22.11.",
        "Issue_preprocessed_content":"Title: helm fetch command for ai engine,sdk helper and includes the release instead of; Content: version . which installation method does this occur on? describe the bug. ai engine fetch command at the guide helm fetch username '$oauthtoken' untar helm fetch username '$oauthtoken' untar helm fetch username '$oauthtoken' untar minimum reproducible example relevant log output full env printout code of conduct i agree to follow morpheus' code of conduct i have searched the and have found no duplicates for this bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/512",
        "Issue_title":"[BUG]: Unable to Start DFP Production MLFlow Server",
        "Issue_label":[
            "bug",
            "invalid"
        ],
        "Issue_creation_time":1669916494000,
        "Issue_closed_time":1669922495000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Version\r\n\r\n23.01\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\nDocker\r\n\r\n### Describe the bug.\r\n\r\nUnable to start the mlflow server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\nDowngrading  mlflow version to `<1.29.0` works fine.\r\n\r\n\r\n### Minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/Morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up mlflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[+] Running 3\/3                                                                                                                                               \r\n \u283f Network production_backend      Created                                                                                                               0.0s \r\n \u283f Network production_frontend     Created                                                                                                               0.0s\r\n \u283f Container mlflow_server  Created                                                                                                               0.1s\r\nAttaching to mlflow_server\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Error initializing backend store\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server  | Traceback (most recent call last):\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 392, in server\r\nmlflow_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 265, in initialize_backend_stores\r\nmlflow_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 244, in _get_tracking_store\r\nmlflow_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\nmlflow_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\nmlflow_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\nmlflow_server  |     return SqlAlchemyStore(store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\nmlflow_server  |     mlflow.store.db.utils._verify_schema(self.engine)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 71, in _verify_schema\r\nmlflow_server  |     raise MlflowException(\r\nmlflow_server  | mlflow.exceptions.MlflowException: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server exited with code 1\r\n```\r\n\r\n\r\n### Full env printout\r\n\r\n```shell\r\n<details><summary>Click here to see environment details<\/summary><pre>\r\n     \r\n     **git***\r\n     commit 9619c0e3a5ddbdd476aba9331f288aac855da7cd (HEAD -> dfp-pipeline-module, origin\/dfp-pipeline-module)\r\n     Author: bsuryadevara <bhargavsuryadevara@gmail.com>\r\n     Date:   Wed Nov 30 17:13:05 2022 -0600\r\n     \r\n     used dill to persist source and preprocess schema\r\n     **git submodules***\r\n     -27efc4fd1c984332920db2a2d6ab1f84d3cb55cd external\/morpheus-visualizations\r\n     \r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU7140058-R1\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.6 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\n     SUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\n     BUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\n     PRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx04 4.15.0-162-generic #170-Ubuntu SMP Mon Oct 18 11:38:05 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\r\n     \r\n     ***GPU Information***\r\n     Thu Dec  1 17:37:05 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    56W \/ 300W |  11763MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    43W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N\/A   28C    P0    41W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N\/A   29C    P0    44W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N\/A   31C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     \r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N\/A  N\/A     31232      C   ...da\/envs\/rapids\/bin\/python      303MiB |\r\n     |    0   N\/A  N\/A     41206      C   ...da\/envs\/rapids\/bin\/python     7051MiB |\r\n     |    0   N\/A  N\/A     52497      C   ...nda3\/envs\/venv\/bin\/python     3137MiB |\r\n     |    0   N\/A  N\/A     55973      C   tritonserver                     1267MiB |\r\n     +-----------------------------------------------------------------------------+\r\n     \r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3267.078\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.17\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n     \r\n     ***CMake***\r\n     \/usr\/bin\/cmake\r\n     cmake version 3.10.2\r\n     \r\n     CMake suite maintained and supported by Kitware (kitware.com\/cmake).\r\n     \r\n     ***g++***\r\n     \/usr\/bin\/g++\r\n     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n     Copyright (C) 2017 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n     \r\n     \r\n     ***nvcc***\r\n     \/usr\/local\/cuda\/bin\/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5\/compiler.30672275_0\r\n     \r\n     ***Python***\r\n     \/usr\/bin\/python\r\n     Python 2.7.17\r\n     \r\n     ***Environment Variables***\r\n     PATH                            : \/usr\/local\/cuda\/bin:\/opt\/bin\/:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin:\/home\/nfs\/bsuryadevara:\/home\/nfs\/bsuryadevara\r\n     LD_LIBRARY_PATH                 :\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    :\r\n     PYTHON_PATH                     :\r\n     \r\n     conda not found\r\n     ***pip packages***\r\n     \/usr\/bin\/pip\r\n\/usr\/lib\/python2.7\/dist-packages\/OpenSSL\/crypto.py:12: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\r\n  from cryptography import x509\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\n     ansible (2.9.9)\r\n     asn1crypto (0.24.0)\r\n     backports.functools-lru-cache (1.6.4)\r\n     backports.shutil-get-terminal-size (1.0.0)\r\n     bcrypt (3.1.7)\r\n     beautifulsoup4 (4.9.3)\r\n     boto3 (1.17.112)\r\n     botocore (1.20.112)\r\n     bs4 (0.0.1)\r\n     certifi (2018.1.18)\r\n     cffi (1.11.5)\r\n     chardet (3.0.4)\r\n     click (7.1.2)\r\n     configparser (4.0.2)\r\n     contextlib2 (0.6.0.post1)\r\n     cryptography (3.3.2)\r\n     decorator (4.1.2)\r\n     defusedxml (0.6.0)\r\n     distro (1.6.0)\r\n     dnspython (1.15.0)\r\n     docker (4.4.4)\r\n     docopt (0.6.2)\r\n     enum34 (1.1.10)\r\n     fastrlock (0.8)\r\n     flake8 (3.9.2)\r\n     functools32 (3.2.3.post2)\r\n     futures (3.3.0)\r\n     gssapi (1.4.1)\r\n     gyp (0.1)\r\n     html-to-json (2.0.0)\r\n     html2text (2019.8.11)\r\n     html5lib (0.999999999)\r\n     http (0.2)\r\n     httplib2 (0.14.0)\r\n     httpserver (1.1.0)\r\n     idna (2.6)\r\n     importlib-metadata (2.1.3)\r\n     ipaclient (4.6.90rc1+git20180411)\r\n     ipaddress (1.0.17)\r\n     ipalib (4.6.90rc1+git20180411)\r\n     ipaplatform (4.6.90rc1+git20180411)\r\n     ipapython (4.6.90rc1+git20180411)\r\n     Jinja2 (2.10)\r\n     jmespath (0.10.0)\r\n     lxml (4.2.1)\r\n     MarkupSafe (1.0)\r\n     mccabe (0.6.1)\r\n     netaddr (0.7.19)\r\n     netifaces (0.10.4)\r\n     numpy (1.16.6)\r\n     ofed-le-utils (1.0.3)\r\n     olefile (0.45.1)\r\n     pandas (0.24.2)\r\n     paramiko (2.11.0)\r\n     pathlib2 (2.3.7.post1)\r\n     Pillow (5.1.0)\r\n     pip (9.0.1)\r\n     ply (3.11)\r\n     pyasn1 (0.4.2)\r\n     pyasn1-modules (0.2.1)\r\n     pycodestyle (2.7.0)\r\n     pycparser (2.18)\r\n     pycrypto (2.6.1)\r\n     pyflakes (2.3.1)\r\n     pygobject (3.26.1)\r\n     PyNaCl (1.4.0)\r\n     pyOpenSSL (17.5.0)\r\n     python-apt (1.6.5+ubuntu0.7)\r\n     python-augeas (0.5.0)\r\n     python-dateutil (2.8.2)\r\n     python-dotenv (0.18.0)\r\n     python-ldap (3.0.0)\r\n     python-yubico (1.3.2)\r\n     pytz (2022.4)\r\n     pyusb (1.0.0)\r\n     PyYAML (5.4.1)\r\n     qrcode (5.3)\r\n     requests (2.27.1)\r\n     s3fs (0.2.2)\r\n     s3transfer (0.4.2)\r\n     scandir (1.10.0)\r\n     setuptools (39.0.1)\r\n     six (1.16.0)\r\n     soupsieve (1.9.6)\r\n     splunk-sdk (1.7.2)\r\n     subprocess32 (3.5.4)\r\n     tqdm (4.60.0)\r\n     typing (3.10.0.0)\r\n     urllib3 (1.26.12)\r\n     webencodings (0.5)\r\n     yapf (0.32.0)\r\n     zipp (1.2.0)\r\n     \r\n<\/pre><\/details>\r\n```\r\n\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]: unable to start dfp production  server; Content: ### describe the bug.\r\n\r\nunable to start the  server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\ndowngrading   version to `<1.29.0` works fine.\r\n\r\n\r\n### minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up \r\n```\r\n\r\n\r\n### relevant log output\r\n\r\n```shell\r\n[+] running 3\/3                                                                                                                                               \r\n \u283f network production_backend      created                                                                                                               0.0s \r\n \u283f network production_frontend     created                                                                                                               0.0s\r\n \u283f container _server  created                                                                                                               0.1s\r\nattaching to _server\r\n_server  | 2022\/12\/01 17:30:28 error .cli: error initializing backend store\r\n_server  | 2022\/12\/01 17:30:28 error .cli: detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). take a backup of your database, then run ' db upgrade <database_uri>' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n_server  | traceback (most recent call last):\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/cli.py\", line 392, in server\r\n_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 265, in initialize_backend_stores\r\n_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 244, in _get_tracking_store\r\n_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\n_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\n_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\n_server  |     return sqlalchemystore(store_uri, artifact_uri)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\n_server  |     .store.db.utils._verify_schema(self.engine)\r\n_server  |   file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/store\/db\/utils.py\", line 71, in _verify_schema\r\n_server  |     raise exception(\r\n_server  | .exceptions.exception: detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). take a backup of your database, then run ' db upgrade <database_uri>' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n_server exited with code 1\r\n```\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to start the Digital Fingerprinting Production Server, as the database schema was out-of-date and needed to be migrated in order to work properly.",
        "Issue_preprocessed_content":"Title: unable to start dfp production server; Content: version . which installation method does this occur on? docker describe the bug. unable to start the server when using but it works fine with downgrading version to works fine. minimum reproducible example relevant log output full env printout code of conduct i agree to follow morpheus' code of conduct i have searched the and have found no duplicates for this bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Issue_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Issue_label":[
            "bug",
            "? - Needs Triage"
        ],
        "Issue_creation_time":1653424629000,
        "Issue_closed_time":1654018977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  deployments create can fail (k8s\/helm); Content: **describe the bug**\r\nfor some reason, ` deployment create ...` can fail unexpectedly. \r\n\r\n```\r\n deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -c \"version=1\"\r\ncopied \/\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nsaved -meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\ntraceback (most recent call last):\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.inferenceserverexception: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nfix is to delete the  pod and start over.\r\n\r\n**steps\/code to reproduce bug**\r\nfollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**expected behavior**\r\nsuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**environment overview (please complete the following information)**\r\n - environment location: launchpad\r\n - method of morpheus install: kubernetes\r\n\r\n**environment details**\r\nlaunchpad helm deployment on a30. unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**additional context**\r\n sqlite db likely gets corrupted or otherwise \"confused\". possibly an issue in tritonclient?\r\ntriton logging complains about unable to read config.pbtxt\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where deployments created with k8s\/helm can fail unexpectedly, requiring the user to delete the pod and start over.",
        "Issue_preprocessed_content":"Title: deployments create can fail; Content: describe the bug for some reason, can fail unexpectedly. fix is to delete the pod and start over. to reproduce bug follow steps in expected behavior successful deployment as described at environment overview environment location launchpad method of morpheus install kubernetes environment details launchpad helm deployment on a . unfortunately, unable to capture the output from ipykernel there. additional context sqlite db likely gets corrupted or otherwise confused . possibly an issue in tritonclient? triton logging complains about unable to read"
    },
    {
        "Issue_link":"https:\/\/github.com\/equinor\/flownet\/issues\/408",
        "Issue_title":"MLFlow expecting mlruns folder",
        "Issue_label":[
            "invalid"
        ],
        "Issue_creation_time":1621607539000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When running hyperparameter tuning, MLflow expects an mlruns folder - which we don't create. If we stick with the standard we can ommit having to run `mlflow ui` with the backend store argument.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  expecting mlruns folder; Content: when running hyperparameter tuning,  expects an mlruns folder - which we don't create. if we stick with the standard we can ommit having to run ` ui` with the backend store argument.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running hyperparameter tuning, as it expects an mlruns folder which was not created, requiring the user to run the 'ui' command with the backend store argument.",
        "Issue_preprocessed_content":"Title: expecting mlruns folder; Content: when running hyperparameter tuning, expects an mlruns folder which we don't create. if we stick with the standard we can ommit having to run with the backend store argument."
    },
    {
        "Issue_link":"https:\/\/github.com\/equinor\/flownet\/issues\/269",
        "Issue_title":"Failed ERT runs are not registered correctly in mlflow",
        "Issue_label":[
            "invalid"
        ],
        "Issue_creation_time":1606471214000,
        "Issue_closed_time":1606475795000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If an ERT subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. This will then lead to a \"successful\" run in mlflow, whereas it should be registered as a failed run.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: failed ert runs are not registered correctly in ; Content: if an ert subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. this will then lead to a \"successful\" run in , whereas it should be registered as a failed run.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where failed ERT runs were not being registered correctly in , leading to a \"successful\" run being registered when it should have been registered as a failed run.",
        "Issue_preprocessed_content":"Title: failed ert runs are not registered correctly in; Content: if an ert subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than is ignored. this will then lead to a successful run in , whereas it should be registered as a failed run."
    },
    {
        "Issue_link":"https:\/\/github.com\/NRCan\/geo-deep-learning\/issues\/440",
        "Issue_title":"Fix logging of parameters on Mlflow",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1671594369000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Logging of parameters on Mlflow works as expected with default parameters set with Hydra; However hydra allows modification of parameters per experiment run, but modified parameters are not logged on Mlflow.  ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fix logging of parameters on ; logging of parameters on  works as expected with default parameters set with hydra; Content: however hydra allows modification of parameters per experiment run, but modified parameters are not logged on .  ",
        "Issue_original_content_gpt_summary":"The user encountered challenges with logging of parameters on , as modified parameters were not logged when using Hydra to modify parameters per experiment run.",
        "Issue_preprocessed_content":"Title: fix logging of parameters on; Content: logging of parameters on works as expected with default parameters set with hydra; however hydra allows modification of parameters per experiment run, but modified parameters are not logged on ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/362",
        "Issue_title":"MlflowArtifactDataset.load() fails if artifact_path is not None and run_id is specified",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1664829089000,
        "Issue_closed_time":1665079955000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nWhen you try to specify an artifact path and a run_id in an ``MlflowArtifactDataSet``, you get an error. \r\n\r\nThis works:\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=None,\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: artifactdataset.load() fails if artifact_path is not none and run_id is specified; Content: ## description\r\n\r\nwhen you try to specify an artifact path and a run_id in an ``artifactdataset``, you get an error. \r\n\r\nthis works:\r\n```python\r\n_csv_dataset = artifactdataset(\r\n    data_set=dict(type=csvdataset, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=none,\r\n    run_id=\"1234\",\r\n)\r\n_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\n_csv_dataset = artifactdataset(\r\n    data_set=dict(type=csvdataset, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\n_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to specify an artifact path and a run_id in an ``artifactdataset``.",
        "Issue_preprocessed_content":"Title: fails if is not none and is specified; Content: description when you try to specify an artifact path and a in an , you get an error. this works while this raises the following error"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Issue_title":"kedro mlflow ui gets a FileNotFoundError",
        "Issue_label":[
            "bug",
            "question"
        ],
        "Issue_creation_time":1664539296000,
        "Issue_closed_time":1664786016000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro  ui gets a filenotfounderror; firstly i'd like to apologize if this is a dummy question.\r\ni'm following the tutorial to get introduced to kedro ,; Content: after running the command \"kedro  init\" i tried to run the command \"kedro mlflofw ui\" but i get an error:\r\n\r\ninfo     the '_tracking_uri' key in .yml is relative ('server._tracking_uri = mlruns'). it is converted to a valid uri: 'file:\/\/\/c:\/users\/e107338\/pycharmprojects\/\/kedro--example\/mlruns'                                                   kedro__config.py:202\r\n\r\nafter the traceback i get an error: filenotfounderrror\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to run the command \"kedro mlflow ui\" after running the command \"kedro init\" while following a tutorial to get introduced to Kedro.",
        "Issue_preprocessed_content":"Title: kedro ui gets a filenotfounderror; Content: firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to kedro ,; after running the command kedro init i tried to run the command kedro mlflofw ui but i get an error the key in is relative . it is converted to a valid uri after the traceback i get an error filenotfounderrror"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Issue_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1656532075000,
        "Issue_closed_time":1657139268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro  init displays a wrong sucess message when the env folder does not exist; Content: ## description\r\n\r\nwhen running ``kedro  init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. we should move this code : \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_\/framework\/cli\/cli.py#l116-l122\r\n\r\ninside the \"try\" block above.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running \"kedro  init --env=xxx\" displays a success message even if the env \"xxx\" folder does not exist, instead of an error message.",
        "Issue_preprocessed_content":"Title: kedro init displays a wrong sucess message when the env folder does not exist; Content: description when running , a success message is displayed even if the env xxx folder does not exist, instead of an error message. we should move this code inside the try block above."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Issue_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1652380533000,
        "Issue_closed_time":1652640252000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro- is broken with kedro==0.18.1; Content: ## description\r\n\r\nthe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## context\r\n\r\ntry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro- installed.\r\n\r\n\r\n## steps to reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro  init\r\nkedro run\r\n```\r\n\r\n## expected result\r\n\r\nthis should run the pipeleine and log the parameters.\r\n\r\n## actual result\r\n\r\nthis raises the following error:\r\n\r\n```bash\r\nattributeerror: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## your environment\r\n\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-` version used (`pip show kedro` and `pip show kedro-`): ``kedro==0.18.1`` and ``kedro-<=0.9.0``\r\n* python version used (`python -v`): all\r\n* operating system and version: all\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## solution\r\n\r\ncurrently, kedro- uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/galileo-galilei\/kedro-\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_\/config\/kedro__config.py#l233-l247) inside a hook. \r\n\r\nwith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nretrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_\/framework\/hooks\/pipeline_hook.py#l108-l109",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with kedro- not working with projects created with ``kedro==0.18.1``, which was solved by moving the configuration setup to the ``after_context_created`` hook.",
        "Issue_preprocessed_content":"Title: kedro is broken with; Content: description the plugin does not work with projects created with context try to launch in a project with and kedro installed. steps to reproduce expected result this should run the pipeleine and log the parameters. actual result this raises the following error your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used all operating system and version all does the bug also happen with the last version on master? yes solution currently, kedro uses inside a hook. with this private attribute was removed and the new recommandation is to use the hook. retrieving the configuration and set it up should be moved to this new hook"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/258",
        "Issue_title":"MlflowArtifactDataSet does not work with PartitionedDataSet",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1636062318000,
        "Issue_closed_time":1644674290000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nIt is not possible to store a ``PartitionedDataSet`` as an mlflow artifact with the ``MlflowArtifactDataSet``.\r\n\r\n## Context\r\n\r\nI had a use case where I need to save a dict with many small result tables to mlflow, and I tried to use ``PartitionedDataSet`` for this.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: PartitionedDataSet  # or any valid kedro DataSet\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.CSVDataSet\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.DataFrame(data=[1,2,3], columns=[\"a\"], \"b\": pd.DataFrame(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## Expected Result\r\n\r\nThe 2 Dataframes should be logged as artifacts in the current mlflow run.\r\n\r\n## Actual Result\r\n\r\nAn error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe error comes from this line:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py#L53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: artifactdataset does not work with partitioneddataset; Content: ## description\r\n\r\nit is not possible to store a ``partitioneddataset`` as an  artifact with the ``artifactdataset``.\r\n\r\n## context\r\n\r\ni had a use case where i need to save a dict with many small result tables to , and i tried to use ``partitioneddataset`` for this.\r\n\r\n## steps to reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_.io.artifacts.artifactdataset\r\n    data_set:\r\n        type: partitioneddataset  # or any valid kedro dataset\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.csvdataset\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.dataframe(data=[1,2,3], columns=[\"a\"], \"b\": pd.dataframe(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## expected result\r\n\r\nthe 2 dataframes should be logged as artifacts in the current  run.\r\n\r\n## actual result\r\n\r\nan error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## potential solution\r\n\r\nthe error comes from this line:\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_\/io\/artifacts\/_artifact_dataset.py#l53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where it was not possible to store a partitioneddataset as an artifact with the artifactdataset.",
        "Issue_preprocessed_content":"Title: artifactdataset does not work with partitioneddataset; Content: description it is not possible to store a as an artifact with the . context i had a use case where i need to save a dict with many small result tables to , and i tried to use for this. steps to reproduce then save a dict using this dataset expected result the dataframes should be logged as artifacts in the current run. actual result an error is raised. does the bug also happen with the last version on master? yes potential solution the error comes from this line maybe we can add a better condition here to default to path if there is no filepath attribute."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/256",
        "Issue_title":"Setting the mlflow experiment does not work in interactive mode",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1636045277000,
        "Issue_closed_time":1636318265000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nIf I specify an experiment in `mlflow.yml`, and the set up the mlflow configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in mlflow \"Default\" (0) experiment. This works when running \"kedro run\" through the CLI.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# mlflow.yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: True  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import KedroSession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith KedroSession.create(project_path=r\"path\/to\/project\"):\r\n    config=get_mlflow_config()\r\n    config.setup()\r\n    \r\n    mlflow.log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"Default\".\r\n\r\n```\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe faulty line is: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L100\r\n\r\n[We should use mlflow ``mlflow.set_experiment`` method](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.set_experiment), but it does not restore deleted experiment. This wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L124-L132",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: setting the  experiment does not work in interactive mode; Content: ## description\r\n\r\nif i specify an experiment in `.yml`, and the set up the  configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in  \"default\" (0) experiment. this works when running \"kedro run\" through the cli.\r\n\r\n## steps to reproduce\r\n\r\n```yaml\r\n# .yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: true  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import kedrosession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_.config import get__config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith kedrosession.create(project_path=r\"path\/to\/project\"):\r\n    config=get__config()\r\n    config.setup()\r\n    \r\n    .log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"default\".\r\n\r\n```\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## potential solution\r\n\r\nthe faulty line is: \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_\/config\/kedro__config.py#l100\r\n\r\n[we should use  ``.set_experiment`` method](https:\/\/www..org\/docs\/latest\/python_api\/.html#.set_experiment), but it does not restore deleted experiment. this wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_\/config\/kedro__config.py#l124-l132",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where specifying an experiment in a .yml file and then setting up the configuration interactively caused all runs to be stored in the \"default\" (0) experiment instead of the specified experiment.",
        "Issue_preprocessed_content":"Title: setting the experiment does not work in interactive mode; Content: description if i specify an experiment in , and the set up the configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in default experiment. this works when running kedro run through the cli. steps to reproduce does the bug also happen with the last version on master? yes potential solution the faulty line is we should use but it does not restore deleted experiment. this wil replace part of the logic here"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Issue_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Issue_label":[
            "bug",
            "good first issue"
        ],
        "Issue_creation_time":1619193727000,
        "Issue_closed_time":1619987466000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro- cli is unavailable inside a kedro project; ## description\r\n\r\ni try to reproduce the minimal example from the docs: a kedro project using the starter `pandas-iris` using the `kedro-` functinality. i do not arrive at initializing the kedro- project, since the cli commands are not available.\r\n\r\n## context\r\n\r\nit is unclear to me if this is connected to #157 \r\ni wanted to start looking into kedro-, but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## steps to reproduce\r\n\r\n```\r\nconda create -n kedro_ python=3.8\r\nconda activate kedro_\r\npip install kedro-\r\nkedro  -h\r\nkedro new --starter=pandas-iris\r\ncd _test\/\r\nkedro  -h\r\n> error \"no such command ''\"\r\n```\r\n\r\n## expected result\r\n\r\n`kedro ` is available in a project directory, i.e. `kedro  -h` gives the same output inside the folder as before\r\n\r\n## actual result\r\n\r\ninside the project folder the `` command is unknown to kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: deprecationwarning: use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_\/lib\/python3.8\/site-packages\/\/types\/schema.py:49: deprecationwarning: `np.object` is a deprecated alias for the builtin `object`. to silence this warning, use `object` by itself. doing this will not modify any behavior and is safe. \r\ndeprecated in numpy 1.20; Content: for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"binarytype\", np.object)\r\n2021-04-23 17:49:52,197 - root - info - registered hooks from 2 installed plugin(s): kedro--0.7.1\r\nusage: kedro [options] command [args]...\r\ntry 'kedro -h' for help.\r\n\r\nerror: no such command ''.\r\n\r\n```\r\n\r\n## your environment\r\n\r\nubuntu 18.04.5\r\n\r\n- kedro 0.17.3\r\n- kedro- 0.7.1\r\n- python 3.8.8.\r\n-  1.15.0\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the kedro- CLI commands were unavailable inside a kedro project, despite the user having installed the necessary packages and following the minimal example from the documentation.",
        "Issue_preprocessed_content":"Title: kedro cli is unavailable inside a kedro project; Content: description i try to reproduce the minimal example from the docs a kedro project using the starter using the functinality. i do not arrive at initializing the kedro project, since the cli commands are not available. context it is unclear to me if this is connected to i wanted to start looking into kedro , but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. steps to reproduce expected result is available in a project directory, gives the same output inside the folder as before actual result inside the project folder the command is unknown to kedro your environment ubuntu kedro kedro python does the bug also happen with the last version on master? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Issue_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1617627646000,
        "Issue_closed_time":1618006798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro  ui does not use arguments from .yml; Content: ## description\r\n\r\nas described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in--yml-and-run-kedro--ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## context & steps to reproduce\r\n\r\n- create a kedro project\r\n- call `kedro  init`\r\n- modify the port in `.yml` to 5001\r\n- launch `kedro  ui`\r\n\r\n## expected result\r\n\r\nthe  ui should open in port 5001.\r\n\r\n## actual result\r\n\r\nit opens on port 5000 (the default).\r\n\r\n## your environment\r\n\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-` version: 0.6.0\r\n* python version used (`python -v`): 3.6.8\r\n* operating system and version: windows\r\n\r\n## does the bug also happen with the last version on master?\r\n\r\nyes\r\n\r\n## solution\r\n\r\nwe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_\/framework\/cli\/cli.py#l149-l151",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the `ui` command in Kedro does not use the options specified in the `.yml` file, despite the expected result being that it should open in the port specified in the `.yml` file.",
        "Issue_preprocessed_content":"Title: kedro ui does not use arguments from; Content: description as described in , the command does not use the options context & steps to reproduce create a kedro project call modify the port in to launch expected result the ui should open in port . actual result it opens on port . your environment include as many relevant details about the environment in which you experienced the bug version version python version used operating system and version windows does the bug also happen with the last version on master? yes solution we should pass the arguments in the command"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Issue_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1610404594000,
        "Issue_closed_time":1615716614000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: kedro  cli is broken if configuration is declared in pyproject.toml; Content: ## description\r\n\r\nkedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). we claim to support both, but the cli commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## steps to reproduce\r\n\r\ncall ``kedro  init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## expected result\r\n\r\nthe cli commands should be available (``init``)\r\n\r\n## actual result\r\nonly the ``new`` command is available. this is not considered as a kedro project.\r\n\r\n```\r\n-- separate them if you have more than one.\r\n```\r\n\r\n## your environment\r\n\r\n* `kedro` and `kedro-` version used (`pip show kedro` and `pip show kedro-`): kedro==16.6, kedro-==0.4.1\r\n* python version used (`python -v`): 3.7.9\r\n* operating system and version: windows 7\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes\r\n\r\n## solution\r\n\r\nthe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the kedro cli commands were not accessible if the project only contained a pyproject.toml file, and the solution was to update the is_kedro_project function to consider a folder as the root of a kedro project even if it does not contain a .kedro.yml file.",
        "Issue_preprocessed_content":"Title: kedro cli is broken if configuration is declared in; Content: description kedro enable to declare configuration either in or in . we claim to support both, but the cli commands are not accessible if the project contains only a . steps to reproduce call inside a project with no file but only a . expected result the cli commands should be available actual result only the command is available. this is not considered as a kedro project. your environment and version used python version used operating system and version windows does the bug also happen with the last version on develop? yes solution the error comes from the function which does not consider that a folder is the root of a kdro project if it does not contain a ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Issue_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605983313000,
        "Issue_closed_time":1606599848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: a kedropipelinemodel cannot be loaded from  if its catalog contains non deepcopy-able datasets; Content: ## description\r\n\r\ni tried to load a kedropipelinemodel from , and i got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## context\r\n\r\ni cannot load a previously saved kedropipelinemodel generated by pipeline_ml_factory.\r\n\r\n## steps to reproduce\r\n\r\nsave a kedropipelinemodel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## expected result\r\n\r\nthe model should be loaded\r\n\r\n## actual result\r\n\r\nan error is raised\r\n\r\n## your environment\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-` version used: 0.16.5 and 0.4.0\r\n* python version used (`python -v`): 3.6.8\r\n* windows 10 & centos were tested\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes\r\n\r\n# potential solution\r\n\r\nthe faulty line is:\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_\/\/kedro_pipeline_model.py#l45",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to load a previously saved KedroPipelineModel generated by pipeline_ml_factory due to the catalog containing an object which cannot be deepcopied.",
        "Issue_preprocessed_content":"Title: a kedropipelinemodel cannot be loaded from if its catalog contains non deepcopy able datasets; Content: description i tried to load a kedropipelinemodel from , and i got a cannot pickle context artifacts error, which is due do the context i cannot load a previously saved kedropipelinemodel generated by steps to reproduce save a kedropipelinemodel with a dataset that contains an object which cannot be deepcopied expected result the model should be loaded actual result an error is raised your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used windows & centos were tested does the bug also happen with the last version on develop? yes potential solution the faulty line is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Issue_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605982845000,
        "Issue_closed_time":1606515096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: runstatus of  run is \"finished\" instead of \"failed\" when the kedro run fails; Content: ## description\r\n\r\nwhen i launch `kedro run` and the run fails, the `on_pipeline_error` closes all the  runs (to avoid interactions with further runs)\r\n\r\n## context\r\n\r\ni cannot distinguish failed runs from sucessful ones in the  ui.\r\n\r\n## steps to reproduce\r\n\r\nlaunch a failing pipeline with kedro run.\r\n\r\n## expected result\r\n\r\nthe  ui should display the run with a red cross\r\n\r\n## actual result\r\n\r\nthe  ui displays the run with a green tick\r\n\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes.\r\n\r\n## potential solution: \r\n\r\nreplace these lines:\r\n\r\n`https:\/\/github.com\/galileo-galilei\/kedro-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_\/framework\/hooks\/pipeline_hook.py#l193-l194`\r\n\r\nwith \r\n\r\n```python\r\nwhile .active_run():\r\n    .end_run(.entities.runstatus.failed)\r\n```\r\nor even better, retrieve current run status from ?\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runstatus of a run is \"finished\" instead of \"failed\" when the kedro run fails, preventing them from distinguishing failed runs from successful ones in the UI.",
        "Issue_preprocessed_content":"Title: runstatus of run is finished instead of failed when the kedro run fails; Content: description when i launch and the run fails, the closes all the runs context i cannot distinguish failed runs from sucessful ones in the ui. steps to reproduce launch a failing pipeline with kedro run. expected result the ui should display the run with a red cross actual result the ui displays the run with a green tick does the bug also happen with the last version on develop? yes. potential solution replace these lines with or even better, retrieve current run status from ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Issue_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1604666166000,
        "Issue_closed_time":1605715301000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: typeerror: unsupported operand type(s) for \/: 'str' and 'str' when using artifactdataset with modelsaverdataset; Content: ## description\r\n\r\n`typeerror: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `artifactdataset` is used with `modelsaverdataset`.\r\n\r\n## context\r\n\r\nlogging locally and to  in one step.\r\n\r\n## steps to reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_.io.artifacts.artifactdataset\r\n    data_set:\r\n        type: kedro_.io.models.modelsaverdataset\r\n        flavor: .sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## expected result\r\n\r\nthe model should be saved locally and in  run at the same time.\r\n\r\n## actual result\r\n\r\n```\r\ntraceback (most recent call last):\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-\/kedro_\/io\/artifacts\/_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\ntypeerror: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nthe above exception was the direct cause of the following exception:\r\n\r\ntraceback (most recent call last):\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  file \"\/users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise dataseterror(message) from exc\r\nkedro.io.core.dataseterror: failed while saving data to data set modelsaverdataset(filepath=\/users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=.sklearn, load_args={}, save_args={}, version=version(load=none, save='2020-11-06t12.28.57.593z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## your environment\r\ninclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro- 0.4.0\r\n* python 3.7.7\r\n* macos catalina\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes.",
        "Issue_original_content_gpt_summary":"The user encountered a `typeerror: unsupported operand type(s) for \/: 'str' and 'str'` when using `artifactdataset` with `modelsaverdataset` in Kedro 0.16.6 and Kedro- 0.4.0 with Python 3.7.7 on macOS Catalina, which also happened with the last version on develop.",
        "Issue_preprocessed_content":"Title: typeerror unsupported operand type for 'str' and 'str' when using artifactdataset with modelsaverdataset; Content: description occurs when is used with . context logging locally and to in one step. steps to reproduce expected result the model should be saved locally and in run at the same time. actual result your environment include as many relevant details about the environment in which you experienced the bug kedro kedro python macos catalina does the bug also happen with the last version on develop? yes."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Issue_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1603488238000,
        "Issue_closed_time":1603665805000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: metricsdataset ignores run_id when prefix is not specified; Content: ## description\r\nwhen `metricsdataset` has no \"prefix\" specified, the name in the catalog is used instead. however, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## steps to reproduce\r\n\r\n1. create a  run interactively: \r\n```python\r\n.start_run()\r\n.end_run()\r\n```\r\nand browse the ui to retrieve the run_id\r\n\r\n2. declare a `metricsdataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_.io.metricsdataset\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## expected result\r\n\r\na metric should be loggedin run \"1346579\".\r\n\r\n## actual result\r\n\r\nthe metric is logged is a new run.\r\n\r\n## does the bug also happen with the last version on develop?\r\n\r\nyes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where metricsdataset ignores the specified run_id when no prefix is specified, resulting in the metric being logged in a new run.",
        "Issue_preprocessed_content":"Title: metricsdataset ignores when prefix is not specified; Content: description when has no prefix specified, the name in the catalog is used instead. however, when the is specified, it is overriden by the current run id when the prefix is automatically set. steps to reproduce . create a run interactively and browse the ui to retrieve the . declare a in the with no prefix and an existing . launch the pipeline which saves this catalog expected result a metric should be loggedin run . actual result the metric is logged is a new run. does the bug also happen with the last version on develop? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/96",
        "Issue_title":"Make mlflow init work when configuration is in pyproject.toml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1603011348000,
        "Issue_closed_time":1603658563000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nSince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. This breaks the `kedro mlflow init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## Context\r\nWe should remove the `_get_project_globals` util function in kedromlflow and use `kedro.framework.context import get_static_project_data` as suggested in #86. **Beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch `kedro mlflow init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## Expected Result\r\nThe mlflow.yml file should be created\r\n\r\n## Actual Result\r\nAn error is raised.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: make  init work when configuration is in pyproject.toml; Content: ## description\r\n\r\nsince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. this breaks the `kedro  init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## context\r\nwe should remove the `_get_project_globals` util function in kedro and use `kedro.framework.context import get_static_project_data` as suggested in #86. **beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## steps to reproduce\r\n\r\nlaunch `kedro  init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## expected result\r\nthe .yml file should be created\r\n\r\n## actual result\r\nan error is raised.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the 'kedro init' command was not compatible with the 'pyproject.toml' configuration file, resulting in an error being raised instead of the expected .yml file being created.",
        "Issue_preprocessed_content":"Title: make init work when configuration is in; Content: description since kedro project can instead of a at the root of the projects. this breaks the command which is only compatible with configuration file. context we should remove the util function in kedro and use as suggested in . beware this will break retrocompatibilty and work only with steps to reproduce launch with no config file in your project but a valid . expected result the file should be created actual result an error is raised."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Issue_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1601476316000,
        "Issue_closed_time":1602278580000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: dataset fails to log on remote storage when underlying dataset filepath is converted as a pureposixpath; Content: when i register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_.io.dataset \r\n  data_set : \r\n    type: pickle.pickledataset\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand i run `kedro run` i got a `expected string or bytes-like object` when **the local path is linux and the `_tracking_uri` is an azure blob storage (it works locally)**. i don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_\/io\/_dataset.py#l51\r\n\r\nhttps:\/\/github.com\/galileo-galilei\/kedro-\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_\/io\/_dataset.py#l55\r\n\r\n@kaemo @akruszewski did you experience some issues with s3 too?\r\n\r\n**edit**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Issue_original_content_gpt_summary":"The user encountered an issue when registering a dataset in the catalog.yml, where the local path was Linux and the _tracking_uri was an Azure Blob Storage, resulting in an \"expected string or bytes-like object\" error when running `kedro run`.",
        "Issue_preprocessed_content":"Title: dataset fails to log on remote storage when underlying dataset filepath is converted as a pureposixpath; Content: when i register a dataset in the and i run i got a when the local path is linux and the is an azure blob storage . i don't know really why this append, but it can be fied by replacing by in these locations did you experience some issues with s too? edit it is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Issue_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1601411953000,
        "Issue_closed_time":1602948810000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: .yml is not parsed properly when using templatedconfigloader; Content: when you have a global variable in the .yml file (e.g `mlruns: ${user}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a templatedconfigloader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.templatedconfigloader.html) in his project. this is due to `get__config()` to manually recreate the default configloader.\r\n\r\nthis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an issue with templatedconfigloader not properly parsing .yml files when a global variable is present, which is due to the manual recreation of the default configloader in `get__config()` and will be fixed by #66.",
        "Issue_preprocessed_content":"Title: is not parsed properly when using templatedconfigloader; Content: when you have a global variable in the file , the global variable is not replaced by its value even if the user has in his project. this is due to to manually recreate the default configloader. this is part of the numerous issues that will be fixed by ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Issue_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1595365457000,
        "Issue_closed_time":1602948810000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: get__config use the working directory instead of given path when called within load_context; Content: this may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Issue_original_content_gpt_summary":"The user may experience strange behaviour when calling the get_config function within the load_context in interactive mode from a different directory than the Kedro project root.",
        "Issue_preprocessed_content":"Title: use the working directory instead of given path when called within; Content: this may lead to strange behaviour when called in interactive mode in another place thant the kedro project root."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Issue_title":"Warning message appears when calling ``kedro mlflow init``",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1593379921000,
        "Issue_closed_time":1600718139000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: warning message appears when calling ``kedro  init``; Content: the warning claims that the project is not initialised yet, and that you must call ``kedro  init`` before calling any command while you are calling ``kedro  init``. it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command.",
        "Issue_original_content_gpt_summary":"The user encountered a warning message when calling ``kedro  init``, which can be safely ignored as the command works as intended, due to a bug caused by the dynamic creation of command.",
        "Issue_preprocessed_content":"Title: warning message appears when calling; Content: the warning claims that the project is not initialised yet, and that you must call before calling any command while you are calling . it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Issue_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1591562037000,
        "Issue_closed_time":1598336871000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: close  run when a pipeline fails in interactive mode; Content: # context\r\ntoday, you can execute a kedro pipeline interactively. the logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[pipeline_name],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# description\r\nif the execution fails for some reason (bug in the pipeline), the  run is not closed. this creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nthis bug does not occur when running from the command line since the  run is automatically closed when exiting.\r\n\r\n# possible implementation \r\nimplement a [``on_pipeline_error`` kedro ``hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the  run when the pipeline fails.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the run was not closed when a pipeline failed in interactive mode, resulting in unintended side effects and a messy Mlflow database.",
        "Issue_preprocessed_content":"Title: close run when a pipeline fails in interactive mode; Content: context today, you can execute a kedro pipeline interactively. the logic would be to load the context, and then to run the pipeline. description if the execution fails for some reason , the run is not closed. this creates unintended side effects for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy. this bug does not occur when running from the command line since the run is automatically closed when exiting. possible implementation implement a to close the run when the pipeline fails."
    },
    {
        "Issue_link":"https:\/\/github.com\/omegaml\/omegaml\/issues\/258",
        "Issue_title":"running mlflow>1.28 projects causes mlflow not found error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1663358103000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"*Currently*\n\n* since mlflow 1.28, running mlflow projects form remote sources causes\n  *mlflow: not found* issue on starting the project\n\n*Reproduce*\n\n* run TestMLFlowProjects.test_mlflow_gitproject_remote_https\n\n*Expected*\n\n* running remote-sourced mlflow project is supported as previously",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: running >1.28 projects causes  not found error; Content: *currently*\n\n* since  1.28, running  projects form remote sources causes\n  *: not found* issue on starting the project\n\n*reproduce*\n\n* run testprojects.test__gitproject_remote_https\n\n*expected*\n\n* running remote-sourced  project is supported as previously",
        "Issue_original_content_gpt_summary":"The user encountered an issue when running projects from remote sources since version 1.28, resulting in a \"not found\" error when starting the project.",
        "Issue_preprocessed_content":"Title: running projects causes not found error; Content: currently since running projects form remote sources causes not found issue on starting the project reproduce run expected running remote sourced project is supported as previously"
    },
    {
        "Issue_link":"https:\/\/github.com\/ugr-sail\/sinergym\/issues\/101",
        "Issue_title":"Experiments with mlflow don't work if MLFLOW_TRACKING_URI container variable specify an incorrect ip address",
        "Issue_label":[
            "bug",
            "Cloud"
        ],
        "Issue_creation_time":1639047613000,
        "Issue_closed_time":1639743100000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Execution get stuck if this case happens. It is necessary to manage this exception properly.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: experiments with  don't work if _tracking_uri container variable specify an incorrect ip address; Content: execution get stuck if this case happens. it is necessary to manage this exception properly.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where experiments with don't work if the _tracking_uri container variable specifies an incorrect IP address, causing execution to get stuck and requiring proper exception management.",
        "Issue_preprocessed_content":"Title: experiments with don't work if container variable specify an incorrect ip address; Content: execution get stuck if this case happens. it is necessary to manage this exception properly."
    },
    {
        "Issue_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Issue_title":"Error when starting new experiment in mlflow",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1608388229000,
        "Issue_closed_time":1610230183000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: error when starting new experiment in ; Content: error in pipeline in githubactions which causes test to not pass",
        "Issue_original_content_gpt_summary":"The user encountered an error in the pipeline in GitHub Actions which caused their tests to not pass when starting a new experiment.",
        "Issue_preprocessed_content":"Title: error when starting new experiment in; Content: error in pipeline in githubactions which causes test to not pass"
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Issue_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1643989114000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: plugin only compatible with kedro-<0.8.0; Content: ```python\r\ndef is__enabled() -> bool:\r\n    try:\r\n        import   # noqa\r\n        from kedro_.framework.context import get__config  # noqa\r\n        return true\r\n    except importerror:\r\n        return false\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where a plugin was only compatible with kedro versions lower than 0.8.0, and the `context` package had been moved or refactored, causing the plugin to always throw an exception.",
        "Issue_preprocessed_content":"Title: plugin only compatible with; Content: alway throws exception since package has been moved or refactored"
    },
    {
        "Issue_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Issue_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1580251523000,
        "Issue_closed_time":1580353817000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: experiment_gbdt raise errors with long parameters and ; Content:  raises error if length of key\/value exceeds 250. if the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\npossible option:\r\n- catch and ignore all errors from \r\n- truncate logging parameters automatically ",
        "Issue_original_content_gpt_summary":"The user encountered challenges when using the experiment_gbdt function, as it raised errors if the length of the key\/value exceeded 250, or if the length of the gbdt parameters or cat_columns was too long.",
        "Issue_preprocessed_content":"Title: raise errors with long parameters and; Content: raises error if length of exceeds . if the length of gbdt parameters or is long, will raise an exception. possible option catch and ignore all errors from truncate logging parameters automatically"
    },
    {
        "Issue_link":"https:\/\/github.com\/artefactory\/one-click-mlflow\/issues\/79",
        "Issue_title":"make one-click-mlflow not working after make destroy because of undeleted bucket",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1633706491000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nProblem encountered by @ucsky. Running ```make one-click-mlflow``` is not working after ```make destroy``` because of the artifacts' bucket which still exists.\r\nGot the following error:\r\n````\r\n\r\nSetting up your GCP project...\r\n\u2577\r\n\u2502 Error: googleapi: Error 409: You already own this bucket. Please select another name., conflict\r\n\u2502 \r\n\u2502   with module.bucket_backend.google_storage_bucket.this,\r\n\u2502   on ..\/modules\/mlflow\/artifacts\/main.tf line 18, in resource \"google_storage_bucket\" \"this\":\r\n\u2502   18: resource \"google_storage_bucket\" \"this\" {\r\n\r\n````\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. run ```make one-click-mlflow``` and finish it\r\n2. run ```make destroy```\r\n3. run ```make one-click-mlflow```\r\n4. See error\r\n\r\n**Expected behavior**\r\nThe second command ```make one-click-mlflow``` should work \r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: make one-click- not working after make destroy because of undeleted bucket; Content: **describe the bug**\r\nproblem encountered by @ucsky. running ```make one-click-``` is not working after ```make destroy``` because of the artifacts' bucket which still exists.\r\ngot the following error:\r\n````\r\n\r\nsetting up your gcp project...\r\n\u2577\r\n\u2502 error: googleapi: error 409: you already own this bucket. please select another name., conflict\r\n\u2502 \r\n\u2502   with module.bucket_backend.google_storage_bucket.this,\r\n\u2502   on ..\/modules\/\/artifacts\/main.tf line 18, in resource \"google_storage_bucket\" \"this\":\r\n\u2502   18: resource \"google_storage_bucket\" \"this\" {\r\n\r\n````\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. run ```make one-click-``` and finish it\r\n2. run ```make destroy```\r\n3. run ```make one-click-```\r\n4. see error\r\n\r\n**expected behavior**\r\nthe second command ```make one-click-``` should work \r\n\r\n",
        "Issue_original_content_gpt_summary":"The user @ucsky encountered a challenge where running \"make one-click-\" was not working after \"make destroy\" due to an undeleted bucket.",
        "Issue_preprocessed_content":"Title: make one click not working after make destroy because of undeleted bucket; Content: describe the bug problem encountered by running is not working after because of the artifacts' bucket which still exists. got the following error ` should work"
    },
    {
        "Issue_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Issue_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Issue_label":[
            "bug",
            "good first issue"
        ],
        "Issue_creation_time":1646132216000,
        "Issue_closed_time":1647350309000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  hardcoded bucket name - impossible to use  with aws s3; Content: https:\/\/github.com\/canonical\/-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/-server\/src\/charm.py#l20\r\n\r\nthe name of the bucket for  is hardcoded. this is a big issue because this makes using minio in gateway mode +  impossible on aws (s3 buckets are globally unique).\r\n\r\nit's a good first issue :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the bucket name was hardcoded, making it impossible to use with AWS S3, which requires globally unique bucket names.",
        "Issue_preprocessed_content":"Title: hardcoded bucket name impossible to use with aws s; Content: the name of the bucket for is hardcoded. this is a big issue because this makes using minio in gateway mode + impossible on aws . it's a good first issue"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Issue_title":"[mlflow] Migration Job should run before upgrade",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1660748480000,
        "Issue_closed_time":1660908206000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [] migration job should run before upgrade; Content: ### describe the bug a clear and concise description of what the bug is.\n\nwhen trying to install  chart i'm trying to migrate from old  version to the new one. i'm using `backendstore.databasemigration: true` value for that. but  pod failed to start with error:\r\n```\r\n.exceptions.exception: detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). take a backup of your database, then run ' db upgrade <database_uri>' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nfrom the looks of things migration job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but i can be wrong here. \r\n\r\nrunning job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nthanks!\n\n### what's your helm version?\n\nv3.9.3\n\n### what's your kubectl version?\n\nv1.24.3\n\n### which chart?\n\n\n\n### what's the chart version?\n\n0.6.0\n\n### what happened?\n\n_no response_\n\n### what you expected to happen?\n\ndb migration job should run before  pod upgrade. \n\n### how to reproduce it?\n\n1. install  with old db schema (1.23.1)\r\n2. try to upgrade with 0.6.0 helm chart\n\n### enter the changed values of values.yaml?\n\n```\r\n:\r\n  nodeselector:\r\n    redacted: shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactroot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsaccesskeyid: \"\"\r\n      awssecretaccesskey: \"\"\r\n  \r\n  extraenvvars:\r\n    aws_default_region: eu-central-1\r\n    _s3_endpoint_url: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendstore:\r\n    databasemigration: true\r\n    databaseconnectioncheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### anything else we need to know?\n\nchart was installed as a part of another umbrella chart",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to migrate from an old version to a new one using the `backendstore.databasemigration: true` value, where the pod failed to start due to an out-of-date database schema, and the migration job should have had pre-install and pre-upgrade hooks instead of post-install and post-upgrade.",
        "Issue_preprocessed_content":"Title: migration job should run before upgrade; Content: describe the bug a clear and concise description of what the bug is. when trying to install chart i'm trying to migrate from old version to the new one. i'm using value for that. but pod failed to start with error from the looks of things migration job should have hooks instead of but i can be wrong here. running job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release. thanks! what's your helm version? what's your kubectl version? which chart? what's the chart version? what happened? what you expected to happen? db migration job should run before pod upgrade. how to reproduce it? . install with old db schema . try to upgrade with helm chart enter the changed values of enter the command that you execute and helm upgrade install values wait create namespace atomic timeout m s f shared services anything else we need to know? chart was installed as a part of another umbrella chart"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Issue_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1660152345000,
        "Issue_closed_time":1660345866000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [] model artifacts not saved in remote s3 artifact store; Content: ### describe the bug a clear and concise description of what the bug is.\r\n\r\ni have local minikube cluster. i installed the helm chart with some changed settings. see below for the changed values. everthing else is same as per default values yaml file. for db backend i am using `bitnami\/postgresql` and for s3 storage minio instance. i also have created a initial bucket named \"\" in minio. \r\n\r\nand then i created a simple k8s pod to run the simple training example from  docs. this pod has env variables set as : `_tracking_uri=http:\/\/.airflow.svc.cluster.local:5000` [here ](https:\/\/raw.githubusercontent.com\/\/\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. i can see the metadata about the model in ui however , artifact section in ui is empty and also the bucket is empty. \r\n\r\n### what's your helm version?\r\n\r\nversion.buildinfo{version:\"v3.9.0\", gitcommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", gittreestate:\"clean\", goversion:\"go1.17.5\"}\r\n\r\n### what's your kubectl version?\r\n\r\nserver version: version.info{major:\"1\", minor:\"23\", gitversion:\"v1.23.3\", gitcommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", gittreestate:\"clean\", builddate:\"2022-01-25t21:19:12z\", goversion:\"go1.17.6\", compiler:\"gc\", platform:\"linux\/amd64\"}\r\n\r\n### which chart?\r\n\r\n\r\n\r\n### what's the chart version?\r\n\r\nlatest\r\n\r\n### what happened?\r\n\r\n_no response_\r\n\r\n### what you expected to happen?\r\n\r\ni would expect the artifacts in minio bucket.\r\n\r\n### how to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. run a simple exmple frpom docs. \r\n\r\n### enter the changed values of values.yaml?\r\n\r\n```\r\nbackendstore:\r\n    databasemigration: true\r\n    databaseconnectioncheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: -postgres-postgresql.airflow.svc.cluster.local\r\n      database: _db\r\n      user: \r\n      password: \r\nartifactroot:\r\n  proxiedartifactstorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: \r\n    awsaccesskeyid: {{ requiredenv \"minio_username\" }}\r\n    awssecretaccesskey: {{ requiredenv \"minio_password\" }}\r\nextraenvvars:\r\n  _s3_endpoint_url: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install -release community-charts\/ --values values.yaml\r\n\r\n### anything else we need to know?\r\n\r\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered challenges with their local minikube cluster when attempting to install a helm chart with changed settings, resulting in model artifacts not being saved in the remote s3 artifact store.",
        "Issue_preprocessed_content":"Title: model artifacts not saved in remote s artifact store; Content: describe the bug a clear and concise description of what the bug is. i have local minikube cluster. i installed the helm chart with some changed settings. see below for the changed values. everthing else is same as per default values yaml file. for db backend i am using and for s storage minio instance. i also have created a initial bucket named in minio. and then i created a simple k s pod to run the simple training example from docs. this pod has env variables set as is the link to that code. i can see the metadata about the model in ui however , artifact section in ui is empty and also the bucket is empty. what's your helm version? gitcommit ceeda c a a d cd f d b a , gittreestate clean , what's your kubectl version? server version minor , gitcommit c ab cff a c eccca f e e d , gittreestate clean , builddate t z , compiler gc , which chart? what's the chart version? latest what happened? what you expected to happen? i would expect the artifacts in minio bucket. how to reproduce it? install the helm chart with minio and postgresql config. run a simple exmple frpom docs. enter the changed values of enter the command that you execute and helm install release values anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Issue_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1658844949000,
        "Issue_closed_time":1658854769000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [] use port name instead of port number in servicemonitor ; Content: ### describe the bug a clear and concise description of what the bug is.\n\nfirst of all, thanks to everyone creating this helm chart as it is really good and easy to use.\r\n\r\nhowever, i encountered a problem when choosing to include servicemonitor and prometheus metrics along the deployment. generally, the created servicemonitor for  is correct, yet in the current form it does not work for me.\r\ni use the latest prometheus deployed using the official helm chart and the  metrics did not show up in the targets, yet it was visible in service discovery panel in prometheus dashboard, but appeared as `0\/1 active targets`.\r\n\r\nafter a couple of hours of educated debugging i changed manually the `targetport: 80` to `port: http` in the deployed servicemonitor manifest. it worked straightaway! \r\n\r\n\r\nwhat i propose is a simple fix:\r\naccording to official prometheus troubleshooting docs the port specified in servicemonitor should use `name` instead of port number ([link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nsimple fix would be to change `targetport: 80` to `port: http` in `templates\/servicemonitor.yaml`. port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\ni am aware that port number of type integer should also work...\r\n\n\n### what's your helm version?\n\n3.6.0\n\n### what's your kubectl version?\n\n1.19\n\n### which chart?\n\n\n\n### what's the chart version?\n\n0.2.21\n\n### what happened?\n\n_no response_\n\n### what you expected to happen?\n\n_no response_\n\n### how to reproduce it?\n\n_no response_\n\n### enter the changed values of values.yaml?\n\n_no response_\n\n### enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace  -tracking-server community-charts\/ --set servicemonitor.enabled=true\n\n### anything else we need to know?\n\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using the servicemonitor and prometheus metrics along with the helm chart, where the port specified in the servicemonitor was not working correctly, and a fix was proposed to use the port name instead of port number.",
        "Issue_preprocessed_content":"Title: use port name instead of port number in servicemonitor; Content: describe the bug a clear and concise description of what the bug is. first of all, thanks to everyone creating this helm chart as it is really good and easy to use. however, i encountered a problem when choosing to include servicemonitor and prometheus metrics along the deployment. generally, the created servicemonitor for is correct, yet in the current form it does not work for me. i use the latest prometheus deployed using the official helm chart and the metrics did not show up in the targets, yet it was visible in service discovery panel in prometheus dashboard, but appeared as . after a couple of hours of educated debugging i changed manually the to in the deployed servicemonitor manifest. it worked straightaway! what i propose is a simple fix according to official prometheus troubleshooting docs the port specified in servicemonitor should use instead of port number simple fix would be to change to in . port name is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name. i am aware that port number of type integer should also what's your helm version? what's your kubectl version? . which chart? what's the chart version? what happened? what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and helm install namespace tracking server set anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Issue_title":"[mlflow] Extra args broken",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657550069000,
        "Issue_closed_time":1657616964000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [] extra args broken; Content: ### describe the bug a clear and concise description of what the bug is.\n\nthe new staticprefix argument being under extraargs breaks the chart for users that need to use the extraargs\n\n### what's your helm version?\n\nversion.buildinfo{version:\"v3.8.1\", gitcommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", gittreestate:\"clean\", goversion:\"go1.17.8\"}\n\n### what's your kubectl version?\n\nclient version: version.info{major:\"1\", minor:\"22\", gitversion:\"v1.22.5\", gitcommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", gittreestate:\"clean\", builddate:\"2021-12-16t08:38:33z\", goversion:\"go1.16.12\", compiler:\"gc\", platform:\"darwin\/arm64\"} server version: version.info{major:\"1\", minor:\"23\", gitversion:\"v1.23.3\", gitcommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", gittreestate:\"clean\", builddate:\"2022-01-25t21:19:12z\", goversion:\"go1.17.6\", compiler:\"gc\", platform:\"linux\/arm64\"}\n\n### which chart?\n\n\n\n### what's the chart version?\n\n0.2.7\n\n### what happened?\n\nthe newly added staticprefix parameter under extraargs breaks the chart when used because it tries to add an extra argument to the  server command that doesnt exist.\n\n### what you expected to happen?\n\n_no response_\n\n### how to reproduce it?\n\n_no response_\n\n### enter the changed values of values.yaml?\n\n_no response_\n\n### enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f \/values.yaml  .\/\/\n\n### anything else we need to know?\n\ni am just creating a pull request to address this in a bit different way and havent tested it yet. just wanted to create a request to highlight a solution.\r\n\r\nyou could also handle the staticprefix as a separate argument in the extraenv when starting up the  server to make this work smoother for a final user, but this solution should work as well.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the newly added staticprefix parameter under extraargs breaks the chart when used, causing the chart to fail when executing the helm install command.",
        "Issue_preprocessed_content":"Title: extra args broken; Content: describe the bug a clear and concise description of what the bug is. the new staticprefix argument being under extraargs breaks the chart for users that need to use the extraargs what's your helm version? gitcommit cb af b b d d a a df ac dd ad , gittreestate clean , what's your kubectl version? client version minor , gitcommit c e ac ff a c d ca e bc a e f e , gittreestate clean , builddate t z , compiler gc , server version minor , gitcommit c ab cff a c eccca f e e d , gittreestate clean , builddate t z , compiler gc , which chart? what's the chart version? what happened? the newly added staticprefix parameter under extraargs breaks the chart when used because it tries to add an extra argument to the server command that doesnt exist. what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and helm install f anything else we need to know? i am just creating a pull request to address this in a bit different way and havent tested it yet. just wanted to create a request to highlight a solution. you could also handle the staticprefix as a separate argument in the extraenv when starting up the server to make this work smoother for a final user, but this solution should work as well."
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Issue_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1656578904000,
        "Issue_closed_time":1656583953000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [] run chart-testing (lint) step returns error validating maintainer 404 not found error; Content: ### describe the bug a clear and concise description of what the bug is.\n\nwhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#l60) file getting the following error.\r\n\r\n```\r\nerror: error linting charts: error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e  => (version: \"0.1.47\", path: \"charts\/\") > error validating maintainer 'burak ince': 404 not found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nbecause of maintainer name for the `ct lint` command must be a github username rather than a real name.\n\n### what's your helm version?\n\nv3.9.0\n\n### what's your kubectl version?\n\nv1.24.2\n\n### which chart?\n\n\n\n### what's the chart version?\n\n0.1.47\n\n### what happened?\n\n_no response_\n\n### what you expected to happen?\n\n_no response_\n\n### how to reproduce it?\n\n_no response_\n\n### enter the changed values of values.yaml?\n\n_no response_\n\n### enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### anything else we need to know?\n\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the chart-testing (lint) step in the release.yaml file, resulting in an error validating the maintainer 'burak ince' with a 404 not found error.",
        "Issue_preprocessed_content":"Title: run chart testing step returns error validating maintainer not found error; Content: describe the bug a clear and concise description of what the bug is. when we open a pull request, chart testing step in file getting the following error. because of maintainer name for the command must be a github username rather than a real name. what's your helm version? what's your kubectl version? which chart? what's the chart version? what happened? what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and ct lint debug config lint conf anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Issue_title":"Improve mlflow logging for population",
        "Issue_label":[
            "bug",
            "enhancement"
        ],
        "Issue_creation_time":1601310463000,
        "Issue_closed_time":1601714116000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: improve  logging for population; Content: separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on ",
        "Issue_original_content_gpt_summary":"The user encountered the challenge of improving logging for population by separating each individual's performance into its own graph, which was solved by appending the population index to each graph.",
        "Issue_preprocessed_content":"Title: improve logging for population; Content: separate each individuals performance into its own graph. graphs for each individual sub runs on"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Issue_title":"MLflow fails to log to a tracking server",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657876344000,
        "Issue_closed_time":1662562977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  fails to log to a tracking server; Content: ### system info\r\n\r\npython 3.9.13 | packaged by conda-forge | (main, may 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(.__version__)\r\n1.27.0\r\n\r\n### who can help?\r\n\r\n_no response_\r\n\r\n### information\r\n\r\n- [ ] the official example scripts\r\n- [x] my own modified scripts\r\n\r\n### tasks\r\n\r\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\r\n- [ ] my own task or dataset (give details below)\r\n\r\n### reproduction\r\n\r\n1. install \r\n2. configure a vanilla training job to use a tracking server (os.environ[\"_tracking_uri\"]=\"...\")\r\n3. run the job\r\n\r\nyou should see an error similar to:\r\n\r\n```\r\ntraceback (most recent call last):\r\n  file \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._max_params_tags_per_batch]))\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/fluent.py\", line 675, in log_params\r\n    client().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(logbatch, req_body)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise restexception(json.loads(response.text))\r\n.exceptions.restexception: invalid_parameter_value: invalid value [{'key': 'logging_nan_inf_filter', 'value': 'true'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'none'}, {'key': 'save_on_each_node', 'value': 'false'}, {'key': 'no_cuda', 'value': 'false'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'none'}, {'key': 'jit_mode_eval', 'value': 'false'}, {'key': 'use_ipex', 'value': 'false'}, {'key': 'bf16', 'value': 'false'}, {'key': 'fp16', 'value': 'false'}, {'key': 'fp16_opt_level', 'value': 'o1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'false'}, {'key': 'fp16_full_eval', 'value': 'false'}, {'key': 'tf32', 'value': 'none'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'none'}, {'key': 'tpu_num_cores', 'value': 'none'}, {'key': 'tpu_metrics_debug', 'value': 'false'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'false'}, {'key': 'eval_steps', 'value': 'none'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'false'}, {'key': 'remove_unused_columns', 'value': 'true'}, {'key': 'label_names', 'value': 'none'}, {'key': 'load_best_model_at_end', 'value': 'false'}, {'key': 'metric_for_best_model', 'value': 'none'}, {'key': 'greater_is_better', 'value': 'none'}, {'key': 'ignore_data_skip', 'value': 'false'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'none'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'false'}, {'key': 'group_by_length', 'value': 'false'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'none'}, {'key': 'ddp_bucket_cap_mb', 'value': 'none'}, {'key': 'dataloader_pin_memory', 'value': 'true'}, {'key': 'skip_memory_metrics', 'value': 'true'}, {'key': 'use_legacy_prediction_loop', 'value': 'false'}, {'key': 'push_to_hub', 'value': 'false'}, {'key': 'resume_from_checkpoint', 'value': 'none'}, {'key': 'hub_model_id', 'value': 'none'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<hub_token>'}, {'key': 'hub_private_repo', 'value': 'false'}, {'key': 'gradient_checkpointing', 'value': 'false'}, {'key': 'include_inputs_for_metrics', 'value': 'false'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'none'}, {'key': 'push_to_hub_organization', 'value': 'none'}, {'key': 'push_to_hub_token', 'value': '<push_to_hub_token>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'false'}, {'key': 'full_determinism', 'value': 'false'}, {'key': 'torchdynamo', 'value': 'none'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. hint: value was of type 'list'. see the api docs for more information about request parameters.\r\n```\r\n\r\ntraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import autotokenizer, trainer, trainingarguments, automodelforsequenceclassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = autotokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=true)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=true)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=true)\r\n\r\nmodel = automodelforsequenceclassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"hf__log_artifacts\"]=\"1\"\r\nos.environ[\"_experiment_name\"]=\"trainer--demo\"\r\nos.environ[\"_flatten_params\"]=\"1\"\r\n#os.environ[\"_tracking_uri\"]=<my_server ip>\r\n\r\ntraining_args = trainingarguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### expected behavior\r\n\r\ni would expect logging to work :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they were unable to log to a tracking server while running a training job with Transformers.",
        "Issue_preprocessed_content":"Title: fails to log to a tracking server; Content: system python packaged by conda forge who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . install . configure a vanilla training job to use a tracking server . run the job you should see an error similar to training script expected behavior i would expect logging to work"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Issue_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1651602226000,
        "Issue_closed_time":1651758596000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: incorrect check for  active run in callback; Content: ### system info\r\n\r\n```shell\r\n- ==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- platform: linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- python version: 3.9.7\r\n- huggingface_hub version: 0.2.1\r\n- pytorch version (gpu?): 1.10.2 (false)\r\n```\r\n\r\n\r\n### who can help?\r\n\r\nshould be fixed by #17067\r\n\r\n### information\r\n\r\n- [x] the official example scripts\r\n- [ ] my own modified scripts\r\n\r\n### tasks\r\n\r\n- [x] an officially supported task in the `examples` folder (such as glue\/squad, ...)\r\n- [ ] my own task or dataset (give details below)\r\n\r\n### reproduction\r\n\r\nsteps to reproduce:\r\n1. follow training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. change the training arguments to use `trainingarguments(output_dir=\"test_trainer\", report_to=[''], run_name=\"run0\")`\r\n3. on `trainer.train()` the  ui should report a run with a run name of `run0` which is not currently the case.\r\n\r\ncause of the issue:\r\n```\r\n>> import \r\n>> print(.active_run is none, .active_run() is none)\r\nfalse true\r\n```\r\n\r\nin `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is none:` need to be replaced by `if self._ml_flow.active_run() is none:`\r\n\r\n### expected behavior\r\n\r\npr #14894 introduce support for run_name in the callback. though, this does not work as expected since the active run is checked using a method reference that always returns true. bug introduced by #16131.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the incorrect check for an active run in the callback caused the run name to not be reported correctly.",
        "Issue_preprocessed_content":"Title: incorrect check for active run in callback; Content: system who can help? should be fixed by the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction steps to reproduce . follow training tutorial as per . change the training arguments to use . on the ui should report a run with a run name of which is not currently the case. cause of the issue in the line need to be replaced by expected behavior pr introduce support for in the callback. though, this does not work as expected since the active run is checked using a method reference that always returns true. bug introduced by ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Issue_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Issue_label":[
            "bug",
            "help wanted",
            "won't fix",
            "logger: mlflow"
        ],
        "Issue_creation_time":1636290176000,
        "Issue_closed_time":1640642583000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: `logger` does not update its status when `trainer.fit` failed; Content: ## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\nwhen an error is raised during training with `logger`, status of a `.entities.run_info.runinfo` object should be updated to be 'failed', while it remains 'running'.\r\ndue to the problem, when you look at  tracking server screen, it seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### to reproduce\r\n\r\n<!--\r\nplease reproduce using the boringmodel!\r\n\r\nyou can use the following colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1hvwvvtk8j2nj52qu4q4ycyzom0_alqf3?usp=sharing\r\nimportant: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nif you could not reproduce using the boringmodel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import dataloader, dataset\r\n\r\nfrom pytorch_lightning import lightningmodule, trainer\r\nfrom pytorch_lightning.loggers import logger ##### added #####\r\n\r\n\r\nclass randomdataset(dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass boringmodel(lightningmodule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.sgd(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = dataloader(randomdataset(32, 64), batch_size=2)\r\n    val_data = dataloader(randomdataset(32, 64), batch_size=2)\r\n    test_data = dataloader(randomdataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = logger() ##### added #####\r\n\r\n    model = boringmodel()\r\n    trainer = trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=false,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # this should be 'failed'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### expected behavior\r\n\r\n<!-- fill in -->\r\nstatus of each 's run is correctly updated when `pl.trainer.fit` failed.\r\n\r\n### environment\r\n\r\n<!--\r\nplease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(for security purposes, please check the contents of the script before running it)\r\n\r\nyou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nyou can also fill out the list below manually.\r\n-->\r\n\r\n- pytorch lightning version: 1.4.9\r\n-  version: 1.12.0\r\n\r\n### additional context\r\n\r\n<!-- add any other context about the problem here. -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the `logger` does not update its status when `trainer.fit` failed, resulting in the tracking server screen appearing as if training is still in progress even though it has been terminated with an error.",
        "Issue_preprocessed_content":"Title: does not update its status when failed; Content: bug when an error is raised during training with , status of a object should be updated to be 'failed', while it remains 'running'. due to the problem, when you look at tracking server screen, it seams as if training is still in progress even though it has been terminated with an error. to reproduce expected behavior status of each 's run is correctly updated when failed. environment pytorch lightning version version additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Issue_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Issue_label":[
            "bug",
            "help wanted",
            "won't fix"
        ],
        "Issue_creation_time":1631568762000,
        "Issue_closed_time":1635553585000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: inconsistency in logger.log_metrics within steps; Content: ## \ud83d\udc1b inconsistency in logger.log_metrics within steps\r\n\r\nthe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers..html) for logger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=none)`\r\n\r\nwhere **metrics** (dict[str, float]) \u2013 dictionary with metric names as keys and measured quantities as values and \r\n**step** (optional[int]) \u2013 step number at which the metrics should be recorded.\r\n\r\nwhen within a training\/validation\/test _step method of a lightningmodule:\r\n- setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `attributeerror: 'client' object has no attribute 'log_metrics'`\r\n- setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `typeerror: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `typeerror: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nfound the behavior from the last two options by luck because of a typo. the logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. even if i use `log_metric` the method expects parameters other than the dict[str, float] stated in the documentation.\r\n\r\n### to reproduce\r\n\r\nthis is the minimum code i found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning__bug\/blob\/main\/pytorch_lightning__bug.py\r\n\r\n### expected behavior\r\n\r\nthe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### environment\r\n\r\n* cuda:\r\n\t- gpu:\r\n\t- available:         false\r\n\t- version:           none\r\n* packages:\r\n\t- numpy:             1.21.2\r\n\t- pytorch_debug:     false\r\n\t- pytorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* system:\r\n\t- os:                linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- elf\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-ubuntu smp sat may 8 02:33:43 utc 2021\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with inconsistency in the logger.log_metrics within steps, where the documentation states one signature but the code requires different parameters.",
        "Issue_preprocessed_content":"Title: inconsistency in within steps; Content: inconsistency in within steps the for logger states that it has a method which signature is as follows where metrics dictionary with metric names as keys and measured quantities as values and step step number at which the metrics should be recorded. when within a method of a lightningmodule setting results in the fit method raising setting results in the fit method raising setting results in the fit method raising found the behavior from the last two options by luck because of a typo. the logger would expect despite the documentation saying the method is called . even if i use the method expects parameters other than the dict stated in the documentation. to reproduce this is the minimum code i found that reproduces the bug expected behavior the code should work with the signature from the documentation. environment cuda gpu available false version none packages numpy false pytorch lightning tqdm system os linux architecture bit elf processor python version ubuntu smp sat may utc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Issue_title":"mlflow run_name unexpected argument error",
        "Issue_label":[
            "bug",
            "help wanted",
            "working as intended"
        ],
        "Issue_creation_time":1626357881000,
        "Issue_closed_time":1626409391000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  run_name unexpected argument error; Content: ## \ud83d\udc1b bug\r\nthe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers..html?highlight=logger#-logger) mentions there is an argument called run_name for the  logger, where the run_name of a given experiment can be provided. although,run_name is an unknown argument to the  logger\r\n\r\n`typeerror: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## please reproduce using the boringmodel\r\ncolab link:  https:\/\/colab.research.google.com\/drive\/1thcminx6tqdonkxk2ir8hooux1uropix?usp=sharing\r\n\r\n### to reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import logger\r\nimport \r\n\r\nmlf_logger = logger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### environment\r\ncolab - https:\/\/colab.research.google.com\/drive\/1thcminx6tqdonkxk2ir8hooux1uropix?usp=sharing\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected argument error when attempting to use the run_name argument for the logger in PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: unexpected argument error; Content: bug the mentions there is an argument called for the logger, where the of a given experiment can be provided. is an unknown argument to the logger please reproduce using the boringmodel colab link to reproduce environment colab"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Issue_title":"mlflow run context is not logged when using MLFlowLogger",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1617115654000,
        "Issue_closed_time":1617875123000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  run context is not logged when using logger; Content: ## \ud83d\udc1b bug\r\nwhen we use the basic  logging via `with .start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the tracking ui ([system tags](https:\/\/.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nbut when we use `logger` as a logger in pytorch_lightning, this info is not logged. as a user, i'd like to have a mirrored functionality out-of-the-box.\r\n\r\ni inspected the `start_run()` method of  and deduced that the only thing is left while creating the run via client is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/.py\r\nfrom .tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> client:\r\n        if self._run_id is none:\r\n            run = self.__client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\ni think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default api, secondly - it's the pytorch_lightning that manages the 's run anyways.\r\n\r\n**pr is following ...**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the run context is not logged when using the logger in PyTorch Lightning, and proposed a solution to add the tags internally to ensure the run context is logged.",
        "Issue_preprocessed_content":"Title: run context is not logged when using logger; Content: bug when we use the basic logging via context manager, we get a better supplementary about the run rendered in the tracking ui but when we use as a logger in this is not logged. as a user, i'd like to have a mirrored functionality out of the box. i inspected the method of and deduced that the only thing is left while creating the run via client is to add from the package i think it's a better idea to add those tags internally as first it's as seamless as in the default api, secondly it's the that manages the 's run anyways. pr is following"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Issue_title":"External MLFlow logging failures cause training job to fail",
        "Issue_label":[
            "bug",
            "help wanted",
            "won't fix",
            "logger",
            "3rd party"
        ],
        "Issue_creation_time":1616445327000,
        "Issue_closed_time":1619815518000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: external  logging failures cause training job to fail; Content: ## \ud83d\udc1b bug\r\n\r\ni am using a `pytorch_lightning.loggers..logger` during training, with the  tracking uri hosted in databricks. when databricks updates, we sometimes lose access to  for a brief period. when this happens, logging to  fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.maxretryerror: httpsconnectionpool(host=xxx.cloud.databricks.com, port=443): max retries exceeded with url: \/api\/2.0\/\/runs\/get?xxx (caused by newconnectionerror(<urllib3.connection.httpsconnection object at 0x7fbbd6096f50>: failed to establish a new connection: [errno 111] connection refused))\r\n```\r\n\r\nnot only does logging fail, but with pytorch lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nideally, there would be flexibility in pytorch lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## please reproduce using the boringmodel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17tqdkz8sjcdpicwc76n5uqc5ikignp7g?usp=sharing \r\n\r\n### to reproduce\r\n\r\nattempt to use a logger that fails to log. the training job will fail, losing all progress. \r\n\r\n### expected behavior\r\n\r\nthere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### environment\r\n\r\n* cuda:\r\n\t- gpu:\r\n\t\t- tesla t4\r\n\t- available:         true\r\n\t- version:           10.1\r\n* packages:\r\n\t- numpy:             1.19.5\r\n\t- pytorch_debug:     false\r\n\t- pytorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* system:\r\n\t- os:                linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 smp thu jul 23 08:00:38 pdt 2020\r\n\r\n### additional context\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where external logging failures caused a training job to fail, with limited error handling options available in PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: external logging failures cause training job to fail; Content: bug i am using a during training, with the tracking uri hosted in databricks. when databricks updates, we sometimes lose access to for a brief period. when this happens, logging to fails with the following error not only does logging fail, but with pytorch lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long running job with limited error handling options currently available. ideally, there would be flexibility in pytorch lightning to allow users to handle logging errors such that it will not always kill the training job. please reproduce using the boringmodel to reproduce attempt to use a logger that fails to log. the training job will fail, losing all progress. expected behavior there is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. environment cuda gpu tesla t available true version packages numpy false pytorch lightning tqdm system os linux architecture bit processor python version smp thu jul pdt additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6205",
        "Issue_title":"MLFlow Logger Makes a New Run When Resuming from hpc Checkpoint",
        "Issue_label":[
            "bug",
            "help wanted",
            "checkpointing",
            "environment: slurm",
            "priority: 2",
            "logger: mlflow"
        ],
        "Issue_creation_time":1614282696000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nCurrently the `MLFlowLogger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. Runs are an MLFlow concept that groups things in their UI, so when resuming after requeue, it should really be reusing the run ID. I think this can be patched into the hpc checkpoint using the logger which I believe exposes the run ID. This can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). I'm happy to attempt to PR this if the owners agree that it's a bug.\r\n\r\n### To Reproduce\r\n\r\nUse `MLFlowLogger` on a slurm cluster and watch the mlflow UI when preemption happens, there will be a new run created.\r\n\r\n### Expected behavior\r\n\r\nRuns are grouped neatly on the MLFlow UI\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.20.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0\r\n        - tqdm:              4.57.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.1\r\n        - version:           #1 SMP Thu Jan 21 16:15:07 EST 2021\r\n\n\ncc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger makes a new run when resuming from hpc checkpoint; Content: ## \ud83d\udc1b bug\r\n\r\ncurrently the `logger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. runs are an  concept that groups things in their ui, so when resuming after requeue, it should really be reusing the run id. i think this can be patched into the hpc checkpoint using the logger which i believe exposes the run id. this can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). i'm happy to attempt to pr this if the owners agree that it's a bug.\r\n\r\n### to reproduce\r\n\r\nuse `logger` on a slurm cluster and watch the  ui when preemption happens, there will be a new run created.\r\n\r\n### expected behavior\r\n\r\nruns are grouped neatly on the  ui\r\n\r\n### environment\r\n\r\n* cuda:\r\n        - gpu:\r\n        - available:         false\r\n        - version:           10.2\r\n* packages:\r\n        - numpy:             1.20.1\r\n        - pytorch_debug:     false\r\n        - pytorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0\r\n        - tqdm:              4.57.0\r\n* system:\r\n        - os:                linux\r\n        - architecture:\r\n                - 64bit\r\n                - elf\r\n        - processor:         x86_64\r\n        - python:            3.8.1\r\n        - version:           #1 smp thu jan 21 16:15:07 est 2021\r\n\n\ncc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger creates a new run when resuming from an hpc checkpoint, resulting in runs being grouped incorrectly on the UI.",
        "Issue_preprocessed_content":"Title: logger makes a new run when resuming from hpc checkpoint; Content: bug currently the creates a new run when resuming from an hpc checkpoint, after preemption by slurm and requeuing. runs are an concept that groups things in their ui, so when resuming after requeue, it should really be reusing the run id. i think this can be patched into the hpc checkpoint using the logger which i believe exposes the run id. this can also be seen on the on the progress bar which changes after preemption . i'm happy to attempt to pr this if the owners agree that it's a bug. to reproduce use on a slurm cluster and watch the ui when preemption happens, there will be a new run created. expected behavior runs are grouped neatly on the ui environment cuda gpu available false version packages numpy false pytorch lightning tqdm system os linux architecture bit elf processor python version smp thu jan est cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Issue_title":"MlflowLogger fail when logging long parameters",
        "Issue_label":[
            "bug",
            "help wanted",
            "priority: 2"
        ],
        "Issue_creation_time":1612927107000,
        "Issue_closed_time":1613510526000,
        "Issue_upvote_count":4,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger fail when logging long parameters; Content: ## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n\r\n## please reproduce using the boringmodel\r\n\r\n\r\n<!-- please paste your boringmodel colab link here. -->\r\n\r\n### to reproduce\r\n\r\nlog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- if you could not reproduce using the boringmodel and still think there's a bug, please post here -->\r\n\r\n### expected behavior\r\n\r\n<!-- fill in -->\r\n\r\nlogger not sending parameters longer than 250 characters to  and log warning to user\r\n\r\n### environment\r\n\r\n\r\n - pytorch version (e.g., 1.0):\r\n - os (e.g., linux): \r\n - how you installed pytorch (`conda`, `pip`, source): pip\r\n - build command you used (if compiling from source):\r\n - python version: 3.7\r\n - cuda\/cudnn version:\r\n - gpu models and configuration:\r\n - any other relevant information:\r\n\r\n### additional context\r\n\r\n only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www..org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/\/\/issues\/1976\r\nhttps:\/\/github.com\/\/\/issues\/3931\r\n\r\n\r\n<!-- add any other context about the problem here. -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger fails when logging long parameters, with the expected behavior being that the logger should not send parameters longer than 250 characters and log a warning to the user.",
        "Issue_preprocessed_content":"Title: logger fail when logging long parameters; Content: bug please reproduce using the boringmodel to reproduce log anything parameters longer than characters expected behavior logger not sending parameters longer than characters to and log warning to user environment pytorch version os how you installed pytorch pip build command you used python version version gpu models and configuration any other relevant additional context only allow paramters to be at most bytes , their limit in database is characters"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4411",
        "Issue_title":"Using log_gpu_memory with MLFLow logger causes an exception.",
        "Issue_label":[
            "bug",
            "help wanted",
            "3rd party"
        ],
        "Issue_creation_time":1603900309000,
        "Issue_closed_time":1605009026000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nUsing log_gpu_memory with MLFLow logger causes an error. It appears the name of the metric is not supported by MLFLow.\r\n\r\n    MlflowException: Invalid metric name: 'gpu_id: 0\/memory.used (MB)'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/).\r\n\r\n### To Reproduce\r\nI reproduced the bug with the BoringModel, in the link bellow:\r\nhttps:\/\/colab.research.google.com\/drive\/1P8uhSfjvYhKPMyRZH-QmfbOUOfnePy6G?usp=sharing\r\n\r\n### Expected behavior\r\nlog_gpu_memory should log gpu memory correctly when using an MLFlow logger.\r\n\r\n### Environment\r\nColab environment:\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cu101\r\n\t- pytorch-lightning: 1.0.3\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: using log_gpu_memory with  logger causes an exception.; Content: ## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\nusing log_gpu_memory with  logger causes an error. it appears the name of the metric is not supported by .\r\n\r\n    exception: invalid metric name: 'gpu_id: 0\/memory.used (mb)'. names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/).\r\n\r\n### to reproduce\r\ni reproduced the bug with the boringmodel, in the link bellow:\r\nhttps:\/\/colab.research.google.com\/drive\/1p8uhsfjvyhkpmyrzh-qmfbouofnepy6g?usp=sharing\r\n\r\n### expected behavior\r\nlog_gpu_memory should log gpu memory correctly when using an  logger.\r\n\r\n### environment\r\ncolab environment:\r\n\r\n* cuda:\r\n\t- gpu:\r\n\t\t- tesla t4\r\n\t- available:         true\r\n\t- version:           10.1\r\n* packages:\r\n\t- numpy:             1.18.5\r\n\t- pytorch_debug:     false\r\n\t- pytorch_version:   1.6.0+cu101\r\n\t- pytorch-lightning: 1.0.3\r\n\t- tqdm:              4.41.1\r\n* system:\r\n\t- os:                linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #1 smp thu jul 23 08:00:38 pdt 2020\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to use log_gpu_memory with a logger, due to an invalid metric name.",
        "Issue_preprocessed_content":"Title: using with logger causes an exception.; Content: bug using with logger causes an error. it appears the name of the metric is not supported by . exception invalid metric name '. names may only contain alphanumerics, underscores , dashes , periods , spaces , and slashes . to reproduce i reproduced the bug with the boringmodel, in the link bellow expected behavior should log gpu memory correctly when using an logger. environment colab environment cuda gpu tesla t available true version packages numpy false pytorch lightning tqdm system os linux architecture bit processor python version smp thu jul pdt"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Issue_title":"mlflow logger complains about missing run_id",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1602116192000,
        "Issue_closed_time":1602141183000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger complains about missing run_id; Content: ## \ud83d\udc1b bug\r\nwhen using  logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\ntypeerror                                 traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # training_step + training_step_end\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is none:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\ntypeerror: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### expected behavior\r\nthe logger should behave the same as the  api where only key and value argment is needed for log_param() function\r\n\r\n#### code sample\r\n```python\r\nmlf_logger = logger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\ncllass vaeexperiment(lightningmodule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=false,\r\n                  gpus=1, \r\n                  auto_select_gpus=true,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger was complaining about a missing run_id when using the log_param() function in a LightningModule.",
        "Issue_preprocessed_content":"Title: logger complains about missing; Content: bug when using logger, function require expected behavior the logger should behave the same as the api where only key and value argment is needed for function code sample environment"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Issue_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Issue_label":[
            "bug",
            "help wanted",
            "logger"
        ],
        "Issue_creation_time":1599546516000,
        "Issue_closed_time":1599644307000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger slows training steps dramatically, despite only setting metrics to be logged on epoch; Content: ## \ud83d\udc1b bug\r\n\r\nwhen using the  logger, with a remote server, logging per step introduces latency which slows the training loop.\r\ni have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. i suspect the logger is still communicating with the  server on each training step.\r\n\r\n### to reproduce\r\n1. start an  server locally\r\n```\r\n ui\r\n```\r\n2. run the minimal code example below as is, (with  logger set to use the default file uri.)\r\n3. uncomment out the `tracking_uri` to use the local  server and run the code again. you will see a 2-3 times drop in the iterations per second.\r\n\r\n#### code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import tensordataset, dataloader\r\nimport pytorch_lightning as pl\r\n\r\nclass mymodel(pl.lightningmodule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.mseloss()\r\n        self.x = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.x.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = tensordataset(self.x[:-self.num_valid], self.x[:-self.num_valid])\r\n        dl = dataloader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = tensordataset(self.x[-self.num_valid:], self.x[-self.num_valid:])\r\n        dl = dataloader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.trainresult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=true, on_step=false)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.evalresult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=true, on_step=false)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import tensorboardlogger, logger\r\n    mlf_logger = logger(\r\n        experiment_name=f\"mymodel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=true,\r\n        logger=mlf_logger\r\n    )\r\n    model = mymodel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### expected behavior\r\n\r\nwhen using the trainresult and evalresult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. it should be possible to avoid the  logger from communicating with the server in each training loop. \r\nthis would make it feasible to implement the  when a remote server is used for experiment tracking.\r\n\r\n### environment\r\n```\r\n* cuda:\r\n\t- gpu:\r\n\t- available:         false\r\n\t- version:           none\r\n* packages:\r\n\t- numpy:             1.18.2\r\n\t- pytorch_debug:     false\r\n\t- pytorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* system:\r\n\t- os:                linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 smp tue may 26 11:42:35 utc 2020\r\n```\r\n### additional context\r\n\r\nwe host a  instance in aws and would like to be able to track experiments without affecting the training speed. \r\nit appears that in general the  logger is much less performant than the default tensorboard logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### solution\r\ni've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/.py#l125-l129\r\nhere `self.experiment` is called regardless of whether `self._run_id` exists. if we add an `if not self._run_id` here we avoid calling `self.__client.get_experiment_by_name(self._experiment_name)` on each step.\r\nhowever we still call it each time we log metrics to mflow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/.py#l100-l112\r\nhere if we store `expt` within the logger and only call `self.__client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the  logging appears to be working as expected.\r\n\r\ni'd be happy to raise a pr for this fix.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using the logger with a remote server to log metrics per step introduces latency which slows the training loop, despite configuring logging of metrics only per epoch.",
        "Issue_preprocessed_content":"Title: logger slows training steps dramatically, despite only setting metrics to be logged on epoch; Content: bug when using the logger, with a remote server, logging per step introduces latency which slows the training loop. i have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. i suspect the logger is still communicating with the server on each training step. to reproduce . start an server locally . run the minimal code example below as is, . uncomment out the to use the local server and run the code again. you will see a times drop in the iterations per second. code sample expected behavior when using the trainresult and evalresult, or manually handling metric logging using the and callbacks. it should be possible to avoid the logger from communicating with the server in each training loop. this would make it feasible to implement the when a remote server is used for experiment tracking. environment additional context we host a instance in aws and would like to be able to track experiments without affecting the training speed. it appears that in general the logger is much less performant than the default tensorboard logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop. solution i've done a bit of debugging in the codebase and have been able to isolate the cause in two places here is called regardless of whether exists. if we add an here we avoid calling on each step. however we still call it each time we log metrics to mflow, because of the property . here if we store within the logger and only call when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the logging appears to be working as expected. i'd be happy to raise a pr for this fix."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3392",
        "Issue_title":"mlflow training loss not reported until end of run",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1599521969000,
        "Issue_closed_time":1599634019000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"I think I'm logging correctly, this is my `training_step`\r\n\r\n        result = pl.TrainResult(loss)\r\n        result.log('loss\/train', loss)\r\n        return result\r\n\r\nand `validation_step`\r\n\r\n        result = pl.EvalResult(loss)\r\n        result.log('loss\/validation', loss)\r\n        return result\r\n\r\nThe validation loss is updated in mlflow each epoch, however the training loss isn't displayed until training has finished. Then it's available for every step. This may be a mlflow rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png)\r\n\r\nVersions:\r\n\r\npytorch-lightning==0.9.0\r\nmlflow==1.11.0\r\n\r\nEdit: logging TrainResult with on_epoch=True results in the metric appearing in mlflow during training, it's only the default train logging which gets delayed. i.e.\r\n\r\n        result.log('accuracy\/train', acc, on_epoch=True)\r\n\r\nis fine\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  training loss not reported until end of run; Content: i think i'm logging correctly, this is my `training_step`\r\n\r\n        result = pl.trainresult(loss)\r\n        result.log('loss\/train', loss)\r\n        return result\r\n\r\nand `validation_step`\r\n\r\n        result = pl.evalresult(loss)\r\n        result.log('loss\/validation', loss)\r\n        return result\r\n\r\nthe validation loss is updated in  each epoch, however the training loss isn't displayed until training has finished. then it's available for every step. this may be a  rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png)\r\n\r\nversions:\r\n\r\npytorch-lightning==0.9.0\r\n==1.11.0\r\n\r\nedit: logging trainresult with on_epoch=true results in the metric appearing in  during training, it's only the default train logging which gets delayed. i.e.\r\n\r\n        result.log('accuracy\/train', acc, on_epoch=true)\r\n\r\nis fine\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the training loss was not reported until the end of the run, despite being logged correctly, which may be a PyTorch-Lightning issue.",
        "Issue_preprocessed_content":"Title: training loss not reported until end of run; Content: i think i'm logging correctly, this is my result loss return result and result loss return result the validation loss is updated in each epoch, however the training loss isn't displayed until training has finished. then it's available for every step. this may be a rather than pytorch lighting issue somewhere along the line it seems to be buffered? versions edit logging trainresult with results in the metric appearing in during training, it's only the default train logging which gets delayed. acc, is fine"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Issue_title":"MLFlowLogger throws a JSONDecodeError",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1597814570000,
        "Issue_closed_time":1597848054000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logger throws a jsondecodeerror; Content: <!-- \r\n### common bugs:\r\n1. tensorboard not showing in jupyter-notebook see [issue 79](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/issues\/79).    \r\n2. pytorch 1.1.0 vs 1.2.0 support [see faq](https:\/\/github.com\/pytorchlightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b bug\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n\r\n### to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n\r\n#### code sample\r\n<!-- ideally attach a minimal code sample to reproduce the decried issue. \r\nminimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import trainer\r\nfrom pytorch_lightning.loggers import logger\r\n_logger = logger(experiment_name=\"test-experiment\", tracking_uri=\"uri_here\")\r\nt = trainer(logger=_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `jsondecodeerror` exception.\r\n```python\r\ntraceback (most recent call last):\r\n  file \"<stdin>\", line 1, in <module>\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or dummyexperiment()\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 98, in experiment\r\n    expt = self.__client.get_experiment_by_name(self._experiment_name)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(getexperimentbyname, req_body)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  file \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise jsondecodeerror(\"expecting value\", s, err.value) from none\r\njson.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)\r\n```\r\n### expected behavior\r\n\r\n<!-- a clear and concise description of what you expected to happen. -->\r\n\r\n### environment\r\nenvironment details\r\n```\r\n\r\n - pytorch version (e.g., 1.0): 1.6.0\r\n - pytorch lightning version: 0.9.0rc12\r\n - os (e.g., linux): linux\r\n - how you installed pytorch (`conda`, `pip`, source): conda\r\n - build command you used (if compiling from source):\r\n - python version: 3.7.7\r\n - cuda\/cudnn version: not relevant\r\n - gpu models and configuration: not relevant\r\n - any other relevant information: not relevant\r\n\r\n### additional context\r\n\r\n<!-- add any other context about the problem here. -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a JSONDecodeError when attempting to use the logger in PyTorch Lightning, which was caused by an issue with the tracking_uri.",
        "Issue_preprocessed_content":"Title: logger throws a jsondecodeerror; Content: bug to reproduce steps to reproduce the behavior code sample throws a exception. expected behavior environment environment details pytorch version pytorch lightning version os linux how you installed pytorch conda build command you used python version version not relevant gpu models and configuration not relevant any other relevant not relevant additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Issue_title":"mlflow checkpoints in the wrong location ",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1597273128000,
        "Issue_closed_time":1597488847000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  checkpoints in the wrong location ; Content: i'm not sure if i'm doing something wrong, i'm using  instead of tensorboard as a logger. i've used the defaults i.e.\r\n\r\n```\r\n = loggers.logger()\r\ntrainer = pl.trainer.from_argparse_args(args, logger=)\r\n```\r\n\r\ni'm ending up with the following folder structure\r\n\r\n\\\r\n\\\\1\r\n\\\\1\\\\{guid}\\artifacts\r\n\\\\1\\\\{guid}\\metrics\r\n\\\\1\\\\{guid}\\params\r\n\\\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\` folder. \r\n\r\nperhaps this is an  rather than pytorch-lightning issue? \r\n\r\ni'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with checkpoints being stored in the wrong location when using PyTorch-Lightning 0.8.5 on MacOS running in Python 3.7.6.",
        "Issue_preprocessed_content":"Title: checkpoints in the wrong location; Content: i'm not sure if i'm doing something wrong, i'm using instead of tensorboard as a logger. i've used the defaults i'm ending up with the following folder structure the checkpoints are in the wrong location, they should be in the folder. perhaps this is an rather than pytorch lightning issue? i'm using pytorch lightning on macos running in python"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Issue_title":"Hydra MLFlow Clash",
        "Issue_label":[
            "bug",
            "help wanted",
            "good first issue",
            "logger"
        ],
        "Issue_creation_time":1591172197000,
        "Issue_closed_time":1592925645000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: hydra  clash; Content: <!-- \r\n### common bugs:\r\n1. tensorboard not showing in jupyter-notebook see [issue 79](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/issues\/79).    \r\n2. pytorch 1.1.0 vs 1.2.0 support [see faq](https:\/\/github.com\/pytorchlightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b bug\r\n\r\nwhen using the  logger with hydra, because the parameters passed to the lightningmodule is a `dictconfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#l177\r\n\r\n### to reproduce\r\n\r\nuse hydra and  together. \r\n\r\n<!-- if you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\ntraceback (most recent call last):\r\n  file \"\/home\/siavash\/kronikare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=none,\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  file \"\/home\/siavash\/kronikare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/utils\/validation.py\", line 120, in _validate_param_name\r\n    invalid_parameter_value)\r\n.exceptions.exception: invalid parameter name: ''. names may be treated as files in certain cases, and must not resolve to other names when treated as such. this name would resolve to '.'\r\n```\r\n\r\n### expected behavior\r\n\r\ncheck whether the instance if `dict` or `dictconfig` in the given line. \r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges when using the logger with hydra, due to the parameters passed to the lightningmodule being a `dictconfig`, which caused the condition in the `logger\/base.py` to not be met.",
        "Issue_preprocessed_content":"Title: hydra clash; Content: bug when using the logger with hydra, because the parameters passed to the lightningmodule is a , the condition in the is not met. to reproduce use hydra and together. expected behavior check whether the instance if or in the given line."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Issue_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1576464430000,
        "Issue_closed_time":1583540837000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: pickle error from trainer.fit when using logger and distributed data parallel without slurm; Content: ## \ud83d\udc1b bug\r\n\r\ntrainer.fit fails with a pickle error when the logger is logger, and distributed_backend='ddp' on gpus but without slurm.\r\n\r\n### to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n\r\n1. instantiate logger in pytorch 0.5.3.2 with pytorch 1.3.1 and  1.4.0. the execution environment has environment variables _tracking_uri, and also _tracking_username and _tracking_password to connect to the  tracking server with http basic authentication. the  tracking server is also v1.4.0.\r\n2. instantiate trainer with logger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with nvidia gpus but without slurm.\r\n3. run trainer.fit\r\n\r\nfrom the error output, it looks like multiprocessing is attempting to pickle the nested function in  function [_get_rest_store](https:\/\/github.com\/\/\/blob\/v1.4.0\/\/tracking\/_tracking_service\/utils.py#l81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_.py\r\ntraceback (most recent call last):\r\n  file \"test_.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._popen(self)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _popen\r\n    return popen(process_obj)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    forkingpickler(file, protocol).dump(obj)\r\nattributeerror: can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### code sample\r\nsample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = namespace()\r\nmodel = xorgatemodel(test_hparams)\r\n\r\nlogger = logger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['_tracking_uri'])\r\ntrainer = pl.trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### expected behavior\r\n\r\ntrainer.fit runs without error.\r\n\r\n### environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\ncollecting environment information...\r\npytorch version: 1.3.1\r\nis debug build: no\r\ncuda used to build pytorch: 10.1.243\r\n\r\nos: centos linux 7 (core)\r\ngcc version: (gcc) 4.8.5 20150623 (red hat 4.8.5-39)\r\ncmake version: could not collect\r\n\r\npython version: 3.6\r\nis cuda available: yes\r\ncuda runtime version: 10.0.130\r\ngpu models and configuration:\r\ngpu 0: geforce gtx 1080 ti\r\ngpu 1: geforce gtx 1080 ti\r\ngpu 2: geforce gtx 1080 ti\r\ngpu 3: geforce gtx 1080 ti\r\ngpu 4: geforce gtx 1080 ti\r\ngpu 5: geforce gtx 1080 ti\r\ngpu 6: geforce gtx 1080 ti\r\ngpu 7: geforce gtx 1080 ti\r\n\r\nnvidia driver version: 440.33.01\r\ncudnn version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nversions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a pickle error when using logger, distributed_backend='ddp' on gpus without slurm while running trainer.fit in PyTorch 0.5.3.2 with PyTorch 1.3.1 and 1.4.0.",
        "Issue_preprocessed_content":"Title: pickle error from when using logger and distributed data parallel without slurm; Content: bug fails with a pickle error when the logger is logger, and on gpus but without slurm. to reproduce steps to reproduce the behavior . instantiate logger in pytorch with pytorch and the execution environment has environment variables and also and to connect to the tracking server with http basic authentication. the tracking server is also . instantiate trainer with logger instance as logger, and with the gpus parameter on a machine with nvidia gpus but without slurm. . run from the error output, it looks like multiprocessing is attempting to pickle the nested function in function code sample sample code tested with a very simple test model expected behavior runs without error. environment"
    },
    {
        "Issue_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/7232",
        "Issue_title":"Mlflow UI deployment error",
        "Issue_label":[
            "bug",
            "Ingestion"
        ],
        "Issue_creation_time":1662389970000,
        "Issue_closed_time":1662467440000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```\r\nAttributeError: 'DatabaseServiceMetadataPipeline' object has no attribute 'mlModelFilterPattern'\r\n```\r\n\r\nWe need to review which configuration param is being sent here",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  ui deployment error; Content: ```\r\nattributeerror: 'databaseservicemetadatapipeline' object has no attribute 'mlmodelfilterpattern'\r\n```\r\n\r\nwe need to review which configuration param is being sent here",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to deploy a UI, and needs to review the configuration parameters being sent.",
        "Issue_preprocessed_content":"Title: ui deployment error; Content: we need to review which configuration param is being sent here"
    },
    {
        "Issue_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/5688",
        "Issue_title":"MlFlow deployment fails from UI",
        "Issue_label":[
            "bug",
            "P0",
            "Ingestion"
        ],
        "Issue_creation_time":1656394659000,
        "Issue_closed_time":1656421119000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  deployment fails from ui; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to deploy from the UI.",
        "Issue_preprocessed_content":"Title: deployment fails from ui; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Issue_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1647959057000,
        "Issue_closed_time":1650643135000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: input to the script for publishing models to  is overly particular with inputs; Content: **description**\r\nwhen using the `publish_model_to_.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**triton information**\r\nwhat version of triton are you using? 2.19.0\r\n\r\nare you using the triton container or did you build it yourself? container\r\n\r\n**to reproduce**\r\n```\r\npython publish_model_to_.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nthis gives the following error:\r\n\r\n```\r\ntraceback (most recent call last):\r\n  file \"publish_model_to_.py\", line 71, in <module>\r\n    publish_to_()\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  file \"publish_model_to_.py\", line 56, in publish_to_\r\n    triton_flavor.log_model(\r\n  file \"\/\/triton-inference-server\/server\/deploy\/-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    model.log(\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, _model=_model, **kwargs)\r\n  file \"\/\/triton-inference-server\/server\/deploy\/-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  file \"\/opt\/conda\/envs\/\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nfileexistserror: [errno 17] file exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nthe model being used seems to have no effect on the error.\r\n\r\n**expected behavior**\r\nthe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using the `publish_model_to_.py` script, where if the value given for the `--model_directory` argument had a trailing `\/`, the script would fail in unexpected ways.",
        "Issue_preprocessed_content":"Title: input to the script for publishing models to is overly particular with inputs; Content: description when using the script, if the value given for the argument has a trailing , the script will bomb in interesting ways. triton what version of triton are you using? are you using the triton container or did you build it yourself? container to reproduce this gives the following error the model being used seems to have no effect on the error. expected behavior the input provided is syntactically identical to and should provide the same outcome."
    },
    {
        "Issue_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Issue_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1646758611000,
        "Issue_closed_time":1656947080000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [ byom  ] check valid url when creating predictor; Content: at the moment, an  byom predictor with arbitrary urls can be created. we should first check whether an actual  model is served at that url before creating\/linking said model.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to check if a valid model is served at a given URL before creating\/linking the model when creating a BYOM predictor.",
        "Issue_preprocessed_content":"Title: check valid url when creating predictor; Content: at the moment, an byom predictor with arbitrary urls can be created. we should first check whether an actual model is served at that url before said model."
    },
    {
        "Issue_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Issue_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Issue_label":[
            "type:bug",
            "topic:models",
            "topic:reader",
            "journey:intermediate"
        ],
        "Issue_creation_time":1645701717000,
        "Issue_closed_time":1651060598000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: logging always disabled for training `farmreader` models; Content: **describe the bug**\r\nwhen training a reader model, a user might want to log training statistics and metrics to . however, when initializing a `farmreader`, we initialize an `inferencer`. there, we call `logger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#l77), which disables all logging to . therefore, when a user is calling the reader's `train` method after initializing the reader, no tranining statistics wil be logged.\r\n\r\nas a workaround, the user can manually set `logger.disable_logging = false` before calling the `train` method.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where logging was always disabled for training `farmreader` models, requiring a workaround of manually setting `logger.disable_logging = false` before calling the `train` method.",
        "Issue_preprocessed_content":"Title: logging always disabled for training models; Content: describe the bug when training a reader model, a user might want to log training statistics and metrics to . however, when initializing a , we initialize an . there, we call on , which disables all logging to . therefore, when a user is calling the reader's method after initializing the reader, no tranining statistics wil be logged. as a workaround, the user can manually set before calling the method."
    },
    {
        "Issue_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/3146",
        "Issue_title":"bug: failed to containerize when using mlflow",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666803000000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":13.0,
        "Issue_body":"Trying to integrate Mlflow with my current bentoml workflow and following this example\r\n`https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch`\r\nBut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\n```Building docker image for Bento(tag=\"mlflow_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nERROR: failed to solve: executor failed running [\/bin\/sh -c bash <<EOF\r\nset -euxo pipefail\r\n\r\nif [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; then\r\n   set pip_interop_enabled to improve conda-pip interoperability. Conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"Updating conda base environment with environment.yml\"\r\n  \/opt\/conda\/bin\/conda config --set pip_interop_enabled True\r\n  \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml\r\n  \/opt\/conda\/bin\/conda clean --all\r\nfi\r\nEOF]: exit code: 1\r\nFailed building docker image: Command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', 'mlflow_pytfile', 'env\\\\docker\\\\Dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### To reproduce\r\n\r\nBug recreation steps:\r\nClone the repo `https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch` \r\nGoto the folder `examples\/mlflow\/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\nP.S. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### Environment\r\n\r\nbentoml version 1.0.7\r\nPython version  3.9.12\r\nDocker Engine 20.10.17",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: bug: failed to containerize when using ; trying to integrate  with my current bentoml workflow and following this example\r\n`https:\/\/github.com\/bentoml\/bentoml\/tree\/main\/examples\/\/pytorch`\r\nbut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize _pytorch_mnist_demo:latest`\r\n\r\n```building docker image for bento(tag=\"_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nerror: failed to solve: executor failed running [\/bin\/sh -c bash <<eof\r\nset -euxo pipefail\r\n\r\nif [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; Content: then\r\n   set pip_interop_enabled to improve conda-pip interoperability. conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"updating conda base environment with environment.yml\"\r\n  \/opt\/conda\/bin\/conda config --set pip_interop_enabled true\r\n  \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml\r\n  \/opt\/conda\/bin\/conda clean --all\r\nfi\r\neof]: exit code: 1\r\nfailed building docker image: command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', '_pytfile', 'env\\\\docker\\\\dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### to reproduce\r\n\r\nbug recreation steps:\r\nclone the repo `https:\/\/github.com\/bentoml\/bentoml\/tree\/main\/examples\/\/pytorch` \r\ngoto the folder `examples\/\/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize _pytorch_mnist_demo:latest`\r\n\r\np.s. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### environment\r\n\r\nbentoml version 1.0.7\r\npython version  3.9.12\r\ndocker engine 20.10.17",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to containerize a BentoML model using the example from the BentoML GitHub repository, resulting in an error code of 1.",
        "Issue_preprocessed_content":"Title: bug failed to containerize when using; Content: trying to integrate with my current bentoml workflow and following this example but getting error when i try to deploy model with docker, when i run to reproduce bug recreation steps clone the repo goto the folder works fine` environment bentoml version python version docker engine"
    },
    {
        "Issue_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Issue_title":"MLflow pyfunc model can't be loaded",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1641482199000,
        "Issue_closed_time":1642711286000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  pyfunc model can't be loaded; Content: **describe the bug**\r\ni can't load a ** model in the beta version of bentoml 1.0\r\n\r\n**to reproduce**\r\n1. train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport \r\n\r\n\r\n# load training data\r\niris = datasets.load_iris()\r\nx, y = iris.data, iris.target\r\n\r\n# model training\r\nclf = svm.svc()\r\nclf.fit(x, y)\r\n\r\n# wrap up as a custom pyfunc model\r\nclass modelpyfunc(.pyfunc.pythonmodel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# log model\r\nwith .start_run() as run:\r\n    model = modelpyfunc()\r\n    .pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. load it into bentoml\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml..import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml..load(tag)\r\n```\r\n3. the model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml..load(tag)` is failing to **attributeerror** `module '.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**expected behavior**\r\npyfunc model should load without issues\r\n\r\n**screenshots\/logs**\r\n```\r\ntraceback (most recent call last):\r\n  file \"sandbox.py\", line 11, in <module>\r\n    bentoml..load(tag)\r\n  file \"\/users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  file \"\/users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/.py\", line 85, in load\r\n    return loader_module.load_model(_folder)  # noqa\r\nattributeerror: module '.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**environment:**\r\n - os: macos 11.6\r\n - python version python 3.8.5\r\n - bentoml version bentoml-1.0.0a1",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to load a pyfunc model in the beta version of BentoML 1.0.",
        "Issue_preprocessed_content":"Title: pyfunc model can't be loaded; Content: describe the bug i can't load a model in the beta version of bentoml to reproduce . train & log a pyfunc model to mflow . load it into bentoml . the model gets successfully stored in the local model store , however the loading is failing to attributeerror expected behavior pyfunc model should load without issues environment os macos python version python bentoml version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Issue_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Issue_label":[
            "bug",
            "duplicate"
        ],
        "Issue_creation_time":1667105333000,
        "Issue_closed_time":1669124369000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]: runs recorded in  nests all recursively when [full] installed; Content: ### pycaret version checks\r\n\r\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### issue description\r\n\r\nwhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in  dashboard.\r\nthis happens only with [full] installation.\r\n\r\n### reproducible example\r\n\r\n```python\r\n%pip install -u pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport \r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\n.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"class variable\", log_experiment=true)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"class variable\", log_experiment=true)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### expected behavior\r\n\r\nexpected display: (when installed without [full])\r\n![ok](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nactual display: (when installed with [full])\r\n![ng](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### actual results\r\n\r\n```python-traceback\r\nattached the figure also in 'expected behavior'.\r\n```\r\n\r\n\r\n### installed versions\r\n\r\n<details>\r\nsystem:\r\n    python: 3.9.5 (default, nov 23 2021, 15:27:38)  [gcc 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: linux-5.10.102.1-microsoft-standard-wsl2-x86_64-with-glibc2.31\r\n\r\npycaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             ipython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\npycaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              : 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyldavis: not installed\r\n              gensim: not installed\r\n               spacy: not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: not installed\r\n             prophet: not installed\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where installing pycaret with the [full] option caused all runs executed in one script to be shown nested recursively in the dashboard.",
        "Issue_preprocessed_content":"Title: runs recorded in nests all recursively when installed; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description when pycaret is installed with , all runs executed in one script are shown nested recursively in dashboard. this happens only with installation. reproducible example expected behavior expected display actual display actual results installed versions system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats pmdarima psutil pycaret optional dependencies shap interpret umap explainerdashboard autoviz fairlearn xgboost catboost kmodes mlxtend statsforecast ray hyperopt optuna skopt gradio fastapi uvicorn m cgen evidently nltk pyldavis not installed gensim not installed spacy not installed wordcloud textblob fugue streamlit not installed prophet not installed"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Issue_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1663557360000,
        "Issue_closed_time":1663775400000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]: runs recorded in  nests all recursively; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [x] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### issue description\n\nas started runs in logger are never ended, all runs shown in  dashboard seem to be nested recursively.\r\n 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### reproducible example\n\n```python\nimport \r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\n.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"class variable\", log_experiment=true, silent=true)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"class variable\", log_experiment=true, silent=true)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### expected behavior\n\nexpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nactual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### actual results\n\n```python-traceback\nattached the figure also in 'expected behavior'.\n```\n\n\n### installed versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug in pycaret where runs started in the logger were never ended, resulting in all runs shown in the dashboard being nested recursively.",
        "Issue_preprocessed_content":"Title: runs recorded in nests all recursively; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description as started runs in logger are never ended, all runs shown in dashboard seem to be nested recursively. fixed the display of deeply nested runs correctly, so the bug is now problematic. reproducible example expected behavior expected display actual display actual results installed versions"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Issue_title":"MlFlow not logging metrics",
        "Issue_label":[
            "invalid",
            "mlflow"
        ],
        "Issue_creation_time":1660651109000,
        "Issue_closed_time":1660653306000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  not logging metrics; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### issue description\n\nhi,\r\n\r\ni am trying to integrate pycaret with  using your parameter `log_experiment` in `setup()`. when i set it to true, everything is stores as planned in my local  server, but not the metrics.\r\n\r\nin the documentation is says the `log_experiment=true` should control everything. so i am not sure if i do something wrong here of if it is a bug from your side.\r\n\r\nwould be glad if you could help!\n\n### reproducible example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = regressionexperiment()\r\nexp.setup(data=df, log_experiment=true)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### expected behavior\n\nshould log metrics\n\n### actual results\n\n```python-traceback\nno metrics logged.\n```\n\n\n### installed versions\n\n<details>\r\npycaret 3.0.0rc3\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with PyCaret where metrics are not being logged when the parameter 'log_experiment' is set to true.",
        "Issue_preprocessed_content":"Title: not logging metrics; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description hi, i am trying to integrate pycaret with using your parameter in . when i set it to true, everything is stores as planned in my local server, but not the metrics. in the documentation is says the should control everything. so i am not sure if i do something wrong here of if it is a bug from your side. would be glad if you could help! reproducible example expected behavior should log metrics actual results installed versions pycaret"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Issue_title":"[BUG]: MLflow server integration",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1660026191000,
        "Issue_closed_time":1660286399000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]:  server integration; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### issue description\n\ni have a problem saving xgboost run in  server. the run has a status of unfinished, no metrics or artifacts are created. \r\n\r\nwhen i use ` ui` everything is fine, but when i run  server with sqlite as backend store the problem occurs.\r\ncommand used to run  server- ` server --host 0.0.0.0 --port 5000 --default-artifact-root \/\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/\/experiments\/.db`\n\n### expected behavior\n\nartifacts and metrics should be crated. \n\n### actual results\n\n```python-traceback\nerror from logs.log:\r\n\r\n2022-08-09 06:11:05,384:error:dashboard_logger.log_model() for xgbclassifier(base_score=0.5, booster='gbtree', callbacks=none,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=none, enable_categorical=false,\r\n              eval_metric=none, feature_types=none, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=none,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:error:traceback (most recent call last):\r\n  file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/_logger.py\", line 46, in log_params\r\n    .log_params(params)\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/fluent.py\", line 675, in log_params\r\n    client().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(logbatch, req_body)\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise restexception(json.loads(response.text))\r\n.exceptions.restexception: invalid_parameter_value: invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'none'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'none'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'none'}, {'key': 'enable_categorical', 'value': 'false'}, {'key': 'eval_metric', 'value': 'none'}, {'key': 'feature_types', 'value': 'none'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'none'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. hint: value was of type 'list'. see the api docs for more information about request parameters.\n```\n\n\n### installed versions\n\n<details>\r\npycaret- version: 2.3.10 <\/br>\r\n- version: 1.27.0 <\/br>\r\nxgboost-  version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with server integration when attempting to save an XGBoost run, resulting in an unfinished status with no metrics or artifacts created.",
        "Issue_preprocessed_content":"Title: server integration; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description i have a problem saving xgboost run in server. the run has a status of unfinished, no metrics or artifacts are created. when i use everything is fine, but when i run server with sqlite as backend store the problem occurs. command used to run server reproducible example expected behavior artifacts and metrics should be crated. actual results installed versions pycaret version version xgboost version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Issue_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Issue_label":[
            "bug",
            "mlflow",
            "priority_high"
        ],
        "Issue_creation_time":1659210438000,
        "Issue_closed_time":1670522892000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]:  incorrectly logging models \"lasso least angle regression\" and \"least angle regression\"; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [x] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### issue description\n\n logs the name of both models \"least angle regression\" and \"lasso least angle regression\" as \"least angle regression\".\r\n\r\nwhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags..runname`.\r\n\r\npython version: 3.9.5\r\npycaret version: '3.0.0.rc3'\r\npandas version: 1.4.3\r\n\n\n### reproducible example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nexperiment_name = 'diamond_experiment'\r\ns = setup(data=dataset, target='price', log_experiment=true, experiment_name=experiment_name, session_id=42, verbose=true)\r\n\r\nmodel = compare_models(verbose=false)\r\n\r\nprint(f\"notice least angle regression is not unique:\\n{get_logs(experiment_name=experiment_name)['tags..runname'].value_counts()}\")\r\n\r\n# loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# there should be a single unique value for each model\r\nfor model in pull().model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=experiment_name)[get_logs(experiment_name=experiment_name)['tags..runname'] == model])}\")\r\n\r\n# further investigation: model least angle regression has 2 instances (should be lasso least angle regression and least angle regression)\r\nget_logs(experiment_name=experiment_name)[get_logs(experiment_name=experiment_name)['tags..runname'] == 'least angle regression']\r\n```\n```\n\n\n### expected behavior\n\n`tags..runname` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### actual results\n\n```python-traceback\nwhen looking into the `tags..runname` you can see they are all unique but least angle regression is there twice and lasso least angle regression isn't there at all. could this be logged incorrectly?\r\n\r\ngradient boosting regressor - 1\r\ncatboost regressor - 1\r\nlight gradient boosting machine - 1\r\nextreme gradient boosting - 1\r\nlasso regression - 1\r\nridge regression - 1\r\nlinear regression - 1\r\nlasso least angle regression - 0\r\nleast angle regression - 2\r\nextra trees regressor - 1\r\nrandom forest regressor - 1\r\nadaboost regressor - 1\r\ndecision tree regressor - 1\r\northogonal matching pursuit - 1\r\nelastic net - 1\r\nhuber regressor - 1\r\nbayesian ridge - 1\r\nk neighbors regressor - 1\r\ndummy regressor - 1\r\npassive aggressive regressor - 1\n```\n\n\n### installed versions\n\n<details>\r\nsystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, may  3 2021, 13:17:02)  [clang 6.0 (clang-600.0.57)]\r\nexecutable: path_to_env\/venv\/bin\/python\r\n   machine: macos-10.16-x86_64-i386-64bit\r\n\r\npycaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             ipython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logging of models \"lasso least angle regression\" and \"least angle regression\" were incorrectly logged as \"least angle regression\" in the `get_logs()` function.",
        "Issue_preprocessed_content":"Title: incorrectly logging models lasso least angle regression and least angle regression; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description logs the name of both models least angle regression and lasso least angle regression as least angle regression . when looking into the you can see both of those models have unique but both have the same . python version pycaret version pandas version reproducible example python traceback when looking into the you can see they are all unique but least angle regression is there twice and lasso least angle regression isn't there at all. could this be logged incorrectly? gradient boosting regressor catboost regressor light gradient boosting machine extreme gradient boosting lasso regression ridge regression linear regression lasso least angle regression least angle regression extra trees regressor random forest regressor adaboost regressor decision tree regressor orthogonal matching pursuit elastic net huber regressor bayesian ridge k neighbors regressor dummy regressor passive aggressive regressor installed versions system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod installed but version unavailable imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats installed but version unavailable pmdarima psutil"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2801",
        "Issue_title":"[BUG]: pycaret + mlflow integration does not allow probabilities for classification and binary response models",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1658846256000,
        "Issue_closed_time":1669247416000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nWe have been using pycaret 2.2 for the model training procedure and registration to the mlflow server. (python based) My company uses a managed version of this in Azure Databricks. After the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. We would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. This is not working in pycaret and appears to be a bug of some sort. It is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### Reproducible Example\n\n```python\n# import mlflow.sklearn\r\n# model = mlflow.sklearn.load_model(production_algorithm)\r\n# model.predict_prob(X)\n```\n\n\n### Expected Behavior\n\nwe should see the probabilities model.predict_prob(X) but this code errors out. Another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.DataFrame(X))\r\n\n\n### Actual Results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. Ex. 0.4278\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: Linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]: pycaret +  integration does not allow probabilities for classification and binary response models; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### issue description\n\nwe have been using pycaret 2.2 for the model training procedure and registration to the  server. (python based) my company uses a managed version of this in azure databricks. after the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. we would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. this is not working in pycaret and appears to be a bug of some sort. it is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### reproducible example\n\n```python\n# import .sklearn\r\n# model = .sklearn.load_model(production_algorithm)\r\n# model.predict_prob(x)\n```\n\n\n### expected behavior\n\nwe should see the probabilities model.predict_prob(x) but this code errors out. another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.dataframe(x))\r\n\n\n### actual results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. ex. 0.4278\n```\n\n\n### installed versions\n\n<details>\r\nsystem:\r\n    python: 3.8.10 (default, mar 15 2022, 12:22:08)  [gcc 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonenv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\npycaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             ipython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with pycaret + integration not allowing probabilities for classification and binary response models, resulting in an error when attempting to predict probabilities.",
        "Issue_preprocessed_content":"Title: pycaret + integration does not allow probabilities for classification and binary response models; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description we have been using pycaret for the model training procedure and registration to the server. my company uses a managed version of this in azure databricks. after the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response . we would also like to leverage the scikit learn function to create the probabilities in addition to the predicted value. this is not working in pycaret and appears to be a bug of some sort. it is also important to note that we are able to see the during the model training but not when we call the algorithm for a separate scoring function. reproducible example expected behavior we should see the probabilities but this code errors out. another example would be the following actual results installed versions system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod installed but version unavailable imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats installed but version unavailable pmdarima psutil"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Issue_title":"mlflow ui doesn't show any models",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1653399055000,
        "Issue_closed_time":1653501835000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  ui doesn't show any models; Content: ### pycaret version checks\n\n- [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [x] i have confirmed this bug exists on the develop branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### issue description\n\ni have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3aissue++ui+is%3aclosed\r\n\r\ni do not see any models in the ` ui` *during training*, while several models have already converged and logged to the file system. i see some models have already reported auc, mse, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nthanks!\r\n\n\n### reproducible example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=true, feature_selection=true, fix_imbalance=true, \r\n                                  remove_perfect_collinearity=false,\r\n                                  log_experiment=true, \r\n                                  log_plots=true, profile=false, log_profile=false,\r\n                                  silent=true,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=true, n_select=3,errors='raise')\n```\n\n\n### expected behavior\n\nbeing able to see the models that have already converged\n\n### actual results\n\n```python-traceback\nno model is present in the ` ui` dashboard\n```\n\n\n### installed versions\n\n2.3.10",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the UI dashboard of PyCaret did not show any models, despite several models having already converged and logged to the file system.",
        "Issue_preprocessed_content":"Title: ui doesn't show any models; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the develop branch of pycaret . issue description i have tried both the nightly and the release branch, and read the issues posted here i do not see any models in the during training , while several models have already converged and logged to the file system. i see some models have already reported auc, mse, etc. but as shows below, nothing is present in the dashboard thanks! reproducible example expected behavior being able to see the models that have already converged actual results installed versions"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Issue_title":"plots are not saving through MLFLOW",
        "Issue_label":[
            "bug",
            "mlflow",
            "missing_info"
        ],
        "Issue_creation_time":1650342119000,
        "Issue_closed_time":1650470329000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: plots are not saving through ; Content: this is the error i get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected keyword argument error when attempting to save a plot using the plot_model() function.",
        "Issue_preprocessed_content":"Title: plots are not saving through; Content: this is the error i get got an unexpected keyword argument 'system'"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Issue_title":"[BUG] mlflow ui never runs",
        "Issue_label":[
            "bug",
            "Not related to PyCaret",
            "mlflow",
            "missing_info"
        ],
        "Issue_creation_time":1650277642000,
        "Issue_closed_time":1650449956000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  ui never runs; Content: **describe the bug**\r\ngetting error \"filenotfounderror: [winerror 2] the system cannot find the file specified\" while running \" ui\".\r\n\r\n**to reproduce**\r\nrun \" ui\"\r\n\r\n\r\n**expected behavior**\r\nit should run without any issues\r\n\r\n\r\n**versions**\r\n2.3.10\r\n\r\nnot sure if this is the right forum to post this issue. if it is not, please ignore.\r\n<!-- thanks for contributing! -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error while running \"ui\" which prevented it from running, and was unsure if this was the right forum to post the issue.",
        "Issue_preprocessed_content":"Title: ui never runs; Content: describe the bug getting error filenotfounderror the system cannot find the file specified while running ui . to reproduce run ui expected behavior it should run without any issues versions not sure if this is the right forum to post this issue. if it is not, please ignore."
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Issue_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Issue_label":[
            "bug",
            "mlflow",
            "time_series",
            "plot_model"
        ],
        "Issue_creation_time":1634814038000,
        "Issue_closed_time":1635811087000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] issue with  timeseries_beta branch; Content: if in setup log_plot set true then it is giving error in self.__log_model() as \r\nfor plot in log_plots:\r\ntypeerror: 'bool' object is not iterable",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when setting log_plot to true in the setup of the timeseries_beta branch.",
        "Issue_preprocessed_content":"Title: issue with branch; Content: if in setup set true then it is giving error in as for plot in typeerror 'bool' object is not iterable"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Issue_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Issue_label":[
            "bug",
            "mlflow"
        ],
        "Issue_creation_time":1634052175000,
        "Issue_closed_time":1635405096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] some types plot types are not getting saved to the  experiment artifacts dir; Content: **describe the bug**\r\nthank you for creating such a helpful tool!\r\nthe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the  experiment artifacts dir. i think the issue is with inconsistent naming for the saved png for certain plot types.\r\nthank you for your help!\r\n<!--\r\n-->\r\n\r\n**to reproduce**\r\n<!--\r\nadd a minimal, complete, and verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nif the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=false, \r\n      silent=true,\r\n      verbose=false,\r\n      # for  logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = true, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**expected behavior**\r\n<!--\r\n-->\r\ni expect all of the plot types to be logged under the  artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nhowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**additional context**\r\n<!--\r\nadd any other context about the problem here.\r\n-->\r\ni think the issue is with inconsistent naming of the file. here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:info:saving 'calibration.png'\r\n2021-10-11 19:03:20,064:info:visual rendered successfully\r\n2021-10-11 19:03:20,213:info:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:warning:[errno 2] no such file or directory: 'calibration curve.png'\r\n```\r\nso you can see that it is looking for 'calibration curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**versions**\r\npython 3.8.11\r\n\r\n<!--\r\nplease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\npycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- thanks for contributing! -->\r\n",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where some types of plots (e.g. \"calibration\" and \"feature\") are not getting saved to the experiment artifacts directory, due to inconsistent naming of the file.",
        "Issue_preprocessed_content":"Title: some types plot types are not getting saved to the experiment artifacts dir; Content: describe the bug thank you for creating such a helpful tool! the problem i'm facing is that some types plot types are not getting saved to the experiment artifacts dir. i think the issue is with inconsistent naming for the saved png for certain plot types. thank you for your help! to reproduce expected behavior i expect all of the plot types to be logged under the artifacts dir however, and are saved to the working directory. additional context i think the issue is with inconsistent naming of the file. here is a printout of the log when it tries to save the calibration plot so you can see that it is looking for 'calibration but what actually gets produced is versions python pycaret"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1568",
        "Issue_title":"MLFlow logging issue in compare_models of time_series",
        "Issue_label":[
            "bug",
            "time_series"
        ],
        "Issue_creation_time":1631459045000,
        "Issue_closed_time":1634125992000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"MLFlow logging issue in compare_models of time_series\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('airline')\r\n\r\nfrom pycaret.time_series import *\r\ns = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=True, experiment_name = 'airline')\r\n\r\nbest = compare_models()\r\n\r\n!mlflow ui\r\n```\r\n\r\nCheck localhost:5000:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png)\r\n\r\nAll the runs fail in `compare_models`. Parameters are logged but metrics and artifacts didn't. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png)\r\n\r\nI cannot reproduce this with `create_model` it means `create_model` works just fine! ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logging issue in compare_models of time_series; Content:  logging issue in compare_models of time_series\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('airline')\r\n\r\nfrom pycaret.time_series import *\r\ns = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=true, experiment_name = 'airline')\r\n\r\nbest = compare_models()\r\n\r\n! ui\r\n```\r\n\r\ncheck localhost:5000:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png)\r\n\r\nall the runs fail in `compare_models`. parameters are logged but metrics and artifacts didn't. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png)\r\n\r\ni cannot reproduce this with `create_model` it means `create_model` works just fine! ",
        "Issue_original_content_gpt_summary":"The user encountered a logging issue in the compare_models function of the PyCaret time_series module, where parameters were logged but metrics and artifacts were not.",
        "Issue_preprocessed_content":"Title: logging issue in of; Content: logging issue in of check localhost all the runs fail in . parameters are logged but metrics and artifacts didn't. i cannot reproduce this with it means works just fine!"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Issue_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Issue_label":[
            "bug",
            "mlflow",
            "no-issue-activity"
        ],
        "Issue_creation_time":1607745205000,
        "Issue_closed_time":1620299767000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  doesn't save model artifact and some plots - clustering; Content: i'm using clustering module of pycaret and the integration with  but i have problems because i think it doesn't save all artifacs and the status is always failed.\r\nthis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=true,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=true,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=true,\r\n                     log_plots=true,\r\n                     log_profile=true,\r\n                     log_data=true,\r\n                     profile=false,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\n",
        "Issue_original_content_gpt_summary":"The user encountered challenges with PyCaret's clustering module, such as not saving model artifacts and some plots, and the status always failing.",
        "Issue_preprocessed_content":"Title: doesn't save model artifact and some plots clustering; Content: i'm using clustering module of pycaret and the integration with but i have problems because i think it doesn't save all artifacs and the status is always failed. this is my code my logs are the following i'm using pycaret version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/566",
        "Issue_title":"Compare models MLFlowException",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1598718264000,
        "Issue_closed_time":1598806653000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"Hi. I just upgraded to pycaret 2.1. When I ran the compare_models function with the Titanic dataset, I got the following error:\r\n\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nThe same code worked fine in pycaret 2.0.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: compare models exception; Content: hi. i just upgraded to pycaret 2.1. when i ran the compare_models function with the titanic dataset, i got the following error:\r\n\r\nexception: unable to map 'np.object' type to  datatype. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nthe same code worked fine in pycaret 2.0.",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the compare_models function with the titanic dataset in PyCaret 2.1 due to an inability to map the 'np.object' type to a datatype.",
        "Issue_preprocessed_content":"Title: compare models exception; Content: hi. i just upgraded to pycaret when i ran the function with the titanic dataset, i got the following error exception unable to map type to datatype. canbe mapped iff all values have identical data type which is one of , int, float the same code worked fine in pycaret"
    },
    {
        "Issue_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Issue_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Issue_label":[
            "type \/ bug",
            "help wanted",
            "area \/ integrations",
            "phase \/ shipped",
            "area \/ SDK-storage"
        ],
        "Issue_creation_time":1645926561000,
        "Issue_closed_time":1649939186000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: aim convert  --experiment fails for experiment id, works for experiment name; Content: ## \ud83d\udc1b bug\r\n\r\ndoing\r\n\r\n`$ aim convert  --tracking_uri 'file:\/\/\/users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show--logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert  --tracking_uri 'file:\/\/\/users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### to reproduce\r\n\r\nsee above\r\n\r\n### expected behavior\r\n\r\nconvert the experiment by id\r\n\r\n### environment\r\n\r\n- aim version 3.6\r\n- python 3.8.1\r\n- pip3\r\n- ubuntu 20.04.3 lts\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using the experiment ID to convert an experiment with AIM failed, but using the experiment name instead worked.",
        "Issue_preprocessed_content":"Title: aim convert experiment fails for experiment id, works for experiment name; Content: bug doing as described here fails with the following error using the experiment name instead of the experiment id works to reproduce see above expected behavior convert the experiment by id environment aim version python pip ubuntu lts"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1899",
        "Issue_title":"nyc-taxi-mlflow-deployment.yml refers to a folder that doesn't exists",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669062340000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Operating System\n\nLinux\n\n### Version Information\n\nAzure cli v2\n\n### Steps to reproduce\n\nThe azureml-example batch endpoint nyc-taxi-mlflow-deployment.yml file, refers to a  .\/autolog_nyc_taxi folder that doesn't exist\n\n### Expected behavior\n\nIt looks like we need to re-add the folder?\n\n### Actual behavior\n\ncode fails because folder doesn't exist\n\n### Addition information\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: nyc-taxi--deployment.yml refers to a folder that doesn't exists; Content: ### operating system\n\nlinux\n\n### version information\n\nazure cli v2\n\n### steps to reproduce\n\nthe azureml-example batch endpoint nyc-taxi--deployment.yml file, refers to a  .\/autolog_nyc_taxi folder that doesn't exist\n\n### expected behavior\n\nit looks like we need to re-add the folder?\n\n### actual behavior\n\ncode fails because folder doesn't exist\n\n### addition information\n\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered an issue where the azureml-example batch endpoint nyc-taxi--deployment.yml file referred to a folder that did not exist, causing the code to fail.",
        "Issue_preprocessed_content":"Title: refers to a folder that doesn't exists; Content: operating system linux version azure cli v steps to reproduce the azureml example batch endpoint file, refers to a folder that doesn't exist expected behavior it looks like we need to re add the folder? actual behavior code fails because folder doesn't exist addition"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1897",
        "Issue_title":"MLflow cli endpoint example out of date with new syntax from breaking changes in yaml (last updated May 11)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669043018000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Operating System\n\nWindows\n\n### Version Information\n\nlatest cli v2 \n\n### Steps to reproduce\n\nhttps:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/cli\/endpoints\/online\/mlflow\/sklearn-deployment.yaml\r\n\r\nThis yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model.\n\n### Expected behavior\n\nThat the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. \n\n### Actual behavior\n\nIt fails.\n\n### Addition information\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  cli endpoint example out of date with new syntax from breaking changes in yaml (last updated may 11); Content: ### operating system\n\nwindows\n\n### version information\n\nlatest cli v2 \n\n### steps to reproduce\n\nhttps:\/\/github.com\/azure\/azureml-examples\/blob\/main\/cli\/endpoints\/online\/\/sklearn-deployment.yaml\r\n\r\nthis yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model.\n\n### expected behavior\n\nthat the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. \n\n### actual behavior\n\nit fails.\n\n### addition information\n\n_no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with deploying an endpoint using the Azure CLI, due to the syntax in the YAML file being out of date with the latest version of the CLI.",
        "Issue_preprocessed_content":"Title: cli endpoint example out of date with new syntax from breaking changes in yaml; Content: operating system windows version latest cli v steps to reproduce this yaml is out of date, the model yaml config is wrong. name is no longer required when specifying model. expected behavior that the deployment works based on using the cli command , but it fails. actual behavior it fails. addition"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/318",
        "Issue_title":"MLflow 1.13 probably broke deployment",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1609294231000,
        "Issue_closed_time":1609558021000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"\r\n\r\nhttps:\/\/github.com\/Azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/3419\/files",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  1.13 probably broke deployment; Content: \r\n\r\nhttps:\/\/github.com\/azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/\/\/pull\/3419\/files",
        "Issue_original_content_gpt_summary":"The user @trangevi encountered a challenge with deployment after 1.13 was released, as evidenced by a failed run on AzureML-examples.",
        "Issue_preprocessed_content":"Title: probably broke deployment"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Issue_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Issue_label":[
            "doc-bug",
            "Workspace Management"
        ],
        "Issue_creation_time":1581064035000,
        "Issue_closed_time":1581065545000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: attributeerror: 'workspace' object has no attribute 'get__tracking_uri'; Content: i receive the following error when running the following [notebook](https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nin [6]: ws.get__tracking_uri()\r\n---------------------------------------------------------------------------\r\nattributeerror                            traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get__tracking_uri()\r\n\r\nattributeerror: 'workspace' object has no attribute 'get__tracking_uri'\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to run a notebook from the Azure Machine Learning Notebooks repository.",
        "Issue_preprocessed_content":"Title: attributeerror 'workspace' object has no attribute; Content: i receive the following error when running the following"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Issue_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1665708717000,
        "Issue_closed_time":1667718001000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: on qrun:\".exceptions.exception: param value .... had length 780, which exceeded length limit of 500 \"; Content: ## \ud83d\udc1b bug description\r\n\r\n<!-- a clear and concise description of what the bug is. -->\r\n\r\nwhen i do the example:\r\nqrun qrun benchmarks\\gats\\workflow_config_gats_alpha158.yaml\r\n\r\ni got the error info:\r\n\r\n\r\n\r\n(py38) d:\\workspool\\works2021\\adair2021\\s92\\p4\\qlib-main\\examples>qrun benchmarks\\gats\\workflow_config_gats_alpha158_full02.yaml\r\n[7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [config.py:413] - default_conf: client.\r\n[7724:mainthread](2022-10-14 07:53:33,890) info - qlib.workflow - [expm.py:31] - experiment manager uri is at file:d:\\workspool\\works2021\\adair2021\\s92\\p4\\qlib-main\\examples\\mlruns\r\n[7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [__init__.py:76] - data_path={'__default_freq': windowspath('c:\/users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:mainthread](2022-10-14 07:53:33,906) info - qlib.workflow - [expm.py:316] - <.tracking.client.client object at 0x0000017b5d406f40>\r\n[7724:mainthread](2022-10-14 07:53:33,906) info - qlib.workflow - [exp.py:260] - experiment 3 starts running ...\r\n[7724:mainthread](2022-10-14 07:53:34,124) info - qlib.workflow - [recorder.py:339] - recorder 41d40d173e614811bad721127a3204b8 starts running under experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:mainthread](2022-10-14 07:53:34,140) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:mainthread](2022-10-14 07:53:34,158) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:mainthread](2022-10-14 07:53:34,164) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git diff --cached`\r\nexception in thread thread-1:\r\ntraceback (most recent call last):\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"param value\", max_param_val_length, value)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise exception(\r\n.exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<model>', 'dataset': '<dataset>'}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nduring handling of the above exception, another exception occurred:\r\n\r\ntraceback (most recent call last):\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise exception(msg, invalid_parameter_value)\r\n.exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<model>', 'dataset': '<dataset>'}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nthe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nincorrect example:\r\n---------------------------------------\r\nwith .start_run():\r\n    .log_param(\"depth\", 3)\r\n    .log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nwhich will throw an exception for overwriting a\r\nlogged parameter.\r\n\r\ncorrect example:\r\n---------------------------------------\r\nwith .start_run():\r\n    with .start_run(nested=true):\r\n        .log_param(\"depth\", 3)\r\n    with .start_run(nested=true):\r\n        .log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nwhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:mainthread](2022-10-14 07:53:35,515) info - qlib.gats - [pytorch_gats_ts.py:81] - gats pytorch version...\r\n[7724:mainthread](2022-10-14 07:53:35,562) info - qlib.gats - [pytorch_gats_ts.py:100] - gats parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : lstm\r\nmodel_path : none\r\nvisible_gpu : 0\r\nuse_gpu : true\r\nseed : none\r\n[7724:mainthread](2022-10-14 07:53:35,562) info - qlib.gats - [pytorch_gats_ts.py:146] - model:\r\ngatmodel(\r\n  (rnn): lstm(158, 64, num_layers=2, batch_first=true, dropout=0.7)\r\n  (transformation): linear(in_features=64, out_features=64, bias=true)\r\n  (fc): linear(in_features=64, out_features=64, bias=true)\r\n  (fc_out): linear(in_features=64, out_features=1, bias=true)\r\n  (leaky_relu): leakyrelu(negative_slope=0.01)\r\n  (softmax): softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nthen the program re-run again.\r\ni am wondering how to fix it.\r\nthanks a lot.\r\n\r\n\r\n\r\n\r\n## to reproduce\r\n\r\nsteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## expected behavior\r\n\r\n<!-- a clear and concise description of what you expected to happen. -->\r\n\r\n## screenshot\r\n\r\n<!-- a screenshot of the error message or anything shouldn't appear-->\r\n\r\n## environment\r\n\r\n**note**: user could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - qlib version:\r\n - 0.8.6.99'\r\n - python version:\r\n - 3.8.5\r\n - os (`windows`, `linux`, `macos`):\r\n - windows 10\r\n - commit number (optional, please provide it if you are using the dev version):\r\n\r\n## additional notes\r\n\r\n<!-- add any other information about the problem here. -->\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running qrun with a workflow configuration file, resulting in an error message due to exceeding the length limit of 500 for a parameter value.",
        "Issue_preprocessed_content":"Title: on param value had length , which exceeded length limit of; Content: bug description when i do the example qrun qrun i got the error py mainthread , client. mainthread , experiment manager uri is at mainthread , qlib successfully initialized based on client settings. mainthread , mainthread , mainthread , experiment starts running mainthread , recorder d d e bad a b starts running under experiment 'git' mainthread , fail to log the uncommitted code of $cwd when run 'git' mainthread , fail to log the uncommitted code of $cwd when run 'git' mainthread , fail to log the uncommitted code of $cwd when run exception in thread thread traceback file line , in param file line , in file line , in value , value file line , in raise exception file line , in file line , in run file line , in run data file line , in key, value file line , in raise exception param value ' gats pytorch mainthread , gats parameters setting dropout lr metric loss optimizer adam mse lstm none true seed none mainthread , model gatmodel lstm bias true bias true bias true softmax then the program re run again. i am wondering how to fix it. thanks a lot. to reproduce steps to reproduce the behavior . . . expected behavior screenshot environment note user could run under project directory to get system and paste them here directly. qlib version python version os windows commit number additional notes"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1298",
        "Issue_title":"not compatible with mlflow v1.28.0",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1663557425000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"when run workflow:\r\n```\r\nqrun ALSTM_workflow_config_alstm_Alpha158.yaml\r\n```  \r\nmlflow v1.27.0 work fine,but failed when with mlflow v1.28.0:\r\n```\r\nFile \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/client.py\", line 852, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/_tracking_service\/client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 778, which exceeded length limit of 500\r\n```\r\ni think the new mflow feature cause this bug.mlflow limit param valu lengh to 500,by read code ,it can not be overwrite.\r\nmaybe relate with this [issue](https:\/\/github.com\/mlflow\/mlflow\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: not compatible with  v1.28.0; Content: when run workflow:\r\n```\r\nqrun alstm_workflow_config_alstm_alpha158.yaml\r\n```  \r\n v1.27.0 work fine,but failed when with  v1.28.0:\r\n```\r\nfile \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  file \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/-1.28.0-py3.8.egg\/\/tracking\/client.py\", line 852, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  file \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/-1.28.0-py3.8.egg\/\/tracking\/_tracking_service\/client.py\", line 305, in log_param\r\n    raise exception(msg, invalid_parameter_value)\r\n.exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<model>', 'dataset': '<dataset>'}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 778, which exceeded length limit of 500\r\n```\r\ni think the new mflow feature cause this bug. limit param valu lengh to 500,by read code ,it can not be overwrite.\r\nmaybe relate with this [issue](https:\/\/github.com\/\/\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running a workflow with v1.28.0, where the workflow failed due to a parameter value length limit of 500, which was caused by a new mflow feature.",
        "Issue_preprocessed_content":"Title: not compatible with; Content: when run workflow work fine,but failed when with i think the new mflow feature cause this limit param valu lengh to ,by read code ,it can not be overwrite. maybe relate with this"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1035",
        "Issue_title":"run the example workflow_by_code.ipynb, caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' ",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1649238776000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Bug Description\r\nwhen I run the code below in qlib-main\/examples\/workflow_by_code.ipynb\uff0cit caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' \r\n###################################\r\n# train model\r\n###################################\r\ndata_handler_config = {\r\n    \"start_time\": \"2008-01-01\",\r\n    \"end_time\": \"2020-08-01\",\r\n    \"fit_start_time\": \"2008-01-01\",\r\n    \"fit_end_time\": \"2014-12-31\",\r\n    \"instruments\": market,\r\n}\r\n\r\ntask = {\r\n    \"model\": {\r\n        \"class\": \"LGBModel\",\r\n        \"module_path\": \"qlib.contrib.model.gbdt\",\r\n        \"kwargs\": {\r\n            \"loss\": \"mse\",\r\n            \"colsample_bytree\": 0.8879,\r\n            \"learning_rate\": 0.0421,\r\n            \"subsample\": 0.8789,\r\n            \"lambda_l1\": 205.6999,\r\n            \"lambda_l2\": 580.9768,\r\n            \"max_depth\": 8,\r\n            \"num_leaves\": 210,\r\n            \"num_threads\": 20,\r\n        },\r\n    },\r\n    \"dataset\": {\r\n        \"class\": \"DatasetH\",\r\n        \"module_path\": \"qlib.data.dataset\",\r\n        \"kwargs\": {\r\n            \"handler\": {\r\n                \"class\": \"Alpha158\",\r\n                \"module_path\": \"qlib.contrib.data.handler\",\r\n                \"kwargs\": data_handler_config,\r\n            },\r\n            \"segments\": {\r\n                \"train\": (\"2008-01-01\", \"2014-12-31\"),\r\n                \"valid\": (\"2015-01-01\", \"2016-12-31\"),\r\n                \"test\": (\"2017-01-01\", \"2020-08-01\"),\r\n            },\r\n        },\r\n    },\r\n}\r\n\r\n# model initiaiton\r\nmodel = init_instance_by_config(task[\"model\"])\r\ndataset = init_instance_by_config(task[\"dataset\"])\r\n\r\n# start exp to train model\r\nwith R.start(experiment_name=\"train_model\"):\r\n    R.log_params(**flatten_dict(task))\r\n    model.fit(dataset)\r\n    R.save_objects(trained_model=model)\r\n    rid = R.get_recorder().id\r\n\r\n=====================\r\nThe whole error message is below\uff1a\r\n[2607:MainThread](2022-04-06 17:38:12,377) INFO - qlib.timer - [log.py:113] - Time cost: 18.919s | Loading data Done\r\n[2607:MainThread](2022-04-06 17:38:12,737) INFO - qlib.timer - [log.py:113] - Time cost: 0.147s | DropnaLabel Done\r\n\/Users\/yzwu\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/data\/dataset\/processor.py:310: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\r\n  df[cols] = df[cols].groupby(\"datetime\").apply(self.zscore_func)\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 1.650s | CSZScoreNorm Done\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 2.010s | fit & process data Done\r\n[2607:MainThread](2022-04-06 17:38:14,388) INFO - qlib.timer - [log.py:113] - Time cost: 20.930s | Init data Done\r\n[2607:MainThread](2022-04-06 17:38:14,399) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x2859099a0>\r\n[2607:MainThread](2022-04-06 17:38:14,402) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name train_model.\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:391, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    390 try:\r\n--> 391     exp = self.client.get_experiment_by_name(experiment_name)\r\n    392     if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:462, in MlflowClient.get_experiment_by_name(self, name)\r\n    432 \"\"\"\r\n    433 Retrieve an experiment by experiment name from the backend store\r\n    434 \r\n   (...)\r\n    460     Lifecycle_stage: active\r\n    461 \"\"\"\r\n--> 462 return self._tracking_client.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:167, in TrackingServiceClient.get_experiment_by_name(self, name)\r\n    163 \"\"\"\r\n    164 :param name: The experiment name.\r\n    165 :return: :py:class:`mlflow.entities.Experiment`\r\n    166 \"\"\"\r\n--> 167 return self.store.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     67 \"\"\"\r\n     68 Fetch the experiment by name from the backend store.\r\n     69 This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74 :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75 \"\"\"\r\n---> 76 for experiment in self.list_experiments(ViewType.ALL):\r\n     77     if experiment.name == experiment_name:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    259 try:\r\n    260     # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261     experiment = self._get_experiment(exp_id, view_type)\r\n    262     if experiment:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    336 self._check_root_dir()\r\n--> 337 _validate_experiment_id(experiment_id)\r\n    338 experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:189, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    187 try:\r\n    188     return (\r\n--> 189         self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),\r\n    190         False,\r\n    191     )\r\n    192 except ValueError:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:397, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    396 except MlflowException as e:\r\n--> 397     raise ValueError(\r\n    398         \"No valid experiment has been found, please make sure the input experiment name is correct.\"\r\n    399     ) from e\r\n\r\nValueError: No valid experiment has been found, please make sure the input experiment name is correct.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMlflowException                           Traceback (most recent call last)\r\nInput In [6], in <cell line: 51>()\r\n     48 dataset = init_instance_by_config(task[\"dataset\"])\r\n     50 # start exp to train model\r\n---> 51 with R.start(experiment_name=\"train_model\"):\r\n     52     R.log_params(**flatten_dict(task))\r\n     53     model.fit(dataset)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/contextlib.py:113, in _GeneratorContextManager.__enter__(self)\r\n    111 del self.args, self.kwds, self.func\r\n    112 try:\r\n--> 113     return next(self.gen)\r\n    114 except StopIteration:\r\n    115     raise RuntimeError(\"generator didn't yield\") from None\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:69, in QlibRecorder.start(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     25 @contextmanager\r\n     26 def start(\r\n     27     self,\r\n   (...)\r\n     34     resume: bool = False,\r\n     35 ):\r\n     36     \"\"\"\r\n     37     Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\r\n     38 \r\n   (...)\r\n     67         whether to resume the specific recorder with given name under the given experiment.\r\n     68     \"\"\"\r\n---> 69     run = self.start_exp(\r\n     70         experiment_id=experiment_id,\r\n     71         experiment_name=experiment_name,\r\n     72         recorder_id=recorder_id,\r\n     73         recorder_name=recorder_name,\r\n     74         uri=uri,\r\n     75         resume=resume,\r\n     76     )\r\n     77     try:\r\n     78         yield run\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:125, in QlibRecorder.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     84 def start_exp(\r\n     85     self,\r\n     86     *,\r\n   (...)\r\n     92     resume=False,\r\n     93 ):\r\n     94     \"\"\"\r\n     95     Lower level method for starting an experiment. When use this method, one should end the experiment manually\r\n     96     and the status of the recorder may not be handled properly. Here is the example code:\r\n   (...)\r\n    123     An experiment instance being started.\r\n    124     \"\"\"\r\n--> 125     return self.exp_manager.start_exp(\r\n    126         experiment_id=experiment_id,\r\n    127         experiment_name=experiment_name,\r\n    128         recorder_id=recorder_id,\r\n    129         recorder_name=recorder_name,\r\n    130         uri=uri,\r\n    131         resume=resume,\r\n    132     )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:339, in MLflowExpManager.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n    337 if experiment_name is None:\r\n    338     experiment_name = self._default_exp_name\r\n--> 339 experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\r\n    340 # Set up active experiment\r\n    341 self.active_experiment = experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:202, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    200 if pr.scheme == \"file\":\r\n    201     with FileLock(os.path.join(pr.netloc, pr.path, \"filelock\")):  # pylint: disable=E0110\r\n--> 202         return self.create_exp(experiment_name), True\r\n    203 # NOTE: for other schemes like http, we double check to avoid create exp conflicts\r\n    204 try:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:362, in MLflowExpManager.create_exp(self, experiment_name)\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n    361         raise ExpAlreadyExistError() from e\r\n--> 362     raise e\r\n    364 experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\r\n    365 experiment._default_name = self._default_exp_name\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:358, in MLflowExpManager.create_exp(self, experiment_name)\r\n    356 # init experiment\r\n    357 try:\r\n--> 358     experiment_id = self.client.create_experiment(experiment_name)\r\n    359 except MlflowException as e:\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:507, in MlflowClient.create_experiment(self, name, artifact_location, tags)\r\n    464 def create_experiment(\r\n    465     self,\r\n    466     name: str,\r\n    467     artifact_location: Optional[str] = None,\r\n    468     tags: Optional[Dict[str, Any]] = None,\r\n    469 ) -> str:\r\n    470     \"\"\"Create an experiment.\r\n    471 \r\n    472     :param name: The experiment name. Must be unique.\r\n   (...)\r\n    505         Lifecycle_stage: active\r\n    506     \"\"\"\r\n--> 507     return self._tracking_client.create_experiment(name, artifact_location, tags)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:182, in TrackingServiceClient.create_experiment(self, name, artifact_location, tags)\r\n    179 _validate_experiment_name(name)\r\n    180 _validate_experiment_artifact_location(artifact_location)\r\n--> 182 return self.store.create_experiment(\r\n    183     name=name,\r\n    184     artifact_location=artifact_location,\r\n    185     tags=[ExperimentTag(key, value) for (key, value) in tags.items()] if tags else [],\r\n    186 )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:321, in FileStore.create_experiment(self, name, artifact_location, tags)\r\n    319 def create_experiment(self, name, artifact_location=None, tags=None):\r\n    320     self._check_root_dir()\r\n--> 321     self._validate_experiment_name(name)\r\n    322     # Get all existing experiments and find the one with largest numerical ID.\r\n    323     # len(list_all(..)) would not work when experiments are deleted.\r\n    324     experiments_ids = [\r\n    325         int(e.experiment_id)\r\n    326         for e in self.list_experiments(ViewType.ALL)\r\n    327         if e.experiment_id.isdigit()\r\n    328     ]\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:303, in FileStore._validate_experiment_name(self, name)\r\n    299 if name is None or name == \"\":\r\n    300     raise MlflowException(\r\n    301         \"Invalid experiment name '%s'\" % name, databricks_pb2.INVALID_PARAMETER_VALUE\r\n    302     )\r\n--> 303 experiment = self.get_experiment_by_name(name)\r\n    304 if experiment is not None:\r\n    305     if experiment.lifecycle_stage == LifecycleStage.DELETED:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     66 def get_experiment_by_name(self, experiment_name):\r\n     67     \"\"\"\r\n     68     Fetch the experiment by name from the backend store.\r\n     69     This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74     :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75     \"\"\"\r\n---> 76     for experiment in self.list_experiments(ViewType.ALL):\r\n     77         if experiment.name == experiment_name:\r\n     78             return experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    258 for exp_id in rsl:\r\n    259     try:\r\n    260         # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261         experiment = self._get_experiment(exp_id, view_type)\r\n    262         if experiment:\r\n    263             experiments.append(experiment)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    335 def _get_experiment(self, experiment_id, view_type=ViewType.ALL):\r\n    336     self._check_root_dir()\r\n--> 337     _validate_experiment_id(experiment_id)\r\n    338     experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n    339     if experiment_dir is None:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    265 \"\"\"Check that `experiment_id`is a valid string or None, raise an exception if it isn't.\"\"\"\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. just rerun the code in my envirment\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\nDarwin\r\narm64\r\nmacOS-12.2.1-arm64-arm-64bit\r\nDarwin Kernel Version 21.3.0: Wed Jan  5 21:37:58 PST 2022; root:xnu-8019.80.24~20\/RELEASE_ARM64_T6000\r\n\r\nPython version: 3.8.11 (default, Jul 29 2021, 14:57:32)  [Clang 12.0.0 ]\r\n\r\nQlib version: 0.8.4.99\r\nnumpy==1.22.3\r\npandas==1.4.2\r\nscipy==1.8.0\r\nrequests==2.25.1\r\nsacred==0.8.2\r\npython-socketio==5.5.2\r\nredis==4.2.2\r\npython-redis-lock==3.7.0\r\nschedule==1.1.0\r\ncvxpy==1.1.18\r\nhyperopt==0.1.2\r\nfire==0.4.0\r\nstatsmodels==0.13.2\r\nxlrd==2.0.1\r\nplotly==5.6.0\r\nmatplotlib==3.5.1\r\ntables==3.7.0\r\npyyaml==6.0\r\nmlflow==1.24.0\r\ntqdm==4.61.2\r\nloguru==0.6.0\r\nlightgbm==3.3.2\r\ntornado==6.1\r\njoblib==1.1.0\r\nfire==0.4.0\r\nruamel.yaml==0.17.21\r\n\r\n\r\n## Additional Notes\r\n\r\nI installed qlib from source, and my conda env is the version for arm64\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: run the example workflow_by_code.ipynb, caused exception: invalid experiment id: '.ipynb_checkpoints' ; Content: ## \ud83d\udc1b bug description\r\nwhen i run the code below in qlib-main\/examples\/workflow_by_code.ipynb\uff0cit caused exception: invalid experiment id: '.ipynb_checkpoints' \r\n",
        "Issue_original_content_gpt_summary":"The user encountered an exception when running the code in qlib-main\/examples\/workflow_by_code.ipynb, resulting in an invalid experiment id of '.ipynb_checkpoints'.",
        "Issue_preprocessed_content":"Title: run the example caused exception invalid experiment id; Content: bug description when i run the code below in caused exception invalid experiment id train model task , , dataset , segments , , , model initiaiton model dataset start exp to train model with rid the whole error message is below mainthread , time cost loading data done mainthread , time cost dropnalabel done settingwithcopywarning a value is trying to be set on a copy of a slice from a dataframe. try using value instead see the caveats in the documentation df mainthread , time cost cszscorenorm done mainthread , time cost fit & process data done mainthread , time cost init data done mainthread , mainthread , warning no valid experiment found. create a new experiment with name exception traceback file in try > exp if exp is none or deleted file in name retrieve an experiment by experiment name from the backend store active > return file in name param name the experiment name. return > return file in fetch the experiment by name from the backend store. this is a base implementation using , derived classes may have return a single object if it exists. > for experiment in if file in try trap and warn known issues, will raise unexpected exceptions to caller > experiment if experiment file in > file in if is not none and is none > raise exception exception invalid experiment id the above exception was the direct cause of the following exception valueerror traceback file in try return , false, except valueerror file in except exception as e > raise valueerror from e valueerror no valid experiment has been found, please make sure the input experiment name is correct. during handling of the above exception, another exception occurred exception traceback input in , in start exp to train model > with file in del try > return except stopiteration raise runtimeerror from none file in uri, resume def start resume bool false, method to start an experiment. this method can only be called within a python's statement. here is the example code whether to resume the specific recorder with given name under the given experiment. > run uri uri, resume resume, try yield run file in uri, resume def self, , resume false, lower level method for starting an experiment. when use this method, one should end the experiment manually and the status of the recorder may not be handled properly. here is the example code an experiment instance being started. > return uri uri, resume resume, file in uri, resume if is none > experiment, _ set up active experiment experiment file in if file with filelock pylint disable e > return true note for other schemes like http, we double check to avoid create exp conflicts try file in if raise expalreadyexisterror file in init experiment try > except exception as e if file in name, tags def self, name str, optional none, tags optional none, > str create an experiment. param name the experiment name. must be unique. active > return tags file in name, tags > return name name, tags experimenttag for in if tags else , file in name, tags def name, tags none > get all existing experiments and find the one with largest numerical id. would not work when experiments are deleted. file in name if name is none or name raise exception > experiment if experiment is not none if file in def fetch the experiment by name from the backend store. this is a base implementation using , derived classes may have return a single object if it exists. > for experiment in if return experiment file in for in rsl try trap and warn known issues, will raise unexpected exceptions to caller > experiment if experiment file in def > if is none file in check that is a valid string or none, raise an exception if it if is not none and is none > raise exception exception invalid experiment id to reproduce steps to reproduce the behavior . just rerun the code in my envirment expected behavior screenshot environment note user could run under project directory to get system and paste them here directly. darwin arm darwin kernel version wed jan pst ; python version qlib version additional notes i installed qlib from source, and my conda env is the version for arm"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/369",
        "Issue_title":"Double Ensemble MlflowException",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1616595419000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve:\r\nMlflowException: Got invalid value Series([], dtype: float64) for metric 'IC' (timestamp=1616595157552). Please specify value as a valid double (64-bit floating point)\r\n\r\nif i have only sh000300 in my instruments, it's gonna produce the following value error:\r\nValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]).\r\nYou can drop duplicate edges by setting the 'duplicates' kwarg\r\n\r\ni followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this:\r\nqlib_init:\r\n    provider_uri: \"\/content\/gdrive\/MyDrive\/qlib\/qlib_data\/qlib_cn_1d\"\r\n    region: cn\r\nmarket: &market \r\nbenchmark: &benchmark SH000300\r\ndata_handler_config: &data_handler_config\r\n    start_time: 2008-01-01\r\n    end_time: 2021-03-01\r\n    fit_start_time: 2008-01-01\r\n    fit_end_time: 2018-12-31\r\n    instruments: ['SH000300', 'SH000903']\r\nport_analysis_config: &port_analysis_config\r\n    strategy:\r\n        class: TopkDropoutStrategy\r\n        module_path: qlib.contrib.strategy.strategy\r\n        kwargs:\r\n            topk: 50\r\n            n_drop: 5\r\n    backtest:\r\n        verbose: True\r\n        limit_threshold: 0.095\r\n        account: 50000\r\n        benchmark: *benchmark\r\n        deal_price: close\r\n        open_cost: 0.0005\r\n        close_cost: 0.0015\r\n        min_cost: 5\r\ntask:\r\n    model:\r\n        class: DEnsembleModel\r\n        module_path: qlib.contrib.model.double_ensemble\r\n        kwargs:\r\n            base_model: \"gbm\"\r\n            loss: mse\r\n            num_models: 6\r\n            enable_sr: True\r\n            enable_fs: True\r\n            alpha1: 1\r\n            alpha2: 1\r\n            bins_sr: 10\r\n            bins_fs: 5\r\n            decay: 0.5\r\n            sample_ratios:\r\n                - 0.8\r\n                - 0.7\r\n                - 0.6\r\n                - 0.5\r\n                - 0.4\r\n            sub_weights:\r\n                - 1\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n            epochs: 28\r\n            colsample_bytree: 0.8879\r\n            learning_rate: 0.2\r\n            subsample: 0.8789\r\n            lambda_l1: 205.6999\r\n            lambda_l2: 580.9768\r\n            max_depth: 8\r\n            num_leaves: 210\r\n            num_threads: 20\r\n            verbosity: -1\r\n    dataset:\r\n        class: DatasetH\r\n        module_path: qlib.data.dataset\r\n        kwargs:\r\n            handler:\r\n                class: Alpha158\r\n                module_path: qlib.contrib.data.handler\r\n                kwargs: *data_handler_config\r\n            segments:\r\n                train: [2008-01-01, 2018-12-31]\r\n                valid: [2019-01-01, 2020-07-31]\r\n                test: [2020-08-01, 2020-03-01]\r\n    record:\r\n        - class: SignalRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs: {}\r\n        - class: SigAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            ana_long_short: False\r\n            ann_scaler: 252\r\n        - class: PortAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            config: *port_analysis_config\r\n\r\nthanks for answering in advance.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: double ensemble exception; Content: hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve:\r\nexception: got invalid value series([], dtype: float64) for metric 'ic' (timestamp=1616595157552). please specify value as a valid double (64-bit floating point)\r\n\r\nif i have only sh000300 in my instruments, it's gonna produce the following value error:\r\nvalueerror: bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]).\r\nyou can drop duplicate edges by setting the 'duplicates' kwarg\r\n\r\ni followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this:\r\nqlib_init:\r\n    provider_uri: \"\/content\/gdrive\/mydrive\/qlib\/qlib_data\/qlib_cn_1d\"\r\n    region: cn\r\nmarket: &market \r\nbenchmark: &benchmark sh000300\r\ndata_handler_config: &data_handler_config\r\n    start_time: 2008-01-01\r\n    end_time: 2021-03-01\r\n    fit_start_time: 2008-01-01\r\n    fit_end_time: 2018-12-31\r\n    instruments: ['sh000300', 'sh000903']\r\nport_analysis_config: &port_analysis_config\r\n    strategy:\r\n        class: topkdropoutstrategy\r\n        module_path: qlib.contrib.strategy.strategy\r\n        kwargs:\r\n            topk: 50\r\n            n_drop: 5\r\n    backtest:\r\n        verbose: true\r\n        limit_threshold: 0.095\r\n        account: 50000\r\n        benchmark: *benchmark\r\n        deal_price: close\r\n        open_cost: 0.0005\r\n        close_cost: 0.0015\r\n        min_cost: 5\r\ntask:\r\n    model:\r\n        class: densemblemodel\r\n        module_path: qlib.contrib.model.double_ensemble\r\n        kwargs:\r\n            base_model: \"gbm\"\r\n            loss: mse\r\n            num_models: 6\r\n            enable_sr: true\r\n            enable_fs: true\r\n            alpha1: 1\r\n            alpha2: 1\r\n            bins_sr: 10\r\n            bins_fs: 5\r\n            decay: 0.5\r\n            sample_ratios:\r\n                - 0.8\r\n                - 0.7\r\n                - 0.6\r\n                - 0.5\r\n                - 0.4\r\n            sub_weights:\r\n                - 1\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n            epochs: 28\r\n            colsample_bytree: 0.8879\r\n            learning_rate: 0.2\r\n            subsample: 0.8789\r\n            lambda_l1: 205.6999\r\n            lambda_l2: 580.9768\r\n            max_depth: 8\r\n            num_leaves: 210\r\n            num_threads: 20\r\n            verbosity: -1\r\n    dataset:\r\n        class: dataseth\r\n        module_path: qlib.data.dataset\r\n        kwargs:\r\n            handler:\r\n                class: alpha158\r\n                module_path: qlib.contrib.data.handler\r\n                kwargs: *data_handler_config\r\n            segments:\r\n                train: [2008-01-01, 2018-12-31]\r\n                valid: [2019-01-01, 2020-07-31]\r\n                test: [2020-08-01, 2020-03-01]\r\n    record:\r\n        - class: signalrecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs: {}\r\n        - class: siganarecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            ana_long_short: false\r\n            ann_scaler: 252\r\n        - class: portanarecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            config: *port_analysis_config\r\n\r\nthanks for answering in advance.",
        "Issue_original_content_gpt_summary":"The user encountered multiple challenges while running a CN data on Google Colab, including an invalid value series error and a value error related to bin edges, which were solved by cloning the repo and installing via setup.py, uninstalling\/reinstalling numpy, and following instructions on the data collector's markdown page.",
        "Issue_preprocessed_content":"Title: double ensemble exception; Content: hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via and uninstalling \/ reinstalling numpy. but this one i do not know how to solve exception got invalid value series for metric 'ic' . please specify value as a valid double if i have only sh in my instruments, it's gonna produce the following value error valueerror bin edges must be unique array . you can drop duplicate edges by setting the 'duplicates' kwarg i followed instructions on data collector's markdown page to download cn data up until my yaml file looks like this region cn market &market benchmark &benchmark sh instruments strategy class topkdropoutstrategy kwargs topk backtest verbose true account benchmark benchmark close task model class densemblemodel kwargs gbm loss mse true true alpha alpha decay epochs subsample verbosity dataset class dataseth kwargs handler class alpha kwargs segments train valid test record class signalrecord kwargs class siganarecord kwargs false class portanarecord kwargs config thanks for answering in advance."
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Issue_title":"Neptune MalformedQueryException",
        "Issue_label":[
            "type:bug",
            "status:completed"
        ],
        "Issue_creation_time":1659310549000,
        "Issue_closed_time":1664069854000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  malformedqueryexception; Content: <!--- provide a general summary of the issue in the title above -->\r\n<!--- look through existing open and closed issues to see if someone has reported the issue before -->\r\ni started to use amundsen metadata with  database. initially i used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. so i tested it locally, using a vpn to connect to  db, and i found 2 problems. i'll do a pr linked to the issue that solves the problems\r\n## expected behavior\r\n<!--- tell us what should happen -->\r\nwhen calling a route of the metadata api for the  service, the server should respond without problem\r\n## current behavior\r\n<!--- tell us what happens instead of the expected behavior -->\r\n1. when calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. this error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. after the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedmessage\": \"failed to interpret gremlin query: query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"malformedqueryexception\",\r\n    \"requestid\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## possible solution\r\n<!--- not obligatory, but suggest a fix\/reason for the bug -->\r\n1. initialize `tornadotransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. move `order.decr`to `order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. the order.decr and order.incr are deprecated and don't work with \r\n## steps to reproduce\r\n<!--- provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. include code to reproduce, if relevant -->\r\n1. call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with aws  db\r\n## screenshots (if appropriate)\r\n\r\n## context\r\n<!--- how has this issue affected you? -->\r\n<!--- providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## your environment\r\n<!--- include as many relevant details about the environment you experienced the bug in -->\r\n* amunsen version used: last (metadata-3.10.0)\r\n* data warehouse stores: snowflake\r\n* deployment (k8s or native):\r\n* link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Issue_original_content_gpt_summary":"The user encountered two issues when using Amundsen Metadata with a database, resulting in 500 Internal Server Errors, which were solved by initializing the tornadotransport class properly and moving order.decr to order.desc in the gremlin_proxy.py file.",
        "Issue_preprocessed_content":"Title: malformedqueryexception; Content: i started to use amundsen metadata with database. initially i used the metadata docker image to interact with the database, but every tested route gave me a internal server error. so i tested it locally, using a vpn to connect to db, and i found problems. i'll do a pr linked to the issue that solves the problems expected behavior when calling a route of the metadata api for the service, the server should respond without problem current behavior . when calling the api to retrieve a table description, there's an error . this error has already be identified in . after the correction of , another error during the same request possible solution . initialize class properly, removing and in file . move to for and functions in file. the and are deprecated and don't work with steps to reproduce . call the metadata route using the gremlin metadata service with aws db screenshots context your environment amunsen version used last data warehouse stores snowflake deployment link to your fork or repository"
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Issue_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Issue_label":[
            "type:bug",
            "status:needs_reproducing",
            "status:needs_votes",
            "area:databuilder"
        ],
        "Issue_creation_time":1628591009000,
        "Issue_closed_time":1671067447000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: databuilder `bulkloaderapi` constructs wrong iam role arn for aws other than global; Content: for uploading data to aws  we use `csvpublisher`, which internally uses `bulkloaderapi`. the current configuration uses config key `csvpublisher.aws_iam_role_name`, which provides name of iam role for the loader to be able to use s3 and . the issue is that `bulkloaderapi` constructs iam role arn from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of arn aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -aws regions\r\n* `aws-cn` - china regions\r\n* `aws-us-gov` - aws govcloud (us) regions\r\n\r\nsince we use amundsen also in aws china, the above arn is not valid. \r\n\r\n## expected behavior\r\n\r\niam role arn either takes into account aws partition or there is a possibility of passing iam role arn instead of name directly.\r\n\r\n## current behavior\r\n\r\niam role arn is constructed incorrectly outside of aws global.\r\n\r\n## possible solutions\r\n\r\niam role arn should take partition into account. there are two solutions:\r\n1. add partition into current code\r\n2. add option of passing iam role arn directly which supersedes iam role name \r\n\r\n### solution 1\r\n\r\nsince i didn't know or found any good way to get the aws partition, we can use caller identity and arn there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['account']\r\npartition = identity['arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nthis is smaller fix but it is a bit hacky and i'm not sure it'll work in all situation, but it should i guess.\r\n\r\n### solution 2\r\n\r\nadd config key `csvpublisher.aws_iam_role_arn` which either supersedes `csvpublisher.aws_iam_role_name` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nor even replace `csvpublisher.aws_iam_role_name` with `csvpublisher.aws_iam_role_arn`, which is imo cleaner, but would be not backward compatible. \r\n\r\n## steps to reproduce\r\ndeploy amundsen in aws china with  and try to use databuilder to upload csv data from s3. \r\n\r\n## screenshots (if appropriate)\r\n\r\n## context\r\ncurrently we are unable to load data into  as the iam role arn setting is hidden and we get an error:\r\n\r\n```\r\n[error] exception: failed to load csv. response: {'detailedmessage': \"failed to start new load from the source s3:\/\/amundsenbucket\/amundsen\/2021_08_10_01_01_28. couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/rolefor111111-2222\", 'code': 'invalidparameterexception', 'requestid': 'xxx'}\r\ntraceback (most recent call last):\r\n\u00a0\u00a0file \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to__job.launch()\r\n\u00a0\u00a0file \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0file \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0file \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0file \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0file \"\/var\/task\/databuilder\/publisher\/_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise exception(\"failed to load csv. response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## your environment\r\n* amunsen version used: `amundsen-databuilder==4.3.1`\r\n* data warehouse stores: aws \r\n* deployment (k8s or native): aws step functions (k8s for backend but unrelated for now)\r\n* link to your fork or repository:",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the databuilder `bulkloaderapi` constructs an incorrect IAM role ARN for AWS other than global, preventing them from loading data into their data warehouse.",
        "Issue_preprocessed_content":"Title: databuilder constructs wrong iam role arn for aws other than global; Content: for uploading data to aws we use , which internally uses . the current configuration uses config key , which provides name of iam role for the loader to be able to use s and . the issue is that constructs iam role arn from name as follows whereas, can be currently aws regions china regions aws govcloud regions since we use amundsen also in aws china, the above arn is not valid. expected behavior iam role arn either takes into account aws partition or there is a possibility of passing iam role arn instead of name directly. current behavior iam role arn is constructed incorrectly outside of aws global. possible solutions iam role arn should take partition into account. there are two solutions . add partition into current code . add option of passing iam role arn directly which supersedes iam role name solution since i didn't know or found any good way to get the aws partition, we can use caller identity and arn there to get the partition, this is smaller fix but it is a bit hacky and i'm not sure it'll work in all situation, but it should i guess. solution add config key which either supersedes in a way that in constructor we would have something like or even replace with , which is imo cleaner, but would be not backward compatible. steps to reproduce deploy amundsen in aws china with and try to use databuilder to upload csv data from s . screenshots context currently we are unable to load data into as the iam role arn setting is hidden and we get an error your environment amunsen version used data warehouse stores aws deployment aws step functions link to your fork or repository"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Issue_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1635879966000,
        "Issue_closed_time":1635986654000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: configuration options not being set correctly when using cn region  endpoint as host; **describe the bug**\r\nthere are several areas in the code where we have an explicit check for the `.amazonaws.com` dns suffix; Content: this is used to determine if we need to use -specific configuration options and request uri elements. \r\n\r\nhowever, these checks misidentify endpoints of  clusters in aws cn regions, which use the `.<region>.amazonaws.com.cn` dns suffix instead, as non-aws endpoints. as a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nall of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#l160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/\/client.py#l129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#l54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#l14",
        "Issue_original_content_gpt_summary":"The user encountered a bug where configuration options were not being set correctly when using cn region endpoint as host, due to explicit checks for the `.amazonaws.com` dns suffix misidentifying endpoints of clusters in aws cn regions.",
        "Issue_preprocessed_content":"Title: configuration options not being set correctly when using cn region endpoint as host; Content: describe the bug there are several areas in the code where we have an explicit check for the dns suffix; this is used to determine if we need to use specific configuration options and request uri elements. however, these checks misidentify endpoints of clusters in aws cn regions, which use the dns suffix instead, as non aws endpoints. as a result, required config options such as and are not set correctly. all of the following checks need to be changed to"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Issue_title":"[BUG] Neptune ML Export widget throwing error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1627938048000,
        "Issue_closed_time":1628716798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  ml export widget throwing error; Content: **describe the bug**\r\nwhen using the  ml widget to export data like the command below from the 01- node classification notebook:\r\n```\r\n%%_ml export start --export-url {_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nthe following error is thrown\r\n```\r\n{\r\n  \"message\": \"credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**expected behavior**\r\nthe export should run to completion\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered an error when using the ML widget to export data from the 01- node classification notebook, where the expected behavior was for the export to run to completion.",
        "Issue_preprocessed_content":"Title: ml export widget throwing error; Content: describe the bug when using the ml widget to export data like the command below from the node classification notebook the following error is thrown expected behavior the export should run to completion"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Issue_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1626912970000,
        "Issue_closed_time":1632957644000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: limit issue .with(\"#ml.limit\",3); Content: hi\r\n\r\nas per the below code it is allowing only default limit as 1 and the limit 3 is not working and throwing error for introduction to node classification gremlin\r\n\r\n%%gremlin\r\ng.with(\"#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"#ml.limit\", 3 ).v().has('title', 'toy story (1995)').properties(\"genre\").with(\"#ml.classification\").value()\r\n\r\nerror\r\n{\r\n  \"requestid\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedmessage\": \"incompatible data from external service. please check your service configuration and query again.\",\r\n  \"code\": \"constraintviolationexception\"\r\n}\r\n\r\ncan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a limit issue with the code provided in the document, which only allowed a default limit of 1 and threw an error when attempting to use a limit of 3.",
        "Issue_preprocessed_content":"Title: limit issue; Content: hi as per the below code it is allowing only default limit as and the limit is not working and throwing error for introduction to node classification gremlin %%gremlin 'toy story error requestid fbab b a c f accc fc , detailedmessage incompatible data from external service. please check your service configuration and query code constraintviolationexception can some one suggest is there something wrong with the code which was mentioned in the document"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/116",
        "Issue_title":"[BUG] Neptune ML notebooks have incorrect Genre stated in the text",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1619195852000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"For the  01 notebooks for Neptune ML the text in the notebook incorrectly specifies that the genre returned for a node classification task on `Toy Story` is `Comedy` when it should be `Drama`",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  ml notebooks have incorrect genre stated in the text; Content: for the  01 notebooks for  ml the text in the notebook incorrectly specifies that the genre returned for a node classification task on `toy story` is `comedy` when it should be `drama`",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the genre stated in the ML notebooks for a node classification task on 'Toy Story' was incorrectly specified as 'Comedy' instead of 'Drama'.",
        "Issue_preprocessed_content":"Title: ml notebooks have incorrect genre stated in the text; Content: for the notebooks for ml the text in the notebook incorrectly specifies that the genre returned for a node classification task on is when it should be"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Issue_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1615509404000,
        "Issue_closed_time":1620330541000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":22.0,
        "Issue_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] _ml widget error in 2.0.9; Content: **describe the bug**\r\nstarting in version 2.0.9 the _ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': jsondecodeerror('expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. run through the 01-introduction-to-node-classification-gremlin notebook\r\n2. when you get to the export step the error occurs\r\n\r\n**additional context**\r\nthis is not a problem in version 2.0.7",
        "Issue_original_content_gpt_summary":"The user encountered a bug in version 2.0.9 of the _ml widget where json values were resulting in an error, which could be reproduced by running through the 01-introduction-to-node-classification-gremlin notebook and exporting at the end.",
        "Issue_preprocessed_content":"Title: widget error in; Content: describe the bug starting in version the widget is having an issue where the json values being passed in are getting the following error to reproduce steps to reproduce the behavior . run through the introduction to node classification gremlin notebook . when you get to the export step the error occurs additional context this is not a problem in version"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Issue_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Issue_label":[
            "bug",
            "documentation"
        ],
        "Issue_creation_time":1607104372000,
        "Issue_closed_time":1608658157000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] missing documentation on connecting to  from macos; Content: **describe the bug**\r\nthere are some missing details for how to connect to  from a macos device, we should add them to our doc on connecting to  via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/)\r\n\r\none main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**additional context**\r\nthis is coming from a bug report from connectivity not working as found in #40 ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with missing documentation on connecting to a database from a MacOS device, requiring the addition of a host alias to get things working properly.",
        "Issue_preprocessed_content":"Title: missing documentation on connecting to from macos; Content: describe the bug there are some missing details for how to connect to from a macos device, we should add them to our doc on connecting to via ssh tunnel found one main piece that we are missing is that a host alias needs to be made in order to get things working properly. additional context this is coming from a bug report from connectivity not working as found in"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Issue_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Issue_label":[
            "bug",
            "documentation"
        ],
        "Issue_creation_time":1606948478000,
        "Issue_closed_time":1607104425000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] no documentation on how to connect local notebook to remote  ssl; Content: **ssl connection to remote  not working**\r\ni am unable to figure out how can i specify the correct certificate sfsrootcag2.pem when running queries against ssl-enabled .\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. i set up ssh tunnel via bastion to the  cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -n -l 8182:yourendpoint:8182_'\r\n2. i start graph-notebook as '_jupyter notebook notebook\/destination__'. this gives me the output _jupyter notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. i open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"default\",\r\n  \"iam_credentials_provider_type\": \"role\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. i run the command \r\n_%%sparql        \r\nselect * where {?s ?p ?o} limit 1_\r\n\r\n5. it gives me the error\r\n**{'error': sslerror(maxretryerror('httpsconnectionpool(host=\\'localhost\\', port=8182): max retries exceeded with url: \/sparql (caused by sslerror(sslcertverificationerror(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**expected behavior**\r\ni expect to be able to connect to a remote  that has ssl enabled.\r\n\r\n**screenshots**\r\nnone\r\n\r\n**desktop (please complete the following information):**\r\n - macos 10.15.7 catalina\r\n - browser chrome\r\n - version 86.0.4240.198 (official build) (x86_64)\r\n\r\n**additional context**\r\nadd any other context about the problem here.none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to connect to a remote SSL-enabled cluster, as they were unable to figure out how to specify the correct certificate sfsrootcag2.pem when running queries.",
        "Issue_preprocessed_content":"Title: no documentation on how to connect local notebook to remote ssl; Content: ssl connection to remote not working i am unable to figure out how can i specify the correct certificate when running queries against ssl enabled . to reproduce steps to reproduce the behavior . i set up ssh tunnel via bastion to the cluster i ec user n l . i start graph notebook as notebook this gives me the output notebook is running at . i open my notebook and run the following magic commands host localhost , port , default , role , , , ssl true . i run the command select where limit . it gives me the error 'error' port max retries exceeded with url macos catalina browser chrome version additional context add any other context about the problem"
    },
    {
        "Issue_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Issue_title":"Neptune_catalyst.ipynb fails",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1624893279000,
        "Issue_closed_time":1625030635000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: _catalyst.ipynb fails; Content: seems that the _catalyst.ipynb is failing. \r\nperhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the _catalyst.ipynb failing, potentially due to a missing `run` object.",
        "Issue_preprocessed_content":"Title: fails; Content: seems that the is failing. perhaps there is some type as it seems to be missing the object."
    },
    {
        "Issue_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/57",
        "Issue_title":"[BUG] neptune minimal launcher link points to full launcher",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1625774761000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nNeptune guide's quicklaunch link points to the full launcher\r\n\r\n**Expected behavior**\r\nShould point to the minimal launcher\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  minimal launcher link points to full launcher; Content: **describe the bug**\r\n guide's quicklaunch link points to the full launcher\r\n\r\n**expected behavior**\r\nshould point to the minimal launcher\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the guide's quicklaunch link points to the full launcher instead of the minimal launcher, which is the expected behavior.",
        "Issue_preprocessed_content":"Title: minimal launcher link points to full launcher; Content: describe the bug guide's quicklaunch link points to the full launcher expected behavior should point to the minimal launcher"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/45",
        "Issue_title":"[BUG] AWS neptune templates for p3.2, p3.16 fail to start",
        "Issue_label":[
            "bug",
            "graphistry",
            "neptune",
            "aws"
        ],
        "Issue_creation_time":1615094175000,
        "Issue_closed_time":1615109545000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\n\r\nCloud formation for neptune fails on a p3.2 and p3.16 yet succeeds on a g4dn\r\n\r\nReported by a Neptune user\r\n\r\n**To Reproduce**\r\n\r\nRun through Neptune tutorial and use a p3.16\r\n\r\n**Expected behavior**\r\nIt launches\r\n\r\n**Actual behavior**\r\nFormation template stalls out and auto-deletes\r\n\r\nWorking on getting logs. After 10min, GPU services (forge-etl-python + streamgl) failed to start. V100 issue?\r\n\r\n**Screenshots**\r\n\r\n**Browser environment (please complete the following information):**\r\nall\r\n\r\n**PyGraphistry environment**\r\nAll\r\n\r\n**Additional context**\r\nCurrent graph-app-kit",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] aws  templates for p3.2, p3.16 fail to start; Content: **describe the bug**\r\n\r\ncloud formation for  fails on a p3.2 and p3.16 yet succeeds on a g4dn\r\n\r\nreported by a  user\r\n\r\n**to reproduce**\r\n\r\nrun through  tutorial and use a p3.16\r\n\r\n**expected behavior**\r\nit launches\r\n\r\n**actual behavior**\r\nformation template stalls out and auto-deletes\r\n\r\nworking on getting logs. after 10min, gpu services (forge-etl-python + streamgl) failed to start. v100 issue?\r\n\r\n**screenshots**\r\n\r\n**browser environment (please complete the following information):**\r\nall\r\n\r\n**pygraphistry environment**\r\nall\r\n\r\n**additional context**\r\ncurrent graph-app-kit",
        "Issue_original_content_gpt_summary":"The user encountered a bug where Cloud Formation for AWS fails on a p3.2 and p3.16 yet succeeds on a g4dn, with GPU services (forge-etl-python + streamgl) failing to start after 10 minutes.",
        "Issue_preprocessed_content":"Title: aws templates for fail to start; Content: describe the bug cloud formation for fails on a and yet succeeds on a g dn reported by a user to reproduce run through tutorial and use a expected behavior it launches actual behavior formation template stalls out and auto deletes working on getting logs. after min, gpu services failed to start. v issue? screenshots browser environment all pygraphistry environment all additional context current graph app kit"
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Issue_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Issue_label":[
            "bug",
            "air"
        ],
        "Issue_creation_time":1659034993000,
        "Issue_closed_time":1659051271000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [air] fix  \/\/doc\/source\/tune\/examples:_example; Content: ### what happened + what you expected to happen\n\nnotebook is broken due to missing permission first, maybe more issues down the road. @yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### versions \/ dependencies\n\nmaster\n\n### reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:_example`\n\n### issue severity\n\nmedium: it is a significant difficulty but i can work around it.",
        "Issue_original_content_gpt_summary":"The user encountered a medium severity issue with the notebook being broken due to missing permission, and is attempting to keep track of it by creating an issue.",
        "Issue_preprocessed_content":"Title: fix; Content: what happened + what you expected to happen notebook is broken due to missing permission first, maybe more issues down the road. looked into it earlier and we're creating this issue to keep track of it. versions \/ dependencies master reproduction script issue severity medium it is a significant difficulty but i can work around it."
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/24864",
        "Issue_title":"[tune] SigOptSearch suggester is not serialisable",
        "Issue_label":[
            "bug",
            "P2"
        ],
        "Issue_creation_time":1652740461000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### What happened + What you expected to happen\n\nI tried to run a ray tune job using the Sigopt suggester on a remote cluster. The sigopt suggester object was later found to be unserialisable however the stack trace gave no indication of this.\r\n\r\nThe stack trace looks like this\r\n\r\ndiscussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/CNECXMW22\/p1652417782100299\r\n\r\nThanks to Matthew Deng for finding the issue on this one!\n\n### Versions \/ Dependencies\n\nPython 3.8.12\r\nray==1.12.0\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport numpy as np\r\nimport os\r\nos.environ['SIGOPT_KEY'] = APIKEYHERE\r\n\r\nfrom ray.tune.suggest.sigopt import SigOptSearch\r\nfrom ray import tune\r\nWORKING_DIR = os.getcwd()\r\n\r\n\r\n\r\ndef main():\r\n\r\n\tray.init(\r\n\t\taddress = \"ray:\/\/127.0.0.1:10001\",\r\n\t\t# address = \"auto\",\r\n\t\truntime_env = {\r\n\t\t\t\"working_dir\": WORKING_DIR,\r\n\t\t\t\"pip\": [\"sigopt==5.7.0\"]\r\n\t\t}\r\n\t)\r\n\t\r\n\tn_observations = 20\r\n\r\n\thyperparameter_space = [\r\n          {\r\n              'name': 'learning_rate',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'max': np.log(0.01),\r\n                  'min': np.log(0.0001)\r\n              },\r\n          },\r\n          {\r\n              'name': 'momentum',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'min': 0.85,\r\n                  'max': 0.99\r\n              },\r\n          },\r\n      ]\r\n\t\r\n\tsigopt_search = SigOptSearch(\r\n\t\t# OmegaConf.to_container(config.search_space),\r\n        hyperparameter_space,\r\n\t\tname=\"Tune distributed\",\r\n\t\tmax_concurrent=2, \r\n\t\tobservation_budget=n_observations,\r\n\t\tproject=\"sigopt-ray-integration\",\r\n\t\tmetric=[\"val_loss\"],\r\n\t\tmode=[\"min\"]\r\n\t\t# metric=[\"val_loss\", \"training_loss\"],\r\n\t\t# mode=[\"max\", \"min\"]\r\n\t)\r\n\r\n\ttune_config = {\r\n\t\t# \"config\": config\r\n\t}\r\n\tanalysis = tune.run(\r\n\t\ttrain_model,\r\n\t\tmetric=\"val_loss\",\r\n\t\tmode=\"min\",\r\n\t\tconfig=tune_config,\r\n\t\tnum_samples=n_observations,\r\n\t\tname=\"Tune distributed\",\r\n\t\tresources_per_trial={'gpu': 1},\r\n\t\tsearch_alg=sigopt_search,\r\n\t\t# scheduler=FIFOScheduler(),\r\n\t)\r\n\r\n\r\n\r\n\r\ndef train_model(config):\r\n    pass\r\n\r\nmain()\r\n\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [tune] search suggester is not serialisable; Content: ### what happened + what you expected to happen\n\ni tried to run a ray tune job using the  suggester on a remote cluster. the  suggester object was later found to be unserialisable however the stack trace gave no indication of this.\r\n\r\nthe stack trace looks like this\r\n\r\ndiscussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/cnecxmw22\/p1652417782100299\r\n\r\nthanks to matthew deng for finding the issue on this one!\n\n### versions \/ dependencies\n\npython 3.8.12\r\nray==1.12.0\n\n### reproduction script\n\n```\r\nimport ray\r\nimport numpy as np\r\nimport os\r\nos.environ['_key'] = apikeyhere\r\n\r\nfrom ray.tune.suggest. import search\r\nfrom ray import tune\r\nworking_dir = os.getcwd()\r\n\r\n\r\n\r\ndef main():\r\n\r\n\tray.init(\r\n\t\taddress = \"ray:\/\/127.0.0.1:10001\",\r\n\t\t# address = \"auto\",\r\n\t\truntime_env = {\r\n\t\t\t\"working_dir\": working_dir,\r\n\t\t\t\"pip\": [\"==5.7.0\"]\r\n\t\t}\r\n\t)\r\n\t\r\n\tn_observations = 20\r\n\r\n\thyperparameter_space = [\r\n          {\r\n              'name': 'learning_rate',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'max': np.log(0.01),\r\n                  'min': np.log(0.0001)\r\n              },\r\n          },\r\n          {\r\n              'name': 'momentum',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'min': 0.85,\r\n                  'max': 0.99\r\n              },\r\n          },\r\n      ]\r\n\t\r\n\t_search = search(\r\n\t\t# omegaconf.to_container(config.search_space),\r\n        hyperparameter_space,\r\n\t\tname=\"tune distributed\",\r\n\t\tmax_concurrent=2, \r\n\t\tobservation_budget=n_observations,\r\n\t\tproject=\"-ray-integration\",\r\n\t\tmetric=[\"val_loss\"],\r\n\t\tmode=[\"min\"]\r\n\t\t# metric=[\"val_loss\", \"training_loss\"],\r\n\t\t# mode=[\"max\", \"min\"]\r\n\t)\r\n\r\n\ttune_config = {\r\n\t\t# \"config\": config\r\n\t}\r\n\tanalysis = tune.run(\r\n\t\ttrain_model,\r\n\t\tmetric=\"val_loss\",\r\n\t\tmode=\"min\",\r\n\t\tconfig=tune_config,\r\n\t\tnum_samples=n_observations,\r\n\t\tname=\"tune distributed\",\r\n\t\tresources_per_trial={'gpu': 1},\r\n\t\tsearch_alg=_search,\r\n\t\t# scheduler=fifoscheduler(),\r\n\t)\r\n\r\n\r\n\r\n\r\ndef train_model(config):\r\n    pass\r\n\r\nmain()\r\n\r\n```\n\n### issue severity\n\nmedium: it is a significant difficulty but i can work around it.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running a Ray Tune job using the search suggester on a remote cluster, as the suggester object was found to be unserialisable, resulting in a medium severity issue.",
        "Issue_preprocessed_content":"Title: search suggester is not serialisable; Content: what happened + what you expected to happen i tried to run a ray tune job using the suggester on a remote cluster. the suggester object was later found to be unserialisable however the stack trace gave no indication of this. the stack trace looks like this discussion around this issue can be found here thanks to matthew deng for finding the issue on this one! versions \/ dependencies python reproduction script issue severity medium it is a significant difficulty but i can work around it."
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Issue_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Issue_label":[
            "bug",
            "triage"
        ],
        "Issue_creation_time":1603479422000,
        "Issue_closed_time":1603498330000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [tune]  (multi-metric) api fails with 1.1.0 (tries to hash list); Content: <!--please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nif you run \r\n\r\npy_test(\r\n name = \"_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a  api key...)\r\nyou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  file \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  file \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\ntypeerror: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### reproduction (required)\r\nin python\/ray\/tune\/build  run the  sections that are commented out.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when running a multi-metric API in ray 1.1.0.dev, which attempted to hash a list and failed.",
        "Issue_preprocessed_content":"Title: api fails with; Content: if you run name size medium , srcs , deps , tags , args in you get an output that looks like this file line , in file line , in elif and not in result typeerror unhashable type 'list' ray reproduction in run the sections that are commented out."
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Issue_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1657875526000,
        "Issue_closed_time":1658150380000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: the  api is outdated in transformers trainer.py, the old api could not work; Content: ### system info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- platform: linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- python version: 3.8.10\r\n- huggingface_hub version: 0.7.0\r\n- pytorch version (gpu?): 1.11.0+cu113 (true)\r\n- tensorflow version (gpu?): 2.9.1 (false)\r\n- flax version (cpu?\/gpu?\/tpu?): 0.5.0 (cpu)\r\n- jax version: 0.3.6\r\n- jaxlib version: 0.3.5\r\n- using gpu in script?: <fill in>\r\n- using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### who can help?\r\n\r\n@sgugger \r\n\r\n### information\r\n\r\n- [ ] the official example scripts\r\n- [ ] my own modified scripts\r\n\r\n### tasks\r\n\r\n- [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...)\r\n- [ ] my own task or dataset (give details below)\r\n\r\n### reproduction\r\n\r\n1.enable  hpo in example and run.\r\n2. work log like\"userwarning: you're currently using the old  experience. try out the new and improved  experience by getting started with the docs today. you have until july 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### expected behavior\r\n\r\nhpo with  backend could work correctly without warning",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the outdated API in the transformers trainer.py, which caused a warning when trying to enable HPO in the example and run.",
        "Issue_preprocessed_content":"Title: the api is outdated in transformers the old api could not work; Content: system version platform python version version pytorch version tensorflow version flax version jax version jaxlib version using gpu in script? using distributed or parallel set up in script? who can help? the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction .enable hpo in example and run. . work log like userwarning you're currently using the old experience. try out the new and improved experience by getting started with the docs today. you have until july to migrate over without experiencing breaking expected behavior hpo with backend could work correctly without warning"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/1315",
        "Issue_title":"Unable to import aiplatform module when running Vertex AI Matching Engine sample notebook",
        "Issue_label":[
            "type: bug",
            "priority: p2"
        ],
        "Issue_creation_time":1669999872000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Expected Behavior\r\nI expect the notebook here https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/matching_engine\/sdk_matching_engine_for_indexing.ipynb to work\r\n\r\n\r\n\r\n## Actual Behavior\r\n\r\n, but for some reason whenever I try to import the module `aiplatform` inside a cell of the notebook\r\n```\r\nfrom google.cloud import aiplatform\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/protobuf\/descriptor.py in __new__(cls, name, index, number, type, options, serialized_options, create_key)\r\n    753                 type=None,  # pylint: disable=redefined-builtin\r\n    754                 options=None, serialized_options=None, create_key=None):\r\n--> 755       _message.Message._CheckCalledFromGeneratedFile()\r\n    756       # There is no way we can build a complete EnumValueDescriptor with the\r\n    757       # given parameters (the name of the Enum is not known, for example).\r\n\r\nTypeError: Descriptors cannot not be created directly.\r\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\r\n\r\nMore information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\r\n```\r\n\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1. Clone the sample notebook\r\n1. Import it into a vertex AI Workbench running the Python3 image\r\n1. Try to run through the steps and get stuck in installation issues\r\n\r\n## Specifications\r\n\r\n- Version:\r\n- Platform:",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: unable to import aiplatform module when running  matching engine sample notebook; Content: ## expected behavior\r\ni expect the notebook here https:\/\/github.com\/googlecloudplatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/matching_engine\/sdk_matching_engine_for_indexing.ipynb to work\r\n\r\n\r\n\r\n## actual behavior\r\n\r\n, but for some reason whenever i try to import the module `aiplatform` inside a cell of the notebook\r\n```\r\nfrom google.cloud import aiplatform\r\n```\r\n\r\ni get the following error:\r\n\r\n```\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/protobuf\/descriptor.py in __new__(cls, name, index, number, type, options, serialized_options, create_key)\r\n    753                 type=none,  # pylint: disable=redefined-builtin\r\n    754                 options=none, serialized_options=none, create_key=none):\r\n--> 755       _message.message._checkcalledfromgeneratedfile()\r\n    756       # there is no way we can build a complete enumvaluedescriptor with the\r\n    757       # given parameters (the name of the enum is not known, for example).\r\n\r\ntypeerror: descriptors cannot not be created directly.\r\nif this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nif you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. downgrade the protobuf package to 3.20.x or lower.\r\n 2. set protocol_buffers_python_implementation=python (but this will use pure-python parsing and will be much slower).\r\n\r\nmore information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\r\n```\r\n\r\n\r\n## steps to reproduce the problem\r\n\r\n1. clone the sample notebook\r\n1. import it into a  workbench running the python3 image\r\n1. try to run through the steps and get stuck in installation issues\r\n\r\n## specifications\r\n\r\n- version:\r\n- platform:",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to import the aiplatform module when running the matching engine sample notebook, resulting in an error due to an outdated version of the protobuf package.",
        "Issue_preprocessed_content":"Title: unable to import aiplatform module when running matching engine sample notebook; Content: expected behavior i expect the notebook here to work actual behavior , but for some reason whenever i try to import the module inside a cell of the notebook i get the following error steps to reproduce the problem . clone the sample notebook . import it into a workbench running the python image . try to run through the steps and get stuck in installation issues specifications version platform"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/349",
        "Issue_title":"ModelUploadOp from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\"  does not work",
        "Issue_label":[
            "type: bug"
        ],
        "Issue_creation_time":1646182369000,
        "Issue_closed_time":1646371495000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Expected Behavior\r\nCode example  from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## Actual Behavior\r\nCode example below from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # THIS IS THE METHOD THAT DOESN'T APPEAR TO WORK\r\n    model_upload_op = gcc_aip.ModelUploadOp(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\nOn the other hand, the method below worked:\r\n```\r\n # THIS METHOD DOES WORK\r\n    # aiplatform.Model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\nI'm currently using Vertex AI Pipelines to train a model and upload to Vertex AI. Currently in the pipeline, I'm attempting to use the ModelUploadOp class to upload a custom model to Vertex AI models. The logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## Specifications\r\n\r\nVersion: \r\n- Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp\r\n- Pipelines Version: kfp==1.8.11\r\n- Platform: Google Cloud Vertex AI \r\n\r\n[1]: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: modeluploadop from \" pipelines: model upload using google-cloud-pipeline-components\"  does not work; Content: ## expected behavior\r\ncode example  from \" pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## actual behavior\r\ncode example below from \" pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # this is the method that doesn't appear to work\r\n    model_upload_op = gcc_aip.modeluploadop(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\non the other hand, the method below worked:\r\n```\r\n # this method does work\r\n    # aiplatform.model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\ni'm currently using  pipelines to train a model and upload to . currently in the pipeline, i'm attempting to use the modeluploadop class to upload a custom model to  models. the logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## steps to reproduce the problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## specifications\r\n\r\nversion: \r\n- pipeline sdk (kubeflow pipelines\/tfx) version: kfp\r\n- pipelines version: kfp==1.8.11\r\n- platform: google cloud  \r\n\r\n[1]: https:\/\/github.com\/googlecloudplatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the modeluploadop from the \"pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" code example, where the job was succeeding but the model was not being uploaded.",
        "Issue_preprocessed_content":"Title: modeluploadop from pipelines model upload using google cloud pipeline components does not work; Content: expected behavior code example from pipelines model train, upload, and deploy using google cloud pipeline components should work as intended. actual behavior code example below from pipelines model train, upload, and deploy using google cloud pipeline components had issue and does not work on the other hand, the method below worked i'm currently using pipelines to train a model and upload to . currently in the pipeline, i'm attempting to use the modeluploadop class to upload a custom model to models. the logs show the job is succeeding, but the model never actually gets uploaded. steps to reproduce the problem . . . specifications version pipeline sdk version kfp pipelines version platform google cloud"
    },
    {
        "Issue_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Issue_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666626693000,
        "Issue_closed_time":1667472145000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]:  blogpost is outdated after 0.20.0 release; Content: ### contact details [optional]\n\nfrancogbocci@gmail.com\n\n### system information\n\nzenml version: 0.20.5\r\ninstall path: \/users\/f.bocci\/library\/caches\/pypoetry\/virtualenvs\/banana-bmsm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\npython version: 3.9.6\r\nplatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nenvironment: native\r\nintegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### what happened?\n\ntrying to follow the [guide to run a pipeline using ](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because zenml does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nstack components:\r\n      alerter                 commands to interact with alerters.\r\n      annotator               commands to interact with annotators.\r\n      artifact-store          commands to interact with artifact stores.\r\n      container-registry      commands to interact with container registries.\r\n      data-validator          commands to interact with data validators.\r\n      experiment-tracker      commands to interact with experiment trackers.\r\n      feature-store           commands to interact with feature stores.\r\n      model-deployer          commands to interact with model deployers.\r\n      orchestrator            commands to interact with orchestrators.\r\n      secrets-manager         commands to interact with secrets managers.\r\n      step-operator           commands to interact with step operators.\r\n$ zenml metadata-store\r\nerror: no such command 'metadata-store'.\r\n```\n\n### reproduction steps\n\n1. zenml metadata-store\r\n\r\nif i don't add it and run the  pipeline, it fails.\r\n\n\n### relevant log output\n\n_no response_\n\n### code of conduct\n\n- [x] i agree to follow this project's code of conduct",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to follow a guide to run a pipeline using ZenML, as the command 'metadata-store' was not recognized due to the 0.20.0 release.",
        "Issue_preprocessed_content":"Title: blogpost is outdated after release; Content: contact details system zenml version install path python version platform environment native integrations what happened? trying to follow the , it fails because zenml does not now have a stack category. reproduction steps . zenml metadata store if i don't add it and run the pipeline, it fails. relevant log output code of conduct i agree to follow this project's code of conduct"
    },
    {
        "Issue_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Issue_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Issue_label":[
            "type: bug",
            "priority: p2",
            "api: vertex-ai"
        ],
        "Issue_creation_time":1664933940000,
        "Issue_closed_time":1664935217000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  endpoint prediction error, 4 deadline_exceeded: deadline exceeded; Content: #### environment details\r\n\r\n  - os: mac m1 pro\r\n  - node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### steps to reproduce\r\n\r\n  1. i've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. the process paused and shows `4 deadline_exceeded: deadline exceeded` in the line: `await predictionserviceclient.predict(request);`\r\n\r\n\r\nthanks!\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with endpoint prediction error, resulting in a \"4 deadline_exceeded: deadline exceeded\" error, when running a demo on their local computer.",
        "Issue_preprocessed_content":"Title: endpoint prediction error, deadline exceeded; Content: environment details os mac m pro version npm version version steps to reproduce . i've run this demo on my local computer . the process paused and shows in the line thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/issues\/171",
        "Issue_title":"[Bug] scikit learn model feature definition doesn't work on Vertex AI Prediction.",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1646626569000,
        "Issue_closed_time":1648259081000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nScikit Learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#L46) doesn't work in Vertex AI prediction environment, since it assumes the input as Pandas Dataframe and cannot handle JSON from Web API.\r\n\r\nAfter deploying the model following the labs, this issue can be reproduced with this code snippet.\r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [{'Elevation': [2841.0]},\r\n {'Aspect': [45, 0]},\r\n {'Slope': [0, 0]},\r\n {'Horizontal_Distance_To_Hydrology': [644.0]},\r\n {'Vertical_Distance_To_Hydrology': [282.0]},\r\n {'Horizontal_Distance_To_Roadways': [1376.0]},\r\n {'Hillshade_9am': [218.0]},\r\n {'Hillshade_Noon': [237.0]},\r\n {'Hillshade_3pm': [156.0]},\r\n {'Horizontal_Distance_To_Fire_Points': [1003.0]},\r\n {'Wilderness_Area': ['Commanche']},\r\n {'Soil_Type': ['C4758']}]\r\n\r\nendpoint.predict([instance])\r\n```\r\n\r\nreturns:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png)\r\n\r\n## Approach\r\nRewrite feature definition part of `train.py` from:\r\nhttps:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#L43-L63\r\n\r\nto:\r\n```python\r\n    numeric_feature_indexes = slice(0, 10)\r\n    categorical_feature_indexes = slice(10, 12)\r\n\r\n    preprocessor = ColumnTransformer(\r\n    transformers=[\r\n        ('num', StandardScaler(), numeric_feature_indexes),\r\n        ('cat', OneHotEncoder(), categorical_feature_indexes) \r\n    ])\r\n```\r\n\r\nAnd it should run with this \r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [\r\n    2841.0,\r\n    45.0,\r\n    0.0,\r\n    644.0,\r\n    282.0,\r\n    1376.0,\r\n    218.0,\r\n    237.0,\r\n    156.0,\r\n    1003.0,\r\n    \"Commanche\",\r\n    \"C4758\",\r\n]\r\nendpoint.predict([instance])\r\n```\r\n\r\nOutput:\r\n```\r\nPrediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=None)\r\n```\r\n\r\n## Target Files\r\n[These 8 files ](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/search?q=numeric_features+%3D+%5B+++++++++%22Elevation%22%2C) should be update.",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] scikit learn model feature definition doesn't work on  prediction.; Content: ## description\r\n\r\nscikit learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#l46) doesn't work in  prediction environment, since it assumes the input as pandas dataframe and cannot handle json from web api.\r\n\r\nafter deploying the model following the labs, this issue can be reproduced with this code snippet.\r\n\r\n```python\r\nendpoint = aiplatform.endpoint.list()[0]\r\n\r\ninstance = [{'elevation': [2841.0]},\r\n {'aspect': [45, 0]},\r\n {'slope': [0, 0]},\r\n {'horizontal_distance_to_hydrology': [644.0]},\r\n {'vertical_distance_to_hydrology': [282.0]},\r\n {'horizontal_distance_to_roadways': [1376.0]},\r\n {'hillshade_9am': [218.0]},\r\n {'hillshade_noon': [237.0]},\r\n {'hillshade_3pm': [156.0]},\r\n {'horizontal_distance_to_fire_points': [1003.0]},\r\n {'wilderness_area': ['commanche']},\r\n {'soil_type': ['c4758']}]\r\n\r\nendpoint.predict([instance])\r\n```\r\n\r\nreturns:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png)\r\n\r\n## approach\r\nrewrite feature definition part of `train.py` from:\r\nhttps:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#l43-l63\r\n\r\nto:\r\n```python\r\n    numeric_feature_indexes = slice(0, 10)\r\n    categorical_feature_indexes = slice(10, 12)\r\n\r\n    preprocessor = columntransformer(\r\n    transformers=[\r\n        ('num', standardscaler(), numeric_feature_indexes),\r\n        ('cat', onehotencoder(), categorical_feature_indexes) \r\n    ])\r\n```\r\n\r\nand it should run with this \r\n\r\n```python\r\nendpoint = aiplatform.endpoint.list()[0]\r\n\r\ninstance = [\r\n    2841.0,\r\n    45.0,\r\n    0.0,\r\n    644.0,\r\n    282.0,\r\n    1376.0,\r\n    218.0,\r\n    237.0,\r\n    156.0,\r\n    1003.0,\r\n    \"commanche\",\r\n    \"c4758\",\r\n]\r\nendpoint.predict([instance])\r\n```\r\n\r\noutput:\r\n```\r\nprediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=none)\r\n```\r\n\r\n## target files\r\n[these 8 files ](https:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/search?q=numeric_features+%3d+%5b+++++++++%22elevation%22%2c) should be update.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the scikit learn model feature definition in the kubeflow_pipelines\/pipelines directory did not work on prediction, due to the input being a pandas dataframe and not being able to handle json from web api.",
        "Issue_preprocessed_content":"Title: scikit learn model feature definition doesn't work on prediction.; Content: description scikit learn model in doesn't work in prediction environment, since it assumes the input as pandas dataframe and cannot handle json from web api. after deploying the model following the labs, this issue can be reproduced with this code snippet. returns approach rewrite feature definition part of from to and it should run with this output target files these files should be update."
    },
    {
        "Issue_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/287",
        "Issue_title":"wandb RuntimeError",
        "Issue_label":[
            "bug",
            "stale"
        ],
        "Issue_creation_time":1585341922000,
        "Issue_closed_time":1591178971000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"When training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and wandb enabled:\r\n```\r\nFile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  runtimeerror; Content: when training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and  enabled:\r\n```\r\nfile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  file \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  file \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  file \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  file \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=true)  # allow_unreachable flag\r\n  file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nruntimeerror: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). use .reshape(...) instead.\r\n```\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a runtimeerror when training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and enabled, due to an incompatibility between the view size and the input tensor's size and stride.",
        "Issue_preprocessed_content":"Title: runtimeerror; Content: when training text classification models using xlnet large cased, albert base v , xlnet base cased and enabled"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Issue_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Issue_label":[
            "bug",
            "dev"
        ],
        "Issue_creation_time":1654717842000,
        "Issue_closed_time":1657703576000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] weird behavior with \"to_\" and confusion matrix; Content: **describe the bug**\r\nafter running a model evaluation suite and exprorint to  using \"to_\" function, the confusion matrix appears in the w&b page without the values\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n\r\n\r\n**expected behavior**\r\nthe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**environment (please complete the following information):**\r\n - os: linux\r\n - python version:3.7.1\r\n - deepchecks version:0.7.2\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the confusion matrix appears in the Weights & Biases page without the values after running a model evaluation suite and exporting to using the \"to_\" function.",
        "Issue_preprocessed_content":"Title: weird behavior with and confusion matrix; Content: describe the bug after running a model evaluation suite and exprorint to using function, the confusion matrix appears in the w&b page without the values to reproduce steps to reproduce the behavior expected behavior the confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown environment os linux python deepchecks"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1210",
        "Issue_title":"[BUG] to_wandb not sectioning by train\/test and overrides runs by checks",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1649320792000,
        "Issue_closed_time":1649580744000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\n to_wandb not sectioning by train\/test and overrides runs by checks\r\n\r\n**To Reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**Expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug] to_ not sectioning by train\/test and overrides runs by checks; Content: **describe the bug**\r\n to_ not sectioning by train\/test and overrides runs by checks\r\n\r\n**to reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**screenshots**\r\nif applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the 'to_' function was not sectioning by train\/test and overriding runs by checks, resulting in an inability to run a suite with a couple of checks.",
        "Issue_preprocessed_content":"Title: not sectioning by and overrides runs by checks; Content: describe the bug not sectioning by and overrides runs by checks to reproduce run a suite with checks and duplicate checks in suite expected behavior sections for each dataset and being able to run a suite with a couple of checks screenshots if applicable, add screenshots to help explain your problem."
    },
    {
        "Issue_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Issue_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Issue_label":[
            "waiting feedback"
        ],
        "Issue_creation_time":1583449225000,
        "Issue_closed_time":1583456108000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  logging does not differentiate between modes; Content: logging using  does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Issue_original_content_gpt_summary":"The user encountered a bug where logging using logbook does not differentiate between training and testing modes.",
        "Issue_preprocessed_content":"Title: logging does not differentiate between modes; Content: logging using does not differentiate between training and testing modes in"
    },
    {
        "Issue_link":"https:\/\/github.com\/vlievin\/fz-openqa\/issues\/216",
        "Issue_title":"Wandb logging: auxiliary values (e.g. `retriever\/entropy`) are not logged",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642504357000,
        "Issue_closed_time":1645097047000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logging: auxiliary values (e.g. `retriever\/entropy`) are not logged; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where auxiliary values such as `retriever\/entropy` were not being logged.",
        "Issue_preprocessed_content":"Title: logging auxiliary values are not logged; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Issue_title":"Add episode to wandb",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1605623692000,
        "Issue_closed_time":1605698611000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: add episode to ; Content: when using , it shows step as x and not episode.\r\n\r\nhence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using a tool that showed the step as x and not episode, making it difficult to compare longer runs.",
        "Issue_preprocessed_content":"Title: add episode to; Content: when using , it shows step as x and not episode. hence, longer runs have more steps and it makes the comparaison between runs difficult."
    },
    {
        "Issue_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Issue_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642918837000,
        "Issue_closed_time":1644017877000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  callback prints errors when a training run resumes not from scratch; it prints\r\n```\r\n: warning step must only increase in log calls.  step 110 < 161; Content: dropping\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where callback prints errors when a training run resumes not from scratch, resulting in a warning that the step must only increase in log calls.",
        "Issue_preprocessed_content":"Title: callback prints errors when a training run resumes not from scratch; Content: it prints"
    },
    {
        "Issue_link":"https:\/\/github.com\/allenai\/tango\/issues\/151",
        "Issue_title":"WandB callback changes the train step's unique ID, but does not change the results",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642916994000,
        "Issue_closed_time":1652740320000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  callback changes the train step's unique id, but does not change the results; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the callback changed the train step's unique id, but did not change the results.",
        "Issue_preprocessed_content":"Title: callback changes the train step's unique id, but does not change the results; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Issue_title":"Resume error in WandB.",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1649286157000,
        "Issue_closed_time":1649731891000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: resume error in .; Content: forgot to create an issue in recent days.\r\nwhen tested with ```resume``` argument in ```callbacks```, i encountered this error. here's the log:\r\n```python\r\n\r\n[errno 2] no such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | debug    | opt.py:override:78 - overriding configuration...\r\n2022-04-04 12:21:56 | info     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"supervisedtrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"basetimmmodel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"focalloss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"loggercallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"checkpointcallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"visualizercallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"tensorboardcallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"callbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"balancedaccuracymetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"f1scoremetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"confusionmatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"errorcases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"adamw\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"schedulerwrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"imagefolderdataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"imagefolderdataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"dataloaderwithcollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"mixupcutmixcollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"balancesampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"dataloaderwithcollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | debug    | opt.py:load_yaml:36 - loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | debug    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - calculating class distribution...\r\ndownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\ntraceback (most recent call last):\r\n  file \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = pipeline(opts)\r\n  file \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=callbacks_registry\r\n  file \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  file \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  file \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\ntypeerror: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\ni guess because of the ```resume``` arg is both repeated in ```global``` and ```callbacks```. maybe it also happens with ```tensorboard```.",
        "Issue_original_content_gpt_summary":"The user encountered an error when testing with the ```resume``` argument in ```callbacks```, due to the argument being repeated in both ```global``` and ```callbacks```.",
        "Issue_preprocessed_content":"Title: resume error in .; Content: forgot to create an issue in recent days. when tested with argument in , i encountered this error. here's the log i guess because of the arg is both repeated in and . maybe it also happens with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kuielab\/mdx-net\/issues\/15",
        "Issue_title":"wandb only logs top k val loss",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1625581852000,
        "Issue_closed_time":1625620015000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  only logs top k val loss",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of only logging the top k values of the loss function during training.",
        "Issue_preprocessed_content":"Title: only logs top k val loss"
    },
    {
        "Issue_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/9",
        "Issue_title":"Fix writing of checkpoints to wandb",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1624867819000,
        "Issue_closed_time":1624957138000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## What\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n## How to reproduce\r\n\r\nReproduce by starting a non-dry run via a notebook\r\n\r\n1. Start the run\r\n2. Look at files on the wandb interface. There are no checkpoints\r\n\r\n## Expected\r\n\r\nCheckpoints should be uploaded to wandb whenever there is a better one available during training.\r\n\r\n## Additional context\r\n\r\nI thought I fixed wandb, but it seems that I don't understand the symlinking model of wandb. Apparently you need to have checkpoints under the project root? But this would mean that you can't run multiple experiements at the same time. ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fix writing of checkpoints to ; Content: ## what\r\n\r\na clear and concise description of what the bug is.\r\n\r\n## how to reproduce\r\n\r\nreproduce by starting a non-dry run via a notebook\r\n\r\n1. start the run\r\n2. look at files on the  interface. there are no checkpoints\r\n\r\n## expected\r\n\r\ncheckpoints should be uploaded to  whenever there is a better one available during training.\r\n\r\n## additional context\r\n\r\ni thought i fixed , but it seems that i don't understand the symlinking model of . apparently you need to have checkpoints under the project root? but this would mean that you can't run multiple experiements at the same time. ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with writing checkpoints to the interface, where checkpoints were not being uploaded to the interface whenever a better one was available during training, despite the user's attempts to fix the issue.",
        "Issue_preprocessed_content":"Title: fix writing of checkpoints to; Content: what a clear and concise description of what the bug is. how to reproduce reproduce by starting a non dry run via a notebook . start the run . look at files on the interface. there are no checkpoints expected checkpoints should be uploaded to whenever there is a better one available during training. additional context i thought i fixed , but it seems that i don't understand the symlinking model of . apparently you need to have checkpoints under the project root? but this would mean that you can't run multiple experiements at the same time."
    },
    {
        "Issue_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Issue_title":"Loading configs from wandb yields incorrect parameters",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1624097966000,
        "Issue_closed_time":1624287268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: loading configs from  yields incorrect parameters; Content: ## what\r\n\r\nwhen loading configs from , the resulting hparams objects are not correct. this can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on .\r\n\r\n## how to reproduce\r\n\r\nload the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load__cfg(run_path)\r\np, ptrain = model.hparams(**p), train.hparams(**ptrain)\r\n```\r\n\r\nvalidate against the run [on ](https:\/\/.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## acceptance criteria\r\n\r\n- [x] bug has been understood and fixed\r\n- [x] the same config given above can be loaded and is correct",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when loading configs from , resulting in incorrect hparams objects which could be seen when attempting to load the model checkpoint or when comparing the object with the info panel for the run on .",
        "Issue_preprocessed_content":"Title: loading configs from yields incorrect parameters; Content: what when loading configs from , the resulting hparams objects are not correct. this can be seen when attempting to load the model checkpoint with the given parameters , or when comparing the object with the panel for the run on . how to reproduce load the configs validate against the run acceptance criteria bug has been understood and fixed the same config given above can be loaded and is correct"
    },
    {
        "Issue_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Issue_title":"Config type in WandBLogger",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1635882064000,
        "Issue_closed_time":1635948747000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: config type in logger; Content: this is more like a suggestion than a bug. the `config` parameter to the logger is supposed to be of type `args.namespace`. therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. this might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a yaml file). wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nthanks :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the `config` parameter to the logger, which is supposed to be of type `args.namespace` and is converted to a dictionary inside its `arge_parse` function, potentially restricting the ability to pass configs directly as a dictionary.",
        "Issue_preprocessed_content":"Title: config type in logger; Content: this is more like a suggestion than a bug. the parameter to the logger is supposed to be of type . therefore it converts it to a dictionary inside its function using . this might be restrictive in some cases if someone wants to pass configs directly as a dictionary . wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input? thanks"
    },
    {
        "Issue_link":"https:\/\/github.com\/alvarobartt\/wandbfsspec\/issues\/7",
        "Issue_title":"`WandbFileSystem.ls` not working fine with nested directories",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1658474901000,
        "Issue_closed_time":1658480572000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"https:\/\/wandb.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: `filesystem.ls` not working fine with nested directories; Content: https:\/\/.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the `filesystem.ls` command not working properly when trying to list the contents of a nested directory.",
        "Issue_preprocessed_content":"Title: not working fine with nested directories"
    },
    {
        "Issue_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Issue_title":"wandb create index name error",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1634804027000,
        "Issue_closed_time":1635155013000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  create index name error; Content: - [x]  index name modify\r\n\r\n create index name error and  change name to \"modelname + save_folder_name\"",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to create an index name and had to modify the name to \"modelname + save_folder_name\".",
        "Issue_preprocessed_content":"Title: create index name error; Content: index name modify create index name error and change name to modelname +"
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/173",
        "Issue_title":"Fix Wandb import issue",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1602624602000,
        "Issue_closed_time":1604903231000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: fix  import issue",
        "Issue_original_content_gpt_summary":"The user encountered an issue with importing a module, which was resolved by updating the module's version.",
        "Issue_preprocessed_content":"Title: fix import issue"
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/162",
        "Issue_title":"Weird memory problem with sweeps Colab Wandb",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600976683000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"```Problem Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.``` I'm running into this issue with a specific model (e.g. DA-RNN w\/meta-data sweep). If runs truly aren't cleared then sweeps could be corrupting subsequent runs. This behavior hasn't been observed previously however.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: weird memory problem with sweeps colab ; Content: ```problem trying to backward through the graph a second time, but the saved intermediate results have already been freed. specify retain_graph=true when calling backward the first time.``` i'm running into this issue with a specific model (e.g. da-rnn w\/meta-data sweep). if runs truly aren't cleared then sweeps could be corrupting subsequent runs. this behavior hasn't been observed previously however.",
        "Issue_original_content_gpt_summary":"The user encountered a weird memory problem with sweeps colab, where running a specific model (e.g. da-rnn w\/meta-data sweep) could potentially corrupt subsequent runs if the intermediate results were not retained.",
        "Issue_preprocessed_content":"Title: weird memory problem with sweeps colab; Content: i'm running into this issue with a specific model . if runs truly aren't cleared then sweeps could be corrupting subsequent runs. this behavior hasn't been observed previously however."
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Issue_title":"Wandb Run stalling",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1600217910000,
        "Issue_closed_time":1600665871000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  run stalling; Content:  sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgh78bnb9a5jp6ncffhb189tijy5c#scrollto=stdgwez0d0qp) advance instead they just stall after the first part of the sweep completes. this is causing problems.",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where their primary notebook is stalling after the first part of the sweep completes, causing problems.",
        "Issue_preprocessed_content":"Title: run stalling; Content: sweep on our advance instead they just stall after the first part of the sweep completes. this is causing problems."
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Issue_title":"Wandb bug when running train long",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1578034238000,
        "Issue_closed_time":1578199794000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  bug when running train long; Content: when running this code https:\/\/github.com\/aistream-peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook  views the entire thing as one training session and continue gradient steps indefinitely. training session should be forced to end when that model stops training not when the meta training loop finishes. should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a bug when running a long training session, where the training session was viewed as one continuous session and the gradient steps were forced to continue indefinitely, instead of ending when the model stopped training.",
        "Issue_preprocessed_content":"Title: bug when running train long; Content: when running this code from a colab notebook views the entire thing as one training session and continue gradient steps indefinitely. training session should be forced to end when that model stops training not when the meta training loop finishes. should only be training steps not ."
    },
    {
        "Issue_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Issue_title":"[BUG] wandb value doesn't update",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1622372396000,
        "Issue_closed_time":1622440667000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  value doesn't update; Content: ![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the values in a list were not updating, except for the lr value, due to incorrect indexing of the list.",
        "Issue_preprocessed_content":"Title: value doesn't update; Content: lr value list"
    },
    {
        "Issue_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Issue_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1663015846000,
        "Issue_closed_time":1663248037000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: test set metrics overwrite validation set metrics in tensorboard and are rejected for logging by  (w&b); **describe the bug**\r\n\r\nat model train completion, the test set loss is written as iteration 0 to the tensorboard \/ w&b chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. as the validation loss and perplexity has already been written to this chart, this results in tensorboard deleting all the validation metrics, overwriting them with the test loss and perplexity values. w&b refuses to add the test metrics to the charts at all, throwing a warning that looks like `: warning step must only increase in log calls.  step 0 < 32000; Content: dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**to reproduce**\r\nsteps to reproduce the behavior:\r\n1. pip install and setup tensorboard and w&b\r\n2. begin training a model with a train, validation, and test set\r\n3. observe in both tensorboard and w&b that validation metrics are being logged\r\n4. allow the model to train to completion\r\n5. observe that the tensorboard validation metrics are now gone, overwritten by the test set metrics\r\n6. observe the w&b error in the text logs \/ program output\r\n\r\n**expected behavior**\r\ntest metrics should be written to their own charts.\r\n\r\n**proposed solution**\r\ntest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**environment (please complete the following information):**\r\n - gpus: 4x a100 80 gb\r\n- configs: (configs that i used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# gpt-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": true,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": true,\r\n    \"reduce_scatter\": true,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": true,\r\n    \"cpu_offload\": false\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# suggested data paths when using gpt-neox locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4tbnvme\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4tbnvme\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4tbnvme\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4tbnvme\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": false,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4tbnvme\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4tbnvme\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_\": true,\r\n  \"_host\": \"https:\/\/api..ai\",\r\n  \"_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**additional context**\r\n\r\ni have a bug fix ready, will follow up with it.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the test set metrics overwrite the validation set metrics in Tensorboard and are rejected for logging by Weights & Biases, resulting in the validation metrics being deleted.",
        "Issue_preprocessed_content":"Title: test set metrics overwrite validation set metrics in tensorboard and are rejected for logging by; Content: describe the bug at model train completion, the test set loss is written as iteration to the tensorboard \/ w&b chart , and the test set perplexity is written as iteration to the chart . as the validation loss and perplexity has already been written to this chart, this results in tensorboard deleting all the validation metrics, overwriting them with the test loss and perplexity values. w&b refuses to add the test metrics to the charts at all, throwing a warning that looks like to reproduce steps to reproduce the behavior . pip install and setup tensorboard and w&b . begin training a model with a train, validation, and test set . observe in both tensorboard and w&b that validation metrics are being logged . allow the model to train to completion . observe that the tensorboard validation metrics are now gone, overwritten by the test set metrics . observe the w&b error in the text logs \/ program output expected behavior test metrics should be written to their own charts. proposed solution test loss and perplexity should be written to their own charts and respectively. screenshots environment gpus x a gb configs additional context i have a bug fix ready, will follow up with it."
    },
    {
        "Issue_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Issue_title":"Local wandb logging is borked",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1618393073000,
        "Issue_closed_time":1624218172000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: local  logging is borked; Content: our current  logging assumes the presence of an api key, which you don't need if you're running  locally.\r\n\r\nwe should configure it so it works with  locally, too. ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with local logging, which requires an API key when running locally, and needs to be configured to work without one.",
        "Issue_preprocessed_content":"Title: local logging is borked; Content: our current logging assumes the presence of an api key, which you don't need if you're running locally. we should configure it so it works with locally, too."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Issue_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Issue_label":[
            "bug",
            "help wanted"
        ],
        "Issue_creation_time":1658586252000,
        "Issue_closed_time":1659198312000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: refused to frame 'https:\/\/.ai\/' because an ancestor violates the following content security policy directive: \"frame-ancestors 'self'\".; Content: ## \ud83d\udc1b bug\r\n\r\nrefused to frame 'https:\/\/.ai\/' because an ancestor violates the following content security policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### to reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"screen shot 2022-07-23 at 10 23 34 am\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### code sample\r\n\r\n\r\n### expected behavior\r\n\r\n\r\n### environment\r\n\r\n\r\n### additional context\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where a content security policy directive refused to frame 'https:\/\/.ai\/' due to an ancestor violating the directive.",
        "Issue_preprocessed_content":"Title: refused to frame because an ancestor violates the following content security policy directive frame ancestors 'self' .; Content: bug refused to frame because an ancestor violates the following content security policy directive frame ancestors 'self' . to reproduce code sample expected behavior environment additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/icenet-ai\/icenet\/issues\/72",
        "Issue_title":"wandb entity is hardcoded",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1669459637000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"* IceNet version: 0.2.0.dev10\r\n\r\n`icenet\/model\/train.py` has the wandb.init entity hardcoded, oops\r\n\r\nMake this default to $USER, ICENET_WANDB_USER or be overridden by command line (whichever exists right to left... \ud83d\ude09 )\r\n\r\nDo the same for the project too",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  entity is hardcoded; Content: * icenet version: 0.2.0.dev10\r\n\r\n`icenet\/model\/train.py` has the .init entity hardcoded, oops\r\n\r\nmake this default to $user, icenet__user or be overridden by command line (whichever exists right to left... \ud83d\ude09 )\r\n\r\ndo the same for the project too",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the entity was hardcoded in the icenet\/model\/train.py file, and needed to be changed to either $user, icenet__user or be overridden by command line.",
        "Issue_preprocessed_content":"Title: entity is hardcoded; Content: icenet version has the entity hardcoded, oops make this default to $user, or be overridden by command line do the same for the project too"
    },
    {
        "Issue_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Issue_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Issue_label":[
            "bug",
            "waiting"
        ],
        "Issue_creation_time":1615226514000,
        "Issue_closed_time":1615408051000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: remove data\/ and \/ directories and rewrite history; Content: the `data\/mnist` subdirectory slipped through `.gitignore` and is now part of the repo's history. these binary files should be removed. there's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nat the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. let's do that once we have committed all local changes.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to remove the data\/ and \/ directories and rewrite history due to the `data\/mnist` subdirectory slipping through `.gitignore` and becoming part of the repo's history, and needing to use an open-source tool called `bfg` to do so, followed by deleting local clones and cloning a fresh, cleaned version from upstream.",
        "Issue_preprocessed_content":"Title: remove data\/ and \/ directories and rewrite history; Content: the subdirectory slipped through and is now part of the repo's history. these binary files should be removed. there's an open source tool available to do that called . at the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. let's do that once we have committed all local changes."
    },
    {
        "Issue_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Issue_title":"wrong wandb dataset file name",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1634910579000,
        "Issue_closed_time":1634915290000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: wrong  dataset file name; Content: ## tl;dr\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## why?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## what?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## todos\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to upload a golden test dataset to Wandb, where the structure of the existing dataset was changed from train\/validation to training, but the name was not changed to train, resulting in the data not being loaded.",
        "Issue_preprocessed_content":"Title: wrong dataset file name; Content: tl;dr golden test dataset , train\/ validation training train wisdomify . why? . what? . todos ."
    },
    {
        "Issue_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Issue_title":"wandb import failure",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1634732461000,
        "Issue_closed_time":1635333937000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  import failure; Content: **describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import mapping, sequence \r\nmodulenotfounderror: no module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**to reproduce**\r\nrun on @ohsuz 's server.\r\n(cannot reproduce on intel i7 based local condition.)\r\n\r\n**expected behavior**\r\n should be properly imported.\r\n\r\n**server (please complete the following information):**\r\n - os: centos\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a ModuleNotFoundError when attempting to import from six.moves.collections_abc on a CentOS server, despite being able to reproduce the import on an Intel i7-based local machine.",
        "Issue_preprocessed_content":"Title: import failure; Content: describe the bug from import mapping, sequence modulenotfounderror no module named to reproduce run on 's server. cannot reproduce on intel i based local expected behavior should be properly imported. server os centos"
    },
    {
        "Issue_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Issue_title":"wandb api key for github ci",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1642148915000,
        "Issue_closed_time":1642173581000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  api key for github ci; Content:  api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with configuring an API key for GitHub CI.",
        "Issue_preprocessed_content":"Title: api key for github ci; Content: api key not configured for github ci"
    },
    {
        "Issue_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Issue_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1638280869000,
        "Issue_closed_time":1638449992000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: [bug]  logger does not work unless pytorch is installed ; Content: ### \ud83d\udc1b bug report\n\nlogger throws error while import if etna[torch] is not installed.\n\n### expected behavior\n\n logger should work no matter pytorch installation \n\n### how to reproduce\n\n1. create new env\r\n2. install etna and etna[]\r\n3. import logger\r\n\n\n### environment\n\n_no response_\n\n### additional context\n\n_no response_\n\n### checklist\n\n- [x] bug appears at the latest library version",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger does not work unless PyTorch is installed.",
        "Issue_preprocessed_content":"Title: logger does not work unless pytorch is installed; Content: bug report logger throws error while import if etna is not installed. expected behavior logger should work no matter pytorch installation how to reproduce . create new env . install etna and etna . import logger environment additional context checklist bug appears at the latest library version"
    },
    {
        "Issue_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Issue_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1634647592000,
        "Issue_closed_time":1635943713000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: exception in backtest with `aggregate_metrics=true` when using `logger`; Content: ### \ud83d\udc1b bug report\n\nprogram fails when backtest with `aggregate_metrics=true` is used inside `logger` (if given). with `aggregate_metrics=false` everything is fine.\r\n\r\nexception happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nexception was caught in `pipeline.backtest`, but it looks like this bug also appears in `timeseriescrossvalidation` class.\n\n### expected behavior\n\nno error.\n\n### how to reproduce\n\nrun backtest with wandlogger while setting `aggregate_metrics=true`. \n\n### environment\n\n_no response_\n\n### additional context\n\n_no response_\n\n### checklist\n\n- [x] bug appears at the latest library version\n- [x] bug description added\n- [x] steps to reproduce added\n- [x] expected behavior added",
        "Issue_original_content_gpt_summary":"The user encountered a bug when running a backtest with `aggregate_metrics=true` inside `logger`, resulting in an exception in `tslogger.log_backtest_metrics` while constructing `metrics_df`.",
        "Issue_preprocessed_content":"Title: exception in backtest with when using; Content: bug report program fails when backtest with is used inside . with everything is fine. exception happens in while constructing it can't make . exception was caught in , but it looks like this bug also appears in class. expected behavior no error. how to reproduce run backtest with wandlogger while setting . environment additional context checklist bug appears at the latest library version bug description added steps to reproduce added expected behavior added"
    },
    {
        "Issue_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Issue_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Issue_label":[
            "bug",
            "documentation",
            "good first issue",
            "quick-fix",
            "discussion"
        ],
        "Issue_creation_time":1631000506000,
        "Issue_closed_time":1668696973000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: use tensorboard as default logger and get  optional within the project ; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges in using Tensorboard as the default logger and getting optional within the project.",
        "Issue_preprocessed_content":"Title: use tensorboard as default logger and get optional within the project; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/virasad\/semantic_segmentation_service\/issues\/16",
        "Issue_title":"wandb logger",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1653214451000,
        "Issue_closed_time":1653224504000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges in setting up a logger for their application.",
        "Issue_preprocessed_content":"Title: logger; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/478",
        "Issue_title":"Question: How to save hydra config to wandb config.yaml",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1670626715000,
        "Issue_closed_time":1670781696000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Hello, \r\n\r\nI'm using `wandb` logger (and `csv` as well), I found recently `hydra` config no longer save to `wandb` 's`config.yaml` file.\r\nBefore:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.EarlyStopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.ModelCheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichModelSummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichProgressBar\r\nckpt_path:\r\n  desc: null\r\n  value: None\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.MNISTDataModule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.MNISTLitModule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.SimpleDenseNet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.Adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.ReduceLROnPlateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.Trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nNow:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nThis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#L172\r\nAny idea how to restore to previous state?",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: question: how to save hydra config to  config.yaml; Content: hello, \r\n\r\ni'm using `` logger (and `csv` as well), i found recently `hydra` config no longer save to `` 's`config.yaml` file.\r\nbefore:\r\n```\r\n_version: 1\r\n\r\n_:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.earlystopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: none\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: none\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: none\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.modelcheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/users\/caoyu\/github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: none\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: none\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: none\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: none\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.richmodelsummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.richprogressbar\r\nckpt_path:\r\n  desc: null\r\n  value: none\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.mnistdatamodule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/users\/caoyu\/github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.mnistlitmodule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.simpledensenet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.reducelronplateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/users\/caoyu\/github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nnow:\r\n```\r\n_version: 1\r\n\r\n_:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nthis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#l172\r\nany idea how to restore to previous state?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with saving Hydra config to config.yaml, and was looking for a way to restore to the previous state.",
        "Issue_preprocessed_content":"Title: question how to save hydra config to; Content: hello, i'm using logger , i found recently config no longer save to file. before now this may related to any idea how to restore to previous state?"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Issue_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1656590728000,
        "Issue_closed_time":1657912525000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: hydra-optuna-sweeper and  versions conflict; Content: hi!\r\n\r\ni have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\ni faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\ni got the following error:\r\n```\r\ntraceback (most recent call last):\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is none\r\nassertionerror\r\n```\r\nthe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nfile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nbut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nif i understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. when i change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2.  problem\r\nafter the command `pip install -r requrements.txt` ==0.12.20 was installed.\r\nwhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=\r\n```\r\nthe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nexception in thread streamthr:\r\ntraceback (most recent call last):\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\internal\\internal.py\", line 85, in _internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.filehandler(log_fname)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    streamhandler.__init__(self, self._open())\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.basefilename, self.mode, encoding=self.encoding,\r\nfilenotfounderror: [errno 2] no such file or directory: 'c:\\\\users\\\\yusip\\\\desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\ntraceback (most recent call last):\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, none,\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m \")\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nassertionerror\r\nproblem at: c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\.py 357 experiment\r\n: error error communicating with  process\r\n: error try: .init(settings=.settings(start_method='fork'))\r\n: error or:  .init(settings=.settings(start_method='thread'))\r\n: error for more info see: https:\/\/docs..ai\/library\/init#init-start-error\r\nerror executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=']\r\nerror in call to target 'pytorch_lightning.loggers..logger':\r\nusageerror(\"error communicating with  process\\ntry: .init(settings=.settings(start_method='fork'))\\nor:  .init(settings=.settings(start_method='thread'))\\nfor more info see: htt\r\nps:\/\/docs..ai\/library\/init#init-start-error\")\r\nfull_key: logger.\r\n```\r\nit is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered two challenges when attempting to run a hyperparametric search using hydra-optuna-sweeper: a conflict between the installed versions of the packages and an error when initializing the logger.",
        "Issue_preprocessed_content":"Title: hydra optuna sweeper and versions conflict; Content: hi! i have installed all required packages by and tried to run hyperparametric search using the i faced problems . hydra optuna sweeper problem i got the following error the same error was reported in . file contains the following versions for hydra optuna sweeper but the latest versions of the packages are installing if i understand correctly, optuna sweeper's syntax has changed in hydra since version when i change the syntax to the new version everything works without errors. . problem after the command was installed. when running the training process with this logger the first run with the certian parameters combination finished successfully, the second run had the error it is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error."
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Issue_title":"wandb logger not working",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1654326486000,
        "Issue_closed_time":1654420353000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  logger not working; Content: hi there, \r\nthank you for this powerful template! \r\ni run into a problem while trying to use  as logger\r\ni used the -callbacks branch and after `python train.py logger=` i get (cancelled by user after 130 iterations cause  login does not appear)\r\n\r\n````\r\n$ python train.py logger=\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 name          \u2502 type             \u2502 params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 simpledensenet   \u2502  336 k \u2502\r\n\u2502 1  \u2502 model.model   \u2502 sequential       \u2502  336 k \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 linear           \u2502  200 k \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 batchnorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 relu             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 linear           \u2502 65.8 k \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 batchnorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 relu             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 linear           \u2502 65.8 k \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 batchnorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 relu             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 linear           \u2502  2.6 k \u2502\r\n\u2502 12 \u2502 criterion     \u2502 crossentropyloss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 maxmetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\ntrainable params: 336 k\r\nnon-trainable params: 0\r\ntotal params: 336 k\r\ntotal estimated model params size (mb): 1\r\nepoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nerror executing job with overrides: ['logger=']\r\n````\r\n_(note the last line)_\r\n\r\nchanging `logger: ` in train.yaml does not work either. i'm a bit confused because i had it working once before but just don't know what to do anymore. i tried out different conda envs with different torch and pl versions. does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\npackage                 version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nbabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\ngitpython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\njinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nmako                    1.2.0\r\nmarkdown                3.3.7\r\nmarkupsafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\npillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npydeprecate             0.3.2\r\npyflakes                2.4.0\r\npygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\npyyaml                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nsend2trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nsqlalchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\n                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nwerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while trying to use a logger with the -callbacks branch, resulting in an error after 130 iterations.",
        "Issue_preprocessed_content":"Title: logger not working; Content: hi there, thank you for this powerful template! i run into a problem while trying to use as logger i used the callbacks branch and after i get logger `"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Issue_title":"wandb log only 1 run when using ddp and multirun",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1651909943000,
        "Issue_closed_time":1657910798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  log only 1 run when using ddp and multirun; Content: when i use ddp,  and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=`\r\n does not record 3 runs, but only one run.\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to log multiple runs when using distributed data parallelism (DDP) and multirun in a Python script.",
        "Issue_preprocessed_content":"Title: log only run when using ddp and multirun; Content: when i use ddp, and multirun in like this does not record runs, but only one run."
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Issue_title":"Wandb is not compatible with PL 1.6.1",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1650933722000,
        "Issue_closed_time":1654689077000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  is not compatible with pl 1.6.1; Content: hi,\r\n\r\nthere may be version conflict between  and pl 1.6.1\r\n\r\n**os:** ubuntu20.04\r\n**python:** 3.8.13\r\n**pytorch:**  1.11.0\r\n**pl:** 1.6.1\r\n**:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen i use the hyperparameter search, it produces the following error:\r\n\r\n```python\r\nfilenotfounderror: [errno 2] no such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/\/offline-run-20*\/logs\/debug-internal.log'\r\nproblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py 357 experiment\r\n```\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to use a hyperparameter search with incompatible versions of os, python, pytorch, pl, and hydra-core.",
        "Issue_preprocessed_content":"Title: is not compatible with pl; Content: hi, there may be version conflict between and pl os python pytorch pl hydra core when i use the hyperparameter search, it produces the following error"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Issue_title":"WandB fails when config is too large",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1666171234000,
        "Issue_closed_time":1666770135000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  fails when config is too large; Content: i tried to run benchmark.py, with , but got an error because the config is too large, probably due to the train_selection array being too big. `error error while calling api: run config cannot exceed 15 mb (<response [400]>)`\r\n\r\nperhaps the data selections does not need to be uploaded to ?\r\n\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to run benchmark.py with a config that was too large, likely due to the train_selection array being too big.",
        "Issue_preprocessed_content":"Title: fails when config is too large; Content: i tried to run with , but got an error because the config is too large, probably due to the array being too big. perhaps the data selections does not need to be uploaded to ? the full message is"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Issue_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1661861159000,
        "Issue_closed_time":1661948109000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: running train_model from examples after install needs directory \"\"; Content: after installing graphnet from scratch and signing up to , running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: info     2022-08-30 12:21:56 - get_logger - writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: warning  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: warning  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: warning  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: warning  2022-08-30 12:21:56 - <module> - icecube package not available.\r\n: currently logged in as: peterandresen (graphnet-team). use ` login --relogin` to force relogin\r\n: warning path .\/\/\/ wasn't writable, using system temp directory.\r\ntraceback (most recent call last):\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\npermissionerror: [errno 13] permission denied: '\/tmp\/\/run-20220830_122200-1qc85fm4'\r\n: error abnormal program exit\r\ntraceback (most recent call last):\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\npermissionerror: [errno 13] permission denied: '\/tmp\/\/run-20220830_122200-1qc85fm4'\r\n\r\nthe above exception was the direct cause of the following exception:\r\n\r\ntraceback (most recent call last):\r\n  file \"train_model.py\", line 37, in <module>\r\n    _logger = logger(\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or dummyexperiment()\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 361, in experiment\r\n    self._experiment = .init(**self.__init)\r\n  file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1081, in init\r\n    raise exception(\"problem\") from error_seen\r\nexception: problem\r\n```\r\n\r\nwhich can be fixed by creating a folder called \"\" in the place where you are running the file from. would it make sense to automatically create such a folder, if it is not already present?",
        "Issue_original_content_gpt_summary":"The user encountered a PermissionError when running train_model from examples after installing graphnet from scratch, which was fixed by creating a folder called \"\" in the place where the file was being run from.",
        "Issue_preprocessed_content":"Title: running from examples after install needs directory; Content: after installing graphnet from scratch and signing up to , running from examples yields the following error which can be fixed by creating a folder called in the place where you are running the file from. would it make sense to automatically create such a folder, if it is not already present?"
    },
    {
        "Issue_link":"https:\/\/github.com\/neuro-inc\/mlops-wandb-bucket-ref\/issues\/16",
        "Issue_title":"WandB output overwrites wabucketref's output in case of artifact upload",
        "Issue_label":[
            "bug"
        ],
        "Issue_creation_time":1625736474000,
        "Issue_closed_time":1625736868000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nAt the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nHowever, we have:\r\n```\r\n...\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\nwandb: Waiting for W&B process to finish, PID 75\r\n...\r\n```\r\n\r\nWhile it should be:\r\n```\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nwandb: Waiting for W&B process to finish, PID 75\r\nwandb: Program ended successfully.\r\nwandb:                                                                                \r\n```\r\n\r\nOne line was overwritten by the `wandb: Waiting for W&B process to finish, PID 75`, which, apparently is running in a separate process (`wandb.Settings(start_method=\"fork\")`). ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title:  output overwrites wabucketref's output in case of artifact upload; Content: example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nat the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nhowever, we have:\r\n```\r\n...\r\ninfo:wabucketref.api:uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\ninfo:wabucketref.api:artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\ninfo:botocore.credentials:found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\n: generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n: waiting for w&b process to finish, pid 75\r\n...\r\n```\r\n\r\nwhile it should be:\r\n```\r\ninfo:wabucketref.api:uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\ninfo:wabucketref.api:artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\ninfo:botocore.credentials:found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\n: generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\n: waiting for w&b process to finish, pid 75\r\n: program ended successfully.\r\n:                                                                                \r\n```\r\n\r\none line was overwritten by the `: waiting for w&b process to finish, pid 75`, which, apparently is running in a separate process (`.settings(start_method=\"fork\")`). ",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the output of the wabucketref was overwritten by a separate process, resulting in a missing line of output.",
        "Issue_preprocessed_content":"Title: output overwrites wabucketref's output in case of artifact upload; Content: example job job acb d e a aa ce ddc f at the end of the job run, we upload the artifact, where happens, and terminate the job. however, we have while it should be one line was overwritten by the , which, apparently is running in a separate process ` ."
    },
    {
        "Issue_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Issue_title":"In random agent script wandb full episode data logging skips a few steps",
        "Issue_label":[
            "bug",
            "concerns: agents"
        ],
        "Issue_creation_time":1649933570000,
        "Issue_closed_time":1650034426000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content_preprocessed_text":"Title: in random agent script  full episode data logging skips a few steps; Content: ### problem\r\n\r\n in random agent script  full episode data logging skips a few steps. this is because  counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### potential solution\r\n\r\nadd another metric to log that shows timestep and day (proportional).\r\n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the random agent script where full episode data logging was skipping a few steps, and proposed a potential solution of adding another metric to log that shows timestep and day (proportional).",
        "Issue_preprocessed_content":"Title: in random agent script full episode data logging skips a few steps; Content: problem in random agent script full episode data logging skips a few steps. this is because counts the epsiode reward logging steps made prior to the full data logging. potential solution add another metric to log that shows timestep and day ."
    },
    {
        "Issue_link":"https:\/\/gitlab.com\/fluidattacks\/universe\/-\/issues\/8382",
        "Issue_title":"[Sorts] Add sagemaker dependencies",
        "Issue_label":[
            "arena::security",
            "product::sorts",
            "type::bug"
        ],
        "Issue_creation_time":1671481698985,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":null,
        "Issue_body":"<!-- Issues are public, they should not contain confidential information -->\n\n### What is the current _bug_ behavior? how can we reproduce it?\nThe requirements.txt file does not have the entire dependency tree defined\n\n### Possible fixes\nModify the requirements.txt file so that it has the complete tree of dependencies and their respective versions\n### Steps\n\n- [x] Make sure that the\n      [code contributions checklist](https:\/\/docs.fluidattacks.com\/development\/contributing#checklist)\n      has been followed.",
        "Tool":"Amazon SageMaker",
        "Platform":"Gitlab",
        "Issue_original_content_preprocessed_text":"Title: [sorts] add  dependencies; Content: <!-- issues are public, they should not contain confidential information -->\n\n### what is the current _bug_ behavior? how can we reproduce it?\nthe requirements.txt file does not have the entire dependency tree defined\n\n### possible fixes\nmodify the requirements.txt file so that it has the complete tree of dependencies and their respective versions\n### steps\n\n- [x] make sure that the\n      [code contributions checklist](https:\/\/docs.fluidattacks.com\/development\/contributing#checklist)\n      has been followed.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the requirements.txt file did not have the entire dependency tree defined, and needed to modify the file to include the complete tree of dependencies and their respective versions.",
        "Issue_preprocessed_content":"Title: add dependencies; Content: what is the current behavior? how can we reproduce it? the file does not have the entire dependency tree defined possible fixes modify the file so that it has the complete tree of dependencies and their respective versions steps make sure that the has been followed."
    }
]